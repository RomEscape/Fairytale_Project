import asyncio
import re
from typing import Optional, Union, Any, List, Dict
import numpy as np
import json
from loguru import logger

from ..message_handler import message_handler
from .types import WebSocketSend, BroadcastContext
from .tts_manager import TTSTaskManager
from ..agent.output_types import SentenceOutput, AudioOutput
from ..agent.input_types import BatchInput, TextData, ImageData, TextSource, ImageSource
from ..asr.asr_interface import ASRInterface
from ..live2d_model import Live2dModel
from ..tts.tts_interface import TTSInterface
from ..utils.stream_audio import prepare_audio_payload


# Convert class methods to standalone functions
def create_batch_input(
    input_text: str,
    images: Optional[List[Dict[str, Any]]],
    from_name: str,
    metadata: Optional[Dict[str, Any]] = None,
) -> BatchInput:
    """Create batch input for agent processing"""
    return BatchInput(
        texts=[
            TextData(source=TextSource.INPUT, content=input_text, from_name=from_name)
        ],
        images=[
            ImageData(
                source=ImageSource(img["source"]),
                data=img["data"],
                mime_type=img["mime_type"],
            )
            for img in (images or [])
        ]
        if images
        else None,
        metadata=metadata,
    )


async def process_agent_output(
    output: Union[AudioOutput, SentenceOutput],
    character_config: Any,
    live2d_model: Live2dModel,
    tts_engine: TTSInterface,
    websocket_send: WebSocketSend,
    tts_manager: TTSTaskManager,
    translate_engine: Optional[Any] = None,
) -> str:
    """Process agent output with character information and optional translation"""
    output.display_text.name = character_config.character_name
    output.display_text.avatar = character_config.avatar

    full_response = ""
    try:
        if isinstance(output, SentenceOutput):
            full_response = await handle_sentence_output(
                output,
                live2d_model,
                tts_engine,
                websocket_send,
                tts_manager,
                translate_engine,
            )
        elif isinstance(output, AudioOutput):
            full_response = await handle_audio_output(output, websocket_send)
        else:
            logger.warning(f"Unknown output type: {type(output)}")
    except Exception as e:
        logger.error(f"Error processing agent output: {e}")
        await websocket_send(
            json.dumps(
                {"type": "error", "message": f"Error processing response: {str(e)}"}
            )
        )

    return full_response


def _fix_spacing(text: str) -> str:
    """
    Fix spacing after punctuation marks in Korean text.
    Adds space after punctuation marks (.,!?) if not already present.
    Also handles Korean honorific endings followed by Korean characters.
    """
    if not isinstance(text, str) or not text:
        return text
    
    # ì‰¼í‘œ, ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ ë’¤ì— ê³µë°±ì´ ì—†ìœ¼ë©´ ì¶”ê°€
    # ë‹¨, ìˆ«ì ë’¤ì˜ ë§ˆì¹¨í‘œ(ì˜ˆ: 1.2)ë‚˜ ì—°ì†ëœ êµ¬ë‘ì (ì˜ˆ: ...)ì€ ì œì™¸
    # í•œê¸€ ììŒ/ëª¨ìŒ ë’¤ì— êµ¬ë‘ì ì´ ì˜¤ëŠ” ê²½ìš°ë„ ì²˜ë¦¬
    # ì¡´ëŒ“ë§ ì–´ë¯¸ ë’¤ì— í•œê¸€ì´ ë°”ë¡œ ì˜¤ëŠ” ê²½ìš°ë„ ì²˜ë¦¬
    fixed_text = text
    # Korean honorific endings that should have space after them
    honorific_endings = ["ì–´ìš”", "ì•„ìš”", "í•´ìš”", "ì˜ˆìš”", "ì„¸ìš”", "ìŠµë‹ˆë‹¤", "ë„¤ìš”", "ì£ ", "ê¹Œìš”", "ë‚˜ìš”", "ê°€ìš”", "ì§€ìš”", "ì—ˆì–´ìš”", "ì•˜ì–´ìš”", "í–ˆìŠµë‹ˆë‹¤"]
    
    for _ in range(50):  # ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ì—¬ ëª¨ë“  ê²½ìš°ë¥¼ ì²˜ë¦¬
        # ì‰¼í‘œ ë’¤ì— ê³µë°±ì´ ì—†ëŠ” ê²½ìš° (ê°€ì¥ ë¨¼ì € ì²˜ë¦¬)
        new_text = re.sub(r'([,])([^\s\n])', r'\1 \2', fixed_text)
        # êµ¬ë‘ì (ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ) ë’¤ì— ê³µë°±ì´ ì—†ëŠ” ê²½ìš°
        new_text = re.sub(r'([.!?])([^\s\n])', r'\1 \2', new_text)
        # ì—°ì†ëœ êµ¬ë‘ì  ì²˜ë¦¬ (ì˜ˆ: ... ë’¤ì— ê³µë°±ì´ ì—†ëŠ” ê²½ìš°)
        new_text = re.sub(r'([.,!?]{2,})([^\s\n])', r'\1 \2', new_text)
        # í•œê¸€ ë’¤ì— êµ¬ë‘ì ì´ ë¶™ì–´ìˆê³  ê·¸ ë’¤ì— ê³µë°±ì´ ì—†ëŠ” ê²½ìš°
        new_text = re.sub(r'([ê°€-í£])([.,!?])([^\s\n])', r'\1\2 \3', new_text)
        # ì¡´ëŒ“ë§ ì–´ë¯¸ ë’¤ì— í•œê¸€ì´ ë°”ë¡œ ì˜¤ëŠ” ê²½ìš° (ë°˜ë³µ ë°©ì§€ë¥¼ ìœ„í•´)
        # "ìˆì–´ìš”ì´" -> "ìˆì–´ìš” ì´" ê°™ì€ ê²½ìš° ì²˜ë¦¬
        for ending in honorific_endings:
            # ì¡´ëŒ“ë§ ì–´ë¯¸ ë’¤ì— í•œê¸€ì´ ë°”ë¡œ ì˜¤ëŠ” ê²½ìš°
            pattern = re.escape(ending) + r'([ê°€-í£])'
            replacement = ending + r' \1'
            new_text = re.sub(pattern, replacement, new_text)
        # ìˆ«ì ë’¤ì˜ ë§ˆì¹¨í‘œëŠ” ì œì™¸ (ì˜ˆ: 1.2ëŠ” 1. 2ë¡œ ë°”ë€Œì§€ ì•Šë„ë¡)
        new_text = re.sub(r'(\d)\. (\d)', r'\1.\2', new_text)
        
        if new_text == fixed_text:
            break
        fixed_text = new_text
    return fixed_text


async def handle_sentence_output(
    output: SentenceOutput,
    live2d_model: Live2dModel,
    tts_engine: TTSInterface,
    websocket_send: WebSocketSend,
    tts_manager: TTSTaskManager,
    translate_engine: Optional[Any] = None,
) -> str:
    """Handle sentence output type with optional translation support"""
    full_response = ""
    accumulated_display_text = ""  # ëˆ„ì ëœ display_textë¥¼ ì¶”ì 
    
    async for display_text, tts_text, actions in output:
        logger.debug(f"Processing output: '''{tts_text}'''...")

        if translate_engine:
            if len(re.sub(r'[\s.,!?ï¼Œã€‚ï¼ï¼Ÿ\'"ã€ã€ï¼‰ã€‘\s]+', "", tts_text)):
                tts_text = translate_engine.translate(tts_text)
            logger.info(f"Text after translation: '''{tts_text}'''...")
        else:
            logger.debug("No translation engine available. Skipping translation.")

        # ëˆ„ì ëœ í…ìŠ¤íŠ¸ì— í˜„ì¬ chunk ì¶”ê°€
        accumulated_display_text += display_text.text
        
        # ë°˜ë³µ í…ìŠ¤íŠ¸ ì œê±° (ollama_llm.pyì—ì„œ ì²˜ë¦¬í–ˆì§€ë§Œ ì¶”ê°€ ì•ˆì „ì¥ì¹˜)
        # ì—¬ëŸ¬ íŒ¨í„´ì„ ì²´í¬í•˜ì—¬ ë‹¤ì–‘í•œ ë°˜ë³µì„ ê°ì§€
        if len(accumulated_display_text) > 10:
            text_len = len(accumulated_display_text)
            
            # Check 1: ì „ì²´ ë°˜ë³µ (ì—¬ëŸ¬ split ratio ì²´í¬)
            for ratio in [0.4, 0.45, 0.5, 0.55, 0.6]:
                split_point = int(text_len * ratio)
                if split_point >= 10:
                    first_part = accumulated_display_text[:split_point]
                    second_part = accumulated_display_text[split_point:]
                    if second_part.startswith(first_part):
                        remaining = second_part[len(first_part):].strip()
                        if len(remaining) < len(first_part) * 0.1:
                            accumulated_display_text = first_part.strip()
                            logger.debug(f"Removed duplicate text in conversation_utils (ratio {ratio:.2f})")
                            break
            
            # Check 2: ëë¶€ë¶„ ë°˜ë³µ ê°ì§€
            if len(accumulated_display_text) == text_len:  # ì•„ì§ ì œê±°ë˜ì§€ ì•Šì•˜ìœ¼ë©´
                check_len = min(text_len // 3, 50)
                if check_len >= 10:
                    last_part = accumulated_display_text[-check_len:]
                    search_text = accumulated_display_text[:text_len // 2]
                    if last_part in search_text:
                        first_idx = search_text.find(last_part)
                        if first_idx >= 0 and first_idx <= len(search_text) * 0.3:
                            accumulated_display_text = accumulated_display_text[:first_idx + len(last_part)].strip()
                            logger.debug("Removed duplicate phrase at end in conversation_utils")
        
        # Stop sequence ì œê±° (ollama_llm.pyì—ì„œ ì²˜ë¦¬í–ˆì§€ë§Œ ì¶”ê°€ ì•ˆì „ì¥ì¹˜)
        cleaned_text = accumulated_display_text
        stop_sequences = ["<|eot|>", "<|eot_id|>", "\nì‚¬ìš©ì:", "\nì‚¬ìš©ì ë§í•˜ê¸°:"]
        for stop_seq in stop_sequences:
            if stop_seq in cleaned_text:
                cleaned_text = cleaned_text.split(stop_seq)[0]
        # ë¶€ë¶„ì ìœ¼ë¡œ í¬í•¨ëœ stop sequenceë„ ì œê±°
        if "<|eot" in cleaned_text:
            cleaned_text = cleaned_text.split("<|eot")[0]
        # ë‹¨ì¼ ë¬¸ì stop sequenceë„ ì œê±° (ì˜ˆ: "<" ë§Œ ìˆëŠ” ê²½ìš°)
        if cleaned_text.endswith("<"):
            cleaned_text = cleaned_text[:-1].strip()
        
        # ëˆ„ì ëœ ì „ì²´ í…ìŠ¤íŠ¸ì— ë„ì–´ì“°ê¸° ìˆ˜ì • ì ìš© (ê°•ë ¥í•˜ê²Œ)
        fixed_accumulated = _fix_spacing(cleaned_text)
        # ì¶”ê°€ë¡œ êµ¬ë‘ì  ë’¤ ë„ì–´ì“°ê¸° í™•ì¸ ë° ìˆ˜ì •
        fixed_accumulated = re.sub(r'([.!?])([ê°€-í£])', r'\1 \2', fixed_accumulated)
        fixed_accumulated = re.sub(r'([,])([ê°€-í£])', r'\1 \2', fixed_accumulated)
        
        # ë„ì–´ì“°ê¸° ìˆ˜ì • í›„ ë‹¤ì‹œ stop sequence ì²´í¬
        if "<|eot" in fixed_accumulated:
            fixed_accumulated = fixed_accumulated.split("<|eot")[0].strip()
        if fixed_accumulated.endswith("<"):
            fixed_accumulated = fixed_accumulated[:-1].strip()
        
        # ì´ì „ì— ì „ì†¡í•œ í…ìŠ¤íŠ¸ ì´í›„ì˜ ìƒˆë¡œìš´ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        # (ì´ì „ì— ì „ì†¡í•œ ë¶€ë¶„ì€ ì´ë¯¸ ìˆ˜ì •ë˜ì—ˆìœ¼ë¯€ë¡œ, ìƒˆë¡œìš´ ë¶€ë¶„ë§Œ ìˆ˜ì •)
        prev_length = len(full_response)
        new_chunk = fixed_accumulated[prev_length:]
        
        # display_text.textë¥¼ ìˆ˜ì •ëœ ìƒˆë¡œìš´ chunkë¡œ ì—…ë°ì´íŠ¸
        if new_chunk:
            display_text.text = new_chunk
        # new_chunkê°€ ë¹„ì–´ìˆìœ¼ë©´ ì›ë³¸ ìœ ì§€ (ì´ë¯¸ ì „ì†¡ëœ ê²½ìš°)
        
        full_response = fixed_accumulated  # ì „ì²´ ëˆ„ì  í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        
        await tts_manager.speak(
            tts_text=tts_text,
            display_text=display_text,
            actions=actions,
            live2d_model=live2d_model,
            tts_engine=tts_engine,
            websocket_send=websocket_send,
        )
    return full_response


async def handle_audio_output(
    output: AudioOutput,
    websocket_send: WebSocketSend,
) -> str:
    """Process and send AudioOutput directly to the client"""
    full_response = ""
    async for audio_path, display_text, transcript, actions in output:
        full_response += transcript
        audio_payload = prepare_audio_payload(
            audio_path=audio_path,
            display_text=display_text,
            actions=actions.to_dict() if actions else None,
        )
        await websocket_send(json.dumps(audio_payload))
    return full_response


async def send_conversation_start_signals(websocket_send: WebSocketSend) -> None:
    """Send initial conversation signals"""
    await websocket_send(
        json.dumps(
            {
                "type": "control",
                "text": "conversation-chain-start",
            }
        )
    )
    await websocket_send(json.dumps({"type": "full-text", "text": "Thinking..."}))


async def process_user_input(
    user_input: Union[str, np.ndarray],
    asr_engine: ASRInterface,
    websocket_send: WebSocketSend,
) -> str:
    """Process user input, converting audio to text if needed"""
    if isinstance(user_input, np.ndarray):
        logger.info("Transcribing audio input...")
        input_text = await asr_engine.async_transcribe_np(user_input)
        await websocket_send(
            json.dumps({"type": "user-input-transcription", "text": input_text})
        )
        return input_text
    return user_input


async def finalize_conversation_turn(
    tts_manager: TTSTaskManager,
    websocket_send: WebSocketSend,
    client_uid: str,
    broadcast_ctx: Optional[BroadcastContext] = None,
) -> None:
    """Finalize a conversation turn"""
    if tts_manager.task_list:
        await asyncio.gather(*tts_manager.task_list)
        await websocket_send(json.dumps({"type": "backend-synth-complete"}))

        response = await message_handler.wait_for_response(
            client_uid, "frontend-playback-complete"
        )

        if not response:
            logger.warning(f"No playback completion response from {client_uid}")
            return

    await websocket_send(json.dumps({"type": "force-new-message"}))

    if broadcast_ctx and broadcast_ctx.broadcast_func:
        await broadcast_ctx.broadcast_func(
            broadcast_ctx.group_members,
            {"type": "force-new-message"},
            broadcast_ctx.current_client_uid,
        )

    await send_conversation_end_signal(websocket_send, broadcast_ctx)


async def send_conversation_end_signal(
    websocket_send: WebSocketSend,
    broadcast_ctx: Optional[BroadcastContext],
    session_emoji: str = "",
) -> None:
    """Send conversation chain end signal"""
    chain_end_msg = {
        "type": "control",
        "text": "conversation-chain-end",
    }

    await websocket_send(json.dumps(chain_end_msg))

    if broadcast_ctx and broadcast_ctx.broadcast_func and broadcast_ctx.group_members:
        await broadcast_ctx.broadcast_func(
            broadcast_ctx.group_members,
            chain_end_msg,
        )

    logger.info(f"Conversation Chain completed!")


def cleanup_conversation(tts_manager: TTSTaskManager, session_emoji: str) -> None:
    """Clean up conversation resources"""
    tts_manager.clear()
    logger.debug(f"Clearing up conversation.")


EMOJI_LIST = [
    "ğŸ¶",
    "ğŸ±",
    "ğŸ­",
    "ğŸ¹",
    "ğŸ°",
    "ğŸ¦Š",
    "ğŸ»",
    "ğŸ¼",
    "ğŸ¨",
    "ğŸ¯",
    "ğŸ¦",
    "ğŸ®",
    "ğŸ·",
    "ğŸ¸",
    "ğŸµ",
    "ğŸ”",
    "ğŸ§",
    "ğŸ¦",
    "ğŸ¤",
    "ğŸ£",
    "ğŸ¥",
    "ğŸ¦†",
    "ğŸ¦…",
    "ğŸ¦‰",
    "ğŸ¦‡",
    "ğŸº",
    "ğŸ—",
    "ğŸ´",
    "ğŸ¦„",
    "ğŸ",
    "ğŸŒµ",
    "ğŸ„",
    "ğŸŒ²",
    "ğŸŒ³",
    "ğŸŒ´",
    "ğŸŒ±",
    "ğŸŒ¿",
    "â˜˜ï¸",
    "ğŸ€",
    "ğŸ‚",
    "ğŸ",
    "ğŸ„",
    "ğŸŒ¾",
    "ğŸ’",
    "ğŸŒ¹",
    "ğŸŒ¸",
    "ğŸŒ›",
    "ğŸŒ",
    "â­ï¸",
    "ğŸ”¥",
    "ğŸŒˆ",
    "ğŸŒ©",
    "â›„ï¸",
    "ğŸƒ",
    "ğŸ„",
    "ğŸ‰",
    "ğŸ",
    "ğŸ—",
    "ğŸ€„ï¸",
    "ğŸ­",
    "ğŸ¨",
    "ğŸ§µ",
    "ğŸª¡",
    "ğŸ§¶",
    "ğŸ¥½",
    "ğŸ¥¼",
    "ğŸ¦º",
    "ğŸ‘”",
    "ğŸ‘•",
    "ğŸ‘œ",
    "ğŸ‘‘",
]
