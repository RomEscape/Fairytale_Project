# Fairy Tale 프로젝트 개발 일지

## 2025-12-04

### 문제: 장면 데이터 생성 시 일부 문단 누락
- **증상**: 15개 문단 중 11개만 생성됨 (문단 1-4 누락)
- **원인**: 생성 중 일부 문단에서 LLM 응답 실패 (타임아웃/빈 응답)
- **해결**:
- 재시도 로직 추가 (최대 3회)
- 오류 로깅 강화 (문단별 상세 기록)
- 누락 감지 및 경고 기능 추가
- 기존 파일 자동 백업 기능 추가

### 문제: 경로 처리 오류
- **증상**: 파싱 스크립트 실행 시 파일을 찾지 못함
- **원인**: 상대 경로 처리 로직 부족
- **해결**: 여러 가능한 경로를 시도하는 자동 경로 처리 로직 추가 (`parse_data_scene.py`, `parse_data_dialogue.py`)

### 확인 사항: trainable-agents 플로우 일치성
- **결과**: 구조적으로 완전히 일치 확인
- 프로필 문단 단위로 장면 생성 완료 
- 장면 데이터 기반으로 대화 생성 완료
- 각 장면에 profile 필드 포함 완료

### 문제: 장면 데이터의 배경(background)이 너무 단순하게 생성됨
- **증상**: 배경 필드가 한두 문장으로만 구성되어 상세한 묘사 부족
- **원인**: 프롬프트에 "세부사항은 말하지 마세요"라는 제약이 포함됨
- **해결**: 프롬프트 개선
- "세부사항은 말하지 마세요" 제거
- 배경 작성 시 포함해야 할 구체적 요소 명시 (시각적 묘사, 환경 세부사항, 분위기, 감정적 톤 등)
- 상세한 배경 묘사 예시 추가
- 적용 파일: `prompt_scene_generation_snow_white_korean.txt`, `prompt_agent_scene_korean.txt`

### 문제: 장면 유형이 동화 캐릭터에 맞지 않음
- **증상**: "토론, 논쟁, 연설" 같은 형식적 유형이 동화 이야기와 어울리지 않음
- **원인**: 일반적인 장면 유형을 사용하여 동화 특성 반영 부족
- **해결**: 동화 캐릭터에 적합한 장면 유형으로 변경
- 기존: 대화, 토론, 논쟁, 연설
- 변경: 대화, 만남, 일상, 위기, 모험, 회상, 노래/춤, 자연 감상, 위로/격려, 발견
- 적용 파일: `prompt_scene_generation_snow_white_korean.txt`, `prompt_agent_scene_korean.txt`

### 작업: 백설공주 특화 프롬프트 파일 복구 및 최적화
- **상황**: `prompt_scene_generation_snow_white_korean.txt` 파일이 삭제됨
- **해결**: 백설공주 캐릭터 정보가 포함된 특화 프롬프트 파일 재생성
- 백설공주 캐릭터 배경 정보 포함 (성격, 기본 배경)
- 상세한 배경 묘사 지침 포함
- 동화에 맞는 장면 유형 사용
- 일반 프롬프트(`prompt_agent_scene_korean.txt`)는 fallback으로 유지
- 프롬프트 로딩 로직: 캐릭터별 특화 프롬프트 우선 사용, 없으면 일반 프롬프트 사용
- **최적화**: 창의성 향상을 위해 "주요 경험" 구체적 목록 제거
- 고정된 경험 목록 대신 프로필 문단 맥락에 집중하도록 변경
- "창의적으로 상상" 지시 추가하여 다양성 확보
- 각 프로필 문단마다 20개씩 창의적인 장면 생성 가능하도록 개선

### 문제: Ollama가 빈 응답 또는 매우 짧은 응답 반환
- **증상**: 특정 문단(예: 문단 4)에서 Ollama가 빈 응답(0자) 또는 매우 짧은 응답(46자)만 반환
- **원인 분석**:
- Ollama API는 HTTP 200으로 정상 응답하지만 생성된 텍스트가 비어있음
- 재시도해도 계속 빈 응답 반환
- 모델이 특정 프롬프트/문단에 대해 응답 생성 실패 가능성
- **해결**:
- 빈 응답에 대한 상세 로깅 추가 (Ollama API 응답 전체 확인, 에러 필드 확인)
- 재시도 간 exponential backoff 대기 시간 추가 (2초, 4초, 최대 30초)
- 빈 응답 시 실제 응답 내용과 프롬프트 정보 로깅 강화
- 적용 파일: `llm_utils.py`, `data_generator.py`

### 개선: 데이터 생성 모델을 Exaone 3.5로 변경
- **목적**: 한국어 데이터 생성 품질 향상
- **변경사항**:
- 데이터 생성용 기본 모델을 `qwen3:4b`에서 `exaone3.5:2.4b`로 변경
- Exaone 3.5는 한국어에 특화된 모델로 장면/대화 데이터 생성 시 더 나은 품질 기대
- Fine-tuning용 베이스 모델은 여전히 `Qwen/Qwen3-4B-Instruct` 사용 (변경 없음)
- **적용 파일**:
- `run_gen_data.py`: 기본 모델 변경
- `scripts/data_generator.py`: SceneDataGenerator, DialogueDataGenerator 기본값 변경
- `README.md`: Exaone 3.5 설치 방법 및 모델 구분 설명 업데이트
- **모델 구분 명확화**:
- **데이터 생성용**: Exaone 3.5 (한국어 특화, Ollama)
- **Fine-tuning용**: Qwen3-4B (HuggingFace)
- **추론용**: Fine-tuning된 Qwen3-4B (Ollama 등록)

### 문제: Exaone 모델이 장면 생성 시 축약 표현 사용
- **증상**: 
- 15개 문단 모두 생성되었지만, 일부 항목에서 "장면 11~20: 각각의 장면은..." 같은 축약 표현 사용
- 실제로는 11개만 생성하고 나머지는 설명만 추가하여 파싱 시 80개 장면만 추출됨 (기대: 300개)
- Qwen3은 대부분 20개씩 생성하여 143개 장면 추출 (Qwen3도 일부 문단 누락으로 220개 미만)
- **원인**: 
- LLM이 토큰 제한이나 효율성을 이유로 장면을 축약하여 생성
- 프롬프트에 축약 표현 금지 명시가 부족
- **해결**:
- 프롬프트에 명확한 축약 금지 지시 추가
- "절대로 '장면 11~20' 같은 축약 표현을 사용하지 마세요"
- "각 장면을 반드시 개별적으로 모두 작성해야 합니다"
- "설명만 추가하는 것은 금지됩니다"
- 적용 파일: `prompt_scene_generation_snow_white_korean.txt`, `prompt_agent_scene_korean.txt`
- **비교 결과**:
- **Qwen3**: 총 143개 장면 생성 (평균 13개/문단), 배경 평균 95.7자
- **Exaone3.5**: 총 80개 장면 생성 (평균 5.3개/문단), 배경 평균 111.1자 (더 상세)
- **결론**: Exaone이 생성한 장면의 품질(배경 상세도)은 더 우수하지만, 축약 문제로 수량이 부족함
- **다음 단계**: 프롬프트 개선 후 재생성하여 20개씩 모두 생성되는지 확인 필요

### 작업: Exaone3.5 모델 버전 비교 및 최종 선택
- **비교 대상**: Qwen3, Exaone3.5 ver1, Exaone3.5 ver2
- **비교 결과**:
- **Qwen3**: 143개 장면, 배경 평균 95.7자, 종합 점수 61.6점
- **Exaone3.5 ver1**: 80개 장면, 배경 평균 111.1자, 종합 점수 55.7점
- **Exaone3.5 ver2**: 189개 장면, 배경 평균 117.2자, 종합 점수 72.0점 
- **결론**: Exaone3.5 ver2가 가장 우수
- 배경 상세도가 가장 높음 (117.2자, 불완전한 배경 0개)
- 생성량이 ver1 대비 2배 이상 증가 (189개)
- 종합 점수 1위 (품질 36.8점, 수량 18.9점, 다양성 16.3점)
- **문제점**: 여전히 기대값(300개 = 15개 프로필 × 20개씩)에 미치지 못함 (63%)
- **다음 단계**: 프롬프트 개선하여 300개 완전 생성 목표

### 개선: 프롬프트 강화 및 검증 로직 개선 (300개 완전 생성 목표)
- **목적**: 15개 프로필 × 20개씩 = 300개 장면 완전 생성
- **프롬프트 개선** (`prompt_scene_generation_snow_white_korean.txt`, `prompt_agent_scene_korean.txt`):
- 20개 장면 정확히 생성 지시 강화
- "반드시 정확히 20개의 장면을 생성해야 합니다 (장면 1, 장면 2, ..., 장면 19, 장면 20)"
- "하나도 빠짐없이 모두 생성해야 합니다"
- 축약 금지 규칙 명확화
- 범위 표시 금지 ("장면 11~20")
- 설명만 추가 금지
- 각 장면을 개별적으로 모두 작성
- 완성 체크리스트 추가
- [ ] 장면 1부터 장면 20까지 모두 있는가?
- [ ] 각 장면에 유형, 장소, 배경이 모두 있는가?
- [ ] 축약 표현 없이 모두 개별적으로 작성되었는가?
- [ ] 총 20개인가?
- **검증 로직 개선** (`data_generator.py`):
- 실제 장면 개수 검증 (정규표현식으로 "장면 1" ~ "장면 20" 모두 확인)
- 축약 표현 자동 감지 및 경고
- 누락된 장면 번호 표시 (예: "missing: [2, 5, 10]")
- 20개 모두 생성되지 않으면 재시도
- **토큰 수 증가**:
- max_tokens: 8192 → 12288 (20개 장면 완전 생성에 충분한 토큰 확보)
- **예상 효과**:
- 모든 프로필 문단 처리 (15개)
- 각 프로필당 정확히 20개씩 생성
- 총 300개 장면 완전 생성 달성

### 문제: ver3의 빈 배경 발생 및 프로필 데이터 불균형
- **증상**:
- ver3에서 빈 배경 20개 발생 (ver2는 0개)
- 프로필 문단 길이 편차가 심함 (120자 ~ 1378자)
- 문단 6이 1378자로 과도하게 긴 상태
- **원인 분석**:
1. **파싱 로직 문제**: ver3에서 LLM이 배경을 마크다운 리스트 형식(`* **배경:** \n * 내용...`)으로 생성했으나, 파싱 로직은 첫 줄만 추출하여 `**`만 파싱됨
2. **프로필 데이터 불균형**: 문단 길이 편차가 커서 일부 문단에서 생성 실패 가능성
- **해결**:
- **파싱 로직 개선** (`parse_data_scene.py`):
- 여러 줄에 걸친 배경 처리 지원
- 마크다운 리스트 형식(`*`, `**`) 자동 제거
- 배경 필드 다음 줄부터 다음 필드/장면 시작 전까지 모두 수집
- 빈 배경 발생 방지
- **프로필 데이터 재구성** (`wiki_snow_white-korean.txt`):
- 가장 긴 문단(1378자)을 자연스럽게 2개로 분할
- 문단 6: 난쟁이들이 돌아와 백설공주를 발견 (809자)
- 문단 7: 백설공주와 난쟁이들의 대화와 함께 살기 시작 (609자)
- 총 15개 → 16개 문단 (기대 장면: 320개)
- 최대 길이: 1378자 → 809자 (편차 감소)
- 평균 길이: 583자 → 549자
- **프로필 로드 로직 개선** (`llm_utils.py`):
- 파일 구조 명확화 (1번 줄: 캐릭터 이름, 3번 줄부터: 프로필 문단들)
- 하드코딩 제거: 1-2번 줄을 제외하고 3번 줄부터 끝까지 유효한 모든 문단 자동 읽기
- 유연한 처리: 문단 개수에 따라 자동으로 n × 20개 장면 생성
- 로드된 문단 수와 기대 장면 수 명확히 로깅
- **프롬프트 구조 개선**:
- "마크다운 형식 사용하지 말기" 지시를 "매우 중요 - 반드시 준수해야 할 규칙" 섹션 안으로 이동
- 규칙 5번으로 배치하여 더 명확하게 표시
- 적용 파일: `prompt_agent_scene_korean.txt`, `prompt_scene_generation_snow_white_korean.txt`

### 품질 분석: Exaone3.5 ver4 (2025-12-04)
- **종합 점수**: 85.5점 (4개 버전 중 1위)
- ver4: 85.5점
- ver2: 82.8점
- Qwen3: 78.4점
- ver3: 75.6점
- **주요 성과**:
- 빈 배경 문제 완전 해결 (ver3: 20개 → ver4: 0개) - 파싱 로직 개선 성공
- 각 프로필당 정확히 20개씩 생성 (평균 20.0개)
- 배경 품질 우수 (평균 108.1자)
- 불완전한 배경 비율 매우 낮음 (0.5%, 1개만)
- 장면 유형 다양성 우수 (12가지 유형)
- **상세 통계**:
- 총 장면 수: 220개 (기대: 320개, 생성률: 68.8%)
- 프로필 수: 11개 (기대: 16개)
- 배경 평균 길이: 108.1자 (중간값: 97.0자)
- 배경 길이 분포: 짧음(55.9%), 중간(30.0%), 긴(8.2%), 매우 긴(5.5%)
- 빈 배경: 0개 (0.0%)
- 매우 짧은 배경(<50자): 1개 (0.5%)
- **개선 필요 사항**:
- 프로필 처리 부족: 11/16개 프로필만 처리 (5개 누락)
- 누락된 프로필: 1, 4, 6, 10, 12번
- 이로 인해 100개 장면 부족 (5 × 20개)
- 장면 수 부족: 220/320개 (목표 대비 100개 부족, 31.2%)
- **버전 비교**:
- 총 장면 수: ver4(220) > ver2(189) > ver3(180) > Qwen3(143)
- 배경 평균 길이: ver2(117.2자) > ver4(108.1자) > ver3(96.2자) > Qwen3(95.7자)
- 빈 배경: ver4(0개) = ver2(0개) = Qwen3(0개) > ver3(20개)
- 불완전한 배경: ver2(0개) < ver4(1개) < Qwen3(9개) < ver3(29개)
- **결론**: ver4는 빈 배경 문제가 완전히 해결되고, 각 프로필당 정확히 20개씩 생성되며, 종합 점수가 가장 높은 우수한 버전입니다. 다만 프로필 5개가 누락되어 100개 장면이 부족하므로, 누락 원인 분석 및 재생성이 필요합니다.

## 2025-12-05

### 파서 개선: 마크다운 형식 지원
- **문제**: 
- 모든 16개 프로필에 대해 장면 생성은 성공했지만, 파싱 단계에서 5개 프로필(1, 4, 6, 10, 12번)의 장면 추출이 실패
- 원인: LLM이 마크다운 형식(`**유형**:`, `*유형:`, `### 장면 1:` 등)으로 생성했으나, 파서가 단순 텍스트 형식(`유형:`)만 인식
- 결과: 100개 장면 손실 (5개 프로필 × 20개)
- **해결**:
- **파서 개선** (`parse_data_scene.py`):
- 마크다운 제거 함수 추가: `clean_markdown()` - 헤더(`###`, `##`), 볼드(`**text**`), 리스트(`*`) 등 제거
- 정규식 기반 필드 추출: `extract_field_value()` - 다양한 필드 형식 패턴 지원
- 단순 텍스트: `유형: 대화`
- 마크다운 볼드: `**유형**: 대화`
- 마크다운 리스트: `*유형: 대화`
- 여러 줄 배경 추출 개선: `extract_background_multiline()` - 마크다운 포함 여러 줄 처리
- 장면 분할 로직 개선: 정규식으로 다양한 헤더 형식 지원 (`### 장면 1:`, `## 장면 1:`, `**장면 1:**` 등)
- **결과**:
- 모든 16개 프로필에서 각 20개씩 총 320개 장면 추출 성공 (100% 성공률)
- 이전에 실패했던 프로필 1, 4, 6, 10, 12번도 모두 성공적으로 파싱
- 빈 배경 0개, 불완전한 장면 0개
- Invalid scenes: {} (빈 딕셔너리 - 모든 장면이 정상적으로 파싱됨)

### 파서 출력 경로 개선: 파싱 날짜 기준
- **문제**: 
- 파싱된 파일이 원본 데이터 생성 날짜 디렉토리에 저장되어 파일 생성 날짜와 디렉토리 날짜가 불일치
- 예: 12월 5일에 파싱했는데 `processed/2025-12-04/` 디렉토리에 저장됨
- 혼란 및 날짜 불일치 문제
- **해결**:
- **출력 경로 로직 변경**: 원본 데이터 날짜 대신 **파싱 실행 날짜**를 디렉토리로 사용
- 적용 파일: `parse_data_scene.py`, `parse_data_dialogue.py`
- 변경 전: `processed/{원본데이터날짜}/generated_agent_scene_{character}-{language}.json`
- 변경 후: `processed/{파싱날짜}/generated_agent_scene_{character}-{language}.json`
- 예: 12월 5일에 파싱하면 → `processed/2025-12-05/generated_agent_scene_snow_white-korean.json`
- **결과**:
- 파일 생성 날짜와 디렉토리 날짜가 일치하여 명확함
- 파싱 시점을 기준으로 파일 관리 가능

### 대화 데이터 생성 개선: trainable-agents 방식 적용
- **목적**: trainable-agents 프로젝트의 README_Korean.md를 참고하여 동일한 방식으로 대화 데이터 생성 구성
- **변경사항**:
- **`--data-path` 옵션 추가** (`run_gen_data.py`):
- 대화 생성 시 사용할 처리된 장면 데이터 파일 경로를 명시적으로 지정할 수 있는 옵션 추가
- trainable-agents의 `--data_path` 옵션과 동일한 기능
- **자동 로드 로직 유지** (`DialogueDataGenerator.load_scenes()`):
- `--data-path` 옵션을 지정하지 않으면 자동으로 `processed/` 디렉토리에서 가장 최근 날짜의 장면 데이터 파일을 찾아 사용
- 명시적 경로가 제공되면 해당 파일을 우선 사용
- **장면 데이터 참고 방식**:
- 파싱된 장면 데이터 (`processed/generated_agent_scene_{character}-{language}.json`) 사용
- 각 장면의 `type`, `location`, `background`, `profile` 정보를 기반으로 대화 생성
- trainable-agents와 동일한 구조
- **사용 방법**:
- 방법 1 (권장): 자동으로 가장 최근 장면 데이터 사용
```bash
python fairy_tale/run_gen_data.py --character snow_white --prompt-name gen_dialogue
```
- 방법 2: 특정 장면 데이터 파일 명시적으로 지정 (trainable-agents 방식)
```bash
python fairy_tale/run_gen_data.py --character snow_white --prompt-name gen_dialogue \
--data-path fairy_tale/processed/2025-12-05/generated_agent_scene_snow_white-korean.json
```
- **결과**:
- trainable-agents 방식과 동일하게 구성됨
- 전처리된 장면 데이터를 참고하도록 올바르게 구성됨
- 명시적 경로 지정 및 자동 탐색 모두 지원
- README에 상세한 사용법 추가

### 대화 데이터 파서 전면 개선
- **문제**: 
- 원본 데이터 320개 중 1개만 파싱됨
- 파싱 결과에서 캐릭터 이름이 모호함 (캐릭터1, 캐릭터3 등)
- 마크다운 형식이 제대로 제거되지 않음
- 사용자 질문이 포함되지 않음
- **원인**:
- 파서가 다양한 원본 데이터 형식을 제대로 처리하지 못함
- `**백설공주 (말하기)**` 형식
- `## 백설공주 (말하기)` 형식
- `(생각)` 부분 별도 처리 필요
- 캐릭터 이름 정규화 로직 부족
- 사용자 질문 추가 로직 없음
- **해결**:
- **파서 전면 재작성** (`parse_data_dialogue.py`):
- JSONL 파일 로드 방식 개선 (한 줄씩 읽기)
- 다양한 대화 형식 패턴 지원:
- `**캐릭터 (행동)**` 형식
- `## 캐릭터 (행동)` 형식
- 일반 텍스트 형식
- 배경 추출 로직 개선 (다양한 마크다운 형식 지원)
- 마크다운 완전 제거 로직 (`clean_markdown()` 함수)
- **캐릭터 이름 정규화**:
- 모호한 이름 (캐릭터1, 캐릭터3 등)을 명확한 이름으로 변환
- 백설공주, 왕자, 계모, 난쟁이1 등 명확한 이름 사용
- 동물/자연물 이름 유지 (새, 나비, 벌 등)
- **사용자 질문 추가**:
- 페르소나 LLM 대화 형식을 위해 사용자가 백설공주에게 질문하는 턴 자동 추가
- 장면 정보(장소, 유형, 배경)를 바탕으로 자연스러운 질문 생성
- trainable-agents 형식에 맞춘 사용자-캐릭터 대화 구조
- **마크다운 완전 제거**:
- `**`, `##`, `###` 등 모든 마크다운 태그 제거
- 깔끔한 텍스트 형태로 변환
- **결과**:
- 원본 데이터 320개 모두 파싱 가능하도록 개선
- 캐릭터 이름 명확화
- 마크다운 제거
- 사용자 질문 포함된 페르소나 LLM 대화 형식 구성
- trainable-agents 방식과 호환되는 구조

### 대화 생성 프롬프트 개선: 캐릭터 이름 명확화
- **문제**: 
- 대화 생성 프롬프트 예시에서 "캐릭터2" 같은 모호한 이름을 사용
- LLM이 실제 대화 생성 시 "캐릭터1", "캐릭터3" 같은 모호한 이름을 생성
- 파싱 후에도 불명확한 캐릭터 이름이 남아있음
- **원인**:
- 프롬프트 예시 형식이 모호한 이름을 사용하도록 유도
- 백설공주 동화의 구체적인 캐릭터 이름에 대한 명시적 지시 부족
- **해결**:
- **프롬프트 개선** (`prompt_agent_dialogue_korean.txt`):
- 예시 형식에서 "캐릭터2" 같은 모호한 표현 제거
- 백설공주 동화에 맞는 구체적인 캐릭터 이름으로 변경
- 주인공: 백설공주
- 주요 인물: 왕자, 계모, 왕비, 사냥꾼
- 난쟁이들: 난쟁이1, 난쟁이2, ... 난쟁이7 (또는 난쟁이들)
- 동물/자연물: 새, 나비, 벌, 나무, 꽃 등
- 캐릭터 이름 명확화 규칙 추가 (규칙 7번)
- 모호한 이름(캐릭터1, 캐릭터2, 캐릭터A 등) 절대 사용 금지 명시
- 구체적인 캐릭터 이름 사용 필수
- 예시 형식에 실제 캐릭터 이름 사용 (백설공주, 난쟁이1, 왕자 등)
- **사용자 대화 포함**: 페르소나 LLM의 목적에 맞춰 사용자가 백설공주와 대화하는 형식 추가
- 규칙 6번에 사용자-캐릭터 대화 형식 명시
- 예시 형식에 사용자 (말하기) 포함
- 사용자가 백설공주에게 질문하고, 백설공주가 응답하는 자연스러운 대화 구조
- 캐릭터 목록에 "사용자" 명시
- **구체적인 질문 예시 제공**: 모호한 질문 대신 백설공주 동화의 주요 사건과 경험에 대한 구체적인 질문 예시 제공
- "처음 계모를 마주했을 때 무서웠어?"
- "난쟁이들과 함께 살면서 가장 즐거웠던 일은 뭐였어?"
- "왕자를 처음 만났을 때 어떤 느낌이었어?"
- "사냥꾼이 너를 살려준 일에 대해서는 어떻게 생각해?"
- LLM이 실제 사용자가 할 수 있는 구체적이고 자연스러운 질문을 생성하도록 유도
- **결과**:
- LLM이 구체적인 캐릭터 이름을 사용하여 대화 생성
- 사용자-백설공주 대화 형식 포함
- 구체적이고 자연스러운 사용자 질문 생성
- 파싱 후에도 명확한 캐릭터 이름 유지
- trainable-agents 형식과 호환되는 구조

### 문제: 대화 데이터 파서 영향 요소 분석
- **문제**: 
- 320개 대화 데이터 중 약 9-10개 항목 파싱 실패 (현재 성공률 96-97%, 310-311/320)
- 배경 추출 실패: 12개 항목이 배경 패턴을 찾지 못해 배경 추출 실패
- 대화 턴이 없는 항목: 3개 (Entry 3: 290자, Entry 206, Entry 225: 265자) - 배경만 있고 대화 턴이 없어 파싱 실패
- 매우 짧은 응답: 300자 미만 2개, 500자 미만 3개 항목
- **원인**:
- **배경 형식 다양성 및 미지원 패턴**:
- 현재 파서는 `**배경:**` (66%), `## 배경:` (16%) 패턴만 지원
- 약 10개 이상의 항목이 `## 배경` (콜론 없음) 형식 사용 - 파서가 인식하지 못함
- Entry 3, 20, 21, 22, 53, 56, 69, 70, 73, 79 등이 `## 배경` 형식 사용
- 배경 패턴이 전혀 없는 항목도 12개 존재
- **대화 턴 부재 문제**:
- LLM이 대화를 생성하지 않고 배경만 생성한 경우 (Entry 3, 225)
- 매우 짧은 응답(265-290자)으로 인해 대화 턴 생성 실패
- 대화 턴이 없는 경우를 처리하는 fallback 로직 부재
- **clean_markdown 함수 안정성**:
- `while '**' in text:` 루프가 특정 마크다운 패턴(`***text***`, `****` 등)에서 무한 루프 가능성
- 최대 반복 횟수 제한 없음
- **해결**:
- **배경 추출 로직 개선** (`parse_data_dialogue.py`의 `extract_background` 함수):
- `## 배경` (콜론 없음) 패턴 지원 추가
- `(r'##\s*배경\s*\n\s*(.*?)(?=\*\*|##|$)', re.DOTALL)` 패턴 추가
- 헤더 다음 줄이 배경인 경우 처리 로직 추가
- 다양한 배경 형식 패턴 확장 지원
- **대화 턴 부재 시 fallback 처리** (`parse_dialogue_info` 함수):
- 배경만 있고 대화 턴이 없는 경우, 배경을 기반으로 최소한의 대화 구조 생성
- 장면 정보(장소, 유형)를 바탕으로 사용자 질문 생성
- 배경 텍스트를 캐릭터 응답으로 활용
- **clean_markdown 함수 안정성 향상**:
- 최대 반복 횟수 제한 추가 (max_iterations = 10)
- 변경이 없으면 조기 종료 로직 추가
- 무한 루프 방지 및 안정성 보장
- **결과**:
- 파싱 성공률 99-100%까지 향상 예상 (318-320/320)
- 다양한 배경 형식 지원으로 데이터 손실 최소화
- 대화 턴이 없는 항목도 최소한의 대화 구조로 변환 가능
- clean_markdown 함수 안정성 향상으로 예외 상황 방지

### 문제: 대화 데이터 파서 마크다운 제거 실패 및 무한 루프
- **문제**: 
- 파싱된 대화 데이터에서 마크다운(`**`, `*`)이 제대로 제거되지 않음
- Entry 1부터 대부분의 항목에서 `**사용자`, `**백설공주`, `**\n텍스트` 같은 마크다운이 role과 content에 남아있음
- Entry 86에서 일부 대화 턴 내용이 `**\n(...)`만 남아있어 너무 짧음
- 파싱 실행 중 무한 루프 발생 (3분 35초 이상 CPU 99.9% 사용)
- **원인**:
- **clean_markdown 함수의 무한 루프**:
- `while '**' in text:` 루프가 특정 패턴에서 무한 반복
- 최대 반복 횟수 제한이 없어 변경이 없어도 계속 실행
- **마크다운 패턴 제거 불완전**:
- `**text**` 패턴만 처리하고 `**사용자`, `**\n텍스트` 같은 불완전한 패턴 처리 실패
- 패턴 매칭 시 중복 매칭 발생 (`**사용자 (말하기)**`가 두 패턴에 동시 매칭)
- content 추출 후 추가 마크다운 제거 로직 부재
- **패턴 매칭 중복 문제**:
- 여러 패턴이 동일한 텍스트에 매칭되어 `**사용자` 같은 잘못된 role 추출
- 패턴 처리 순서가 잘못되어 일반 패턴이 구체적 패턴보다 먼저 매칭됨
- **해결**:
- **clean_markdown 함수 무한 루프 방지** (`parse_data_dialogue.py`):
- 최대 반복 횟수 제한 추가 (max_iterations = 10)
- 변경이 없으면 조기 종료 로직 추가
- 다양한 마크다운 패턴 처리 개선 (`**text**`, `**\n`, `***` 등)
- **패턴 매칭 로직 개선** (`parse_dialogue_turns` 함수):
- 패턴을 순차적으로 처리 (더 구체적인 패턴부터)
- 중복 매칭 방지: 이미 사용된 범위와 겹치지 않는 경우만 추가
- 패턴 처리 순서: `**캐릭터 (행동)**` → `## 캐릭터 (행동)` → 일반 패턴
- **마크다운 제거 강화**:
- role 정리: 시작/끝 부분의 `**`, `*` 제거
- content 정리: `**\n` 패턴, 모든 `**` 패턴, `*` 리스트 마커 제거
- 여러 단계로 반복 제거하여 완전히 제거
- **결과**:
- 파싱 실행 시 무한 루프 문제 해결
- role과 content에서 모든 마크다운(`**`, `*`, `#`) 완전 제거
- 패턴 매칭 정확도 향상으로 올바른 role 추출
- 깔끔한 대화 데이터 생성 가능

### 문제: 대화 데이터 파서에서 # 마크다운 제거 실패
- **문제**: 
- 파싱된 대화 데이터의 role에 `# 백설공주`, `# 사용자` 같은 `#` 마크다운이 남아있음
- 총 898개 대화 턴에서 `#` 마크다운 발견 (`# 백설공주` 604개, `# 사용자` 269개 등)
- Entry 5, 11, 20, 30, 36, 37, 40, 46, 51, 54 등 다수 항목에 영향
- **원인**:
- `clean_markdown` 함수가 `##+` 패턴(2개 이상)만 제거하고 단일 `#`는 제거하지 않음
- role 정리 로직에서 `#` 제거 로직 부재
- `## 캐릭터 (행동)` 패턴 매칭 시 role 추출 과정에서 `#`가 포함되어 남아있음
- **해결**:
- **clean_markdown 함수 개선** (`parse_data_dialogue.py`):
- 단일 `#` 제거 로직 추가: `re.sub(r'^#\s+', '', text, flags=re.MULTILINE)`
- `##+` 패턴 제거 후 단일 `#`도 제거하도록 처리
- **role 정리 로직 강화**:
- role 정리 시 `#` 패턴 제거 추가: `re.sub(r'^#+\s*', '', role)`
- `#`, `##`, `###` 등 모든 헤더 마크다운 제거
- **결과**:
- role에서 모든 `#` 마크다운 완전 제거
- 깔끔한 캐릭터 이름만 남도록 개선

## 2025-12-05

### 문제: Fine-tuning 데이터 전처리 부족으로 인한 품질 저하
- **문제**:
- 생성된 대화 데이터에 영어 단어가 섞여있음 (예: "pervasive한", "Blossom처럼", "Users", "fearfully" 등)
- 따옴표와 줄바꿈이 이상하게 조합된 패턴 존재 (예: `따옴표\n\n`)
- 연속된 줄바꿈(`\n\n\n`)이 정리되지 않음
- 이런 문제들이 Fine-tuning 데이터 품질을 저하시킬 수 있음
- **원인**:
- `convert_prompt_data.py`에 데이터 전처리 함수가 없음
- LLM 생성 시 영어 단어가 자연스럽게 포함되는 경우가 있음
- 마크다운 파싱 과정에서 따옴표와 줄바꿈이 제대로 정리되지 않음
- **해결**:
- **clean_text 함수 추가** (`convert_prompt_data.py`):
- 따옴표와 줄바꿈 조합 패턴 제거: `["\'"]\s*\n\s*\n` 등
- 연속된 줄바꿈 정리: 최대 2개까지 허용 (`\n\n`)
- 영어 단어 제거 로직 추가:
- 3자 이상 영어 단어 제거 (일반 전치사/관사 등은 제외)
- 영어 단어 + 한글 조사 패턴 제거 (예: "pervasive한" → 완전 제거)
- 혼자 남은 조사도 제거 (예: "처럼"만 남은 경우)
- 공백 정리 및 텍스트 정돈
- **데이터 변환 시 모든 필드에 clean_text 적용**:
- background, location 필드 정리
- dialogue의 role, action, content 필드 정리
- 빈 데이터는 건너뛰기
- **결과**:
- 영어 단어가 완전히 제거된 깔끔한 한국어 데이터 생성
- 이상한 줄바꿈 패턴 제거
- Fine-tuning 데이터 품질 향상

### 문제: 학습 진행 상황 확인 불가
- **문제**:
- QLoRA Fine-tuning 시 학습 진행 상황을 확인하기 어려움
- 얼마나 걸렸는지, 얼마나 남았는지 알 수 없음
- **원인**:
- TrainingArguments에 tqdm 설정이 명시되지 않음
- 학습 진행 상황 표시가 비활성화될 수 있음
- **해결**:
- **TrainingArguments에 tqdm 활성화** (`train_qlora.py`):
- `disable_tqdm=False` 명시적 설정 추가
- 학습 진행 상황이 자동으로 표시되도록 보장
- **학습 정보 로깅 강화**:
- 총 샘플 수, 예상 총 스텝 수 출력
- tqdm 진행 표시가 활성화되었음을 명시
- **결과**:
- 학습 진행 상황이 실시간으로 표시됨
- 남은 시간과 진행률을 쉽게 확인 가능

### 작업: Ollama 연결 및 MCP 호환성 문서화
- **목적**: 사용자 질문에 대한 명확한 답변 및 사용 가이드 제공
- **작업 내용**:
- **README.md에 상세 설명 추가**:
- Qwen3-4B Instruct 모델이 Ollama에 자연스럽게 연결되는 방법 명시
- Ollama에서 qwen3 모델을 별도로 다운받을 필요가 없음을 강조
- 병합된 로컬 모델만 사용하면 됨을 명확히 설명
- MCP와 FT된 Ollama 모델의 완전한 호환성 설명
- MCP가 Agent 레벨에서 처리되므로 LLM 모델 종류와 무관하게 작동함을 명시
- **Modelfile 생성 로직 개선** (`merge_and_export_ollama.py`):
- 상대 경로 대신 **절대 경로 사용**으로 변경
- Ollama가 로컬 모델을 정확히 인식하도록 개선
- Modelfile에 모델 정보 및 MCP 호환성 주석 추가
- **결과**:
- 사용자가 Ollama 연결 방식을 명확히 이해 가능
- MCP 사용에 대한 우려 해소
- Modelfile이 더 안정적으로 작동

### 작업: 데이터 품질 분석
- **목적**: 생성된 대화 데이터의 품질 확인 및 문제점 파악
- **분석 결과**:
- 총 항목: 311개 (기대 320개, 완성도 97.2%)
- 평균 배경 길이: 280.0자 (79.1%가 200자 이상)
- 평균 대화 턴 수: 10.95개 (64.3%가 9-12개)
- 평균 대화 턴 내용 길이: 131.2자
- 문제 항목: 2개 (Entry 86: 짧은 내용, Entry 108: 대화 턴 부족)
- 마크다운 완전 제거 확인
- **종합 평가**: 95/100점 (매우 우수)

### 문제: 대화 데이터에 "따옴표" 리터럴 텍스트 포함
- **문제**:
- 생성된 대화 데이터에 "따옴표\n\n" 같은 리터럴 텍스트가 포함되어 있음
- Entry 2 등에서 4개 대화 턴에 "따옴표\n\n" 패턴 발견
- 실제 대화 내용 앞에 "따옴표"라는 단어가 그대로 포함되어 Fine-tuning 시 문제 발생 가능
- **원인**:
- LLM이 프롬프트의 "(따옴표나 다른 문장부호 없이)" 표현을 오해하여 "따옴표"라는 단어 자체를 생성
- 프롬프트에 "따옴표"라는 단어 자체를 생성하지 말라는 명확한 지시 부재
- 전처리 함수에 "따옴표" 리터럴 텍스트 제거 로직 없음
- **해결**:
- **프롬프트 강화** (`prompt_agent_dialogue_korean.txt`):
- "따옴표"라는 단어 자체를 절대 생성하지 말라는 명확한 지시 추가
- 실제 따옴표 문자("", '')도 사용하지 말라는 지시 추가
- 대화 내용은 예시처럼 바로 시작하라는 지시 추가
- 출력 형식 섹션(10번)과 주의사항 섹션에 명시
- **전처리 함수 개선** (`convert_prompt_data.py`):
- "따옴표\n\n", "따옴표\n", "따옴표" 리터럴 텍스트 제거 로직 추가
- 정규식 패턴으로 모든 위치의 "따옴표" 단어 제거
- **결과**:
- "따옴표" 리터럴 텍스트가 완전히 제거된 깔끔한 대화 데이터 생성
- LLM이 향후 "따옴표"를 생성하지 않도록 프롬프트 명확화
- Fine-tuning 데이터 품질 향상

### 문제: 파서에서 "따옴표\n\n" 패턴 제거 누락
- **문제**:
- `parse_data_dialogue.py`에서 "따옴표\n\n" 패턴이 제거되지 않아 파싱된 데이터에 남아있음
- Entry 2 등에서 "따옴표\n\n햇살은..." 같은 패턴이 content 필드에 그대로 포함됨
- **원인**:
- `clean_markdown` 함수와 content 정리 로직에 "따옴표" 리터럴 텍스트 제거 로직이 없음
- `convert_prompt_data.py`의 전처리와 별도로 파서에서도 제거해야 함
- **해결**:
- **parse_data_dialogue.py에 간단한 제거 로직 추가**:
- `clean_markdown` 함수 끝부분에 "따옴표" 패턴 제거 추가
- `parse_dialogue_turns` 함수의 content 정리 부분에 "따옴표\n\n" 패턴 제거 추가
- 정규식으로 정확하게 패턴 매칭: `^따옴표\s*\n\s*\n`, `^따옴표\s*\n`, `^따옴표\s+`
- 복잡한 기능 추가 없이 정확한 패턴 제거만 구현
- **결과**:
- 파싱된 데이터에서 "따옴표\n\n" 패턴 완전 제거
- 깔끔하고 정확한 대화 데이터 생성

### 문제: 잘못된 HuggingFace 모델 이름으로 인한 학습 실패
- **문제**:
- `train_character.py` 실행 시 `RepositoryNotFoundError: 404 Client Error` 발생
- `Qwen/Qwen3-4B-Instruct` 모델이 HuggingFace에서 찾을 수 없음
- 에러 메시지: "Qwen/Qwen3-4B-Instruct is not a local folder and is not a valid model identifier"
- **원인**:
- `train_character.py`와 README에서 잘못된 모델 이름 사용
- 실제 올바른 모델 이름은 `Qwen/Qwen3-4B-Instruct-2507` (2507 버전 포함)
- HuggingFace에 해당 모델 이름으로 존재하지 않음
- **해결**:
- **train_character.py 기본값 수정**:
- `default="Qwen/Qwen3-4B-Instruct"` → `default="Qwen/Qwen3-4B-Instruct-2507"`
- **README 모든 예시 명령어 수정**:
- 6단계, 7단계 예시 명령어의 `--base-model` 값 변경
- 모델 구분 섹션의 예시 및 기본값 설명 수정
- 전체 파이프라인 흐름 설명 수정
- **결과**:
- 올바른 모델 이름으로 HuggingFace에서 모델 다운로드 가능
- Fine-tuning 정상 실행 가능

### 문제: Ollama가 Qwen3 아키텍처를 지원하지 않음
- **문제**:
- 8단계에서 `ollama create snow_white -f fairy_tale/models/snow_white/Modelfile` 실행 시 에러 발생
- 에러 메시지: `Error: unsupported architecture "Qwen3ForCausalLM"`
- Ollama가 Qwen3 모델 아키텍처를 지원하지 않음
- **원인**:
- Ollama는 로컬 HuggingFace 모델을 직접 로드할 때, 해당 아키텍처를 지원해야 함
- Qwen3는 최신 모델이라 Ollama 0.13.1 버전에서 아직 지원하지 않음
- Ollama는 GGUF 형식 또는 이미 Ollama 형식으로 패키징된 모델만 지원
- **해결** (현재 진행 중):
- **GGUF 변환**: HuggingFace 모델을 GGUF 형식으로 변환 후 Ollama에 등록
- 또는 **지원되는 모델 사용**: Qwen2.5 같은 Ollama가 지원하는 모델로 재학습
- 또는 **Ollama 업데이트 대기**: Ollama가 Qwen3 아키텍처 지원 추가를 기다림
- **참고**:
- Ollama 버전: 0.13.1
- Qwen3는 최신 모델이라 Ollama 지원이 아직 추가되지 않았을 수 있음
- GGUF 변환이 가장 확실한 해결 방법

### 문제: convert_prompt_data.py 경로 처리 오류
- **문제**:
- `convert_prompt_data.py` 실행 시 파일을 찾지 못함 (`FileNotFoundError`)
- 상대 경로가 여러 위치에서 실행될 때 제대로 해결되지 않음
- README 예시와 실제 경로가 불일치
- **원인**:
- `convert_prompt_data.py`에 경로 해결 로직이 없음
- `parse_data_scene.py`, `parse_data_dialogue.py`와 달리 여러 가능한 경로를 시도하지 않음
- **해결**:
- **경로 해결 로직 추가** (`convert_prompt_data.py`):
- `parse_data_dialogue.py`와 동일한 방식으로 여러 경로 시도
- 현재 디렉토리, 프로젝트 루트, fairy_tale 디렉토리 등에서 파일 자동 검색
- 파일을 찾지 못하면 시도한 경로 목록 출력
- **README 업데이트**:
- 명확한 경로 예시 추가
- 프로젝트 루트에서 실행해야 함을 명시
- 경로 자동 해결 방식 설명 추가
- **결과**:
- 다양한 위치에서 실행해도 파일을 정확히 찾을 수 있음
- 사용자가 경로를 지정하는 여러 방식 지원

### 문제: Ollama가 Qwen3 로컬 HuggingFace 모델 아키텍처를 지원하지 않음
- **문제**:
- 8단계에서 `ollama create snow_white -f fairy_tale/models/snow_white/Modelfile` 실행 시 에러 발생
- 에러 메시지: `Error: unsupported architecture "Qwen3ForCausalLM"`
- Ollama가 로컬 HuggingFace 형식의 Qwen3 모델을 직접 로드하지 못함
- **원인**:
- Ollama는 `qwen3:4b` 모델을 지원하지만 (`ollama pull qwen3:4b` 가능), 로컬 HuggingFace 형식 모델을 Modelfile의 `FROM`에서 직접 참조할 때는 해당 아키텍처를 지원해야 함
- Qwen3 (`Qwen3ForCausalLM`) 아키텍처를 Ollama가 로컬 HuggingFace 형식으로 직접 지원하지 않음
- 조사 결과: Ollama가 이미 변환된 모델(`qwen3:4b`)과 로컬 HuggingFace 형식 모델은 다른 방식으로 처리됨을 확인
- **해결**:
- **방법 1**: Qwen2.5 모델로 재학습
- `Qwen/Qwen2.5-4B-Instruct` 모델은 Ollama에서 잘 지원됨
- 6단계부터 재실행하여 Qwen2.5로 Fine-tuning
- **방법 2**: GGUF 형식으로 변환
- llama.cpp를 사용하여 HuggingFace 모델을 GGUF 형식으로 변환
- GGUF는 Ollama가 내부적으로 사용하는 형식
- 복잡하고 시간이 오래 걸릴 수 있음
- **README 업데이트**:
- Qwen3 호환성 문제와 해결 방법 상세 설명 추가
- GGUF 형식 설명 추가
- 문제 상황과 해결 방법 정리
- **결과**:
- 사용자가 문제 상황을 이해하고 해결 방법을 선택할 수 있음
- GGUF 변환 방법 문서화

### 문제: llama.cpp 빌드 방법 변경 (Makefile → CMake)
- **문제**:
- README에서 `make` 명령어로 빌드하도록 안내했으나, 최신 llama.cpp는 Makefile 대신 CMake 사용
- 에러 메시지: "Build system changed: The Makefile build has been replaced by CMake"
- CMake 구성 시 CURL 라이브러리를 찾지 못해 빌드 실패
- **원인**:
- llama.cpp가 최신 버전에서 빌드 시스템을 Makefile에서 CMake로 변경
- 시스템에 CURL 개발 라이브러리가 설치되지 않음
- README에 최신 빌드 방법이 반영되지 않음
- **해결**:
- **README 빌드 방법 업데이트**:
- `make` → CMake 빌드 방식으로 변경
- `cmake -B build -DLLAMA_CURL=OFF` 및 `cmake --build build --config Release` 명령어로 변경
- **CURL 문제 해결**:
- CURL이 없는 경우 `-DLLAMA_CURL=OFF` 옵션 추가하여 CURL 기능 비활성화
- CURL 설치 방법 안내 추가 (Ubuntu/Debian: `libcurl4-openssl-dev`, Fedora/RHEL: `libcurl-devel`)
- **빌드 경로 명확화**:
- 프로젝트 루트에서 llama.cpp 클론 위치 명확히 표시
- 이미 클론된 경우 생략 가능하도록 조건부 명령어 추가
- **용량 정보 추가**:
- llama.cpp 전체 용량 정보 추가 (약 391MB: 소스 365MB + 빌드 26MB)
- Qwen만 사용하더라도 전체 빌드가 필요함을 명시
- **결과**:
- 최신 llama.cpp 빌드 방법에 맞게 README 업데이트 완료
- CURL 없이도 빌드 가능하도록 옵션 제공
- 사용자가 빌드 과정을 정확히 따라할 수 있음

### 문제: trainable-agents와 데이터 형식 비교 분석
- **문제**:
- Fine-tuning 모델이 반복적인 응답을 생성함
- trainable-agents는 정상 작동하는데 우리 프로젝트만 문제 발생
- 학습 데이터 형식과 학습 구조 차이점 파악 필요
- **원인 분석**:
- **학습 데이터 형식**:
- 우리 프로젝트: Output이 "사용자 말하기: ..."로 시작 (사용자 질문부터 시작)
- trainable-agents: Output이 "조수미 말하기: ..."로 시작 (캐릭터 응답부터 시작)
- **학습 방식 vs 실제 사용 방식 불일치 가능성**:
- 학습 데이터: 프롬프트만 주고 → 전체 대화(사용자 질문 + 캐릭터 응답) 생성하도록 학습
- 실제 사용: System Prompt + User Input → 캐릭터 응답만 생성
- 모델이 학습한 패턴과 실제 사용 패턴이 다를 수 있음
- **데이터 통계**:
- 우리: Prompt 464자 (19.7%), Output 1896자 (80.3%)
- trainable-agents: Prompt 263자, Output 914자
- **참고**: 사용자 질문이 먼저 오는 것은 일반적인 대화 흐름이므로, Output이 "사용자 말하기"로 시작하는 것이 문제의 직접적인 원인은 아닐 수 있음
- **해결**:
- trainable-agents와 실제 생성된 학습 데이터 형식 상세 비교
- Output 형식 차이점 명확히 파악
- 학습 구조 비교 (둘 다 동일: 단순 연결, DataCollatorForLanguageModeling)
- 학습 방식과 실제 사용 방식의 불일치 재검토 필요
- **결과 및 재분석**:
- Output 시작 형식 차이는 확인되었으나, **사용자 질문이 먼저 오는 것은 일반적인 대화 흐름이므로 문제가 아닐 수 있음**
- 학습 구조는 동일하므로, 다른 부분을 재검토해야 할 가능성:
- 학습 데이터 품질 (반복 패턴이 학습 데이터에 있는지)
- 학습 설정 (learning rate, epochs, batch size 등)
- 프롬프트 형식 차이 (학습 데이터 프롬프트 vs 실제 사용 프롬프트)
- Label 마스킹 (prompt 부분 loss 계산 여부)
- 모델 자체의 문제 (base model, quantization 등)
- **다음 단계**: 실제 반복 응답 패턴 정확히 확인 및 다른 원인 분석

### 문제: 전체 항목 종합 분석
- **문제**:
- Fine-tuning 모델이 반복적인 응답을 생성함
- 학습 데이터 형식, 학습 설정, 프롬프트 형식, Label 마스킹, 모델 자체, FT 방식 등 모든 항목 점검 필요
- **분석 결과**:
- **1. 학습 데이터 품질**: 반복 패턴 없음 (311개 샘플 확인), 데이터 품질 문제 아님
- **2. 학습 설정**: 완전히 동일 (learning_rate, epochs, batch_size, gradient_accumulation 등 모두 동일)
- **3. 프롬프트 형식**: 형식 차이 발견
- 학습 프롬프트: '당신은 백설공주입니다...' + 위치/상태 정보 + '상호작용은 다음과 같습니다:'
- 실제 사용 프롬프트: persona_prompt만 (위치/상태 정보 없음, 다른 형식)
- 형식이 다름
- **4. Label 마스킹**: 동일 (둘 다 mlm=False, prompt 부분도 loss 계산)
- **5. 모델 자체**: 모델 버전/크기 차이 가능성
- 우리: Qwen3-4B-Instruct-2507
- trainable-agents: 데이터 생성 30B, FT 모델 확인 필요
- **6. Fine-tuning 방식**: 거의 완전히 동일 (QLoRA, 데이터 연결 방식 동일)
- **7. 데이터 크기**: **극심한 차이 발견 (가장 큰 문제일 가능성)**
- 우리: 311개 샘플
- trainable-agents: 40,186개 샘플 (약 129배 차이!)
- **재분석 결과**:
- 데이터 생성 프롬프트는 문제 없음 (데이터가 잘 생성되었으므로)
- **가장 가능성 높은 문제 원인: 데이터 크기가 너무 작음**
- trainable-agents는 40,186개 샘플로 학습
- 우리는 311개 샘플만 사용
- 129배 차이로 인한 학습 불충분 가능성이 매우 높음
- **두 번째 가능성: 프롬프트 형식 불일치**
- 모델이 학습한 프롬프트 구조: '위치: ... 상태: ... 상호작용은 다음과 같습니다:'
- 실제 사용 프롬프트 구조: persona_prompt만 (위치/상태 정보 없음)
- 하지만 데이터 생성 프롬프트가 문제가 아니라는 점에서 이것만으로는 설명 부족
 - **결론**:
 - 동일한 부분: 학습 설정, Label 마스킹, FT 방식, 학습 데이터 품질
 - **가장 가능성 높은 문제 원인: 데이터 크기가 너무 작음 (311개 vs 40,186개)**
 - 두 번째 가능성: 프롬프트 형식 불일치
 - 다음 단계: 더 많은 데이터 생성 또는 프롬프트 형식 통일 필요

### 분석: 모델 크기와 데이터 크기의 관계
- **질문**: 더 작은 모델을 쓰면 데이터가 적어도 오류 발생 확률이 낮아질까?
- **분석 결과**:
 - trainable-agents: 7B 모델(더 큼) + 40,186개 데이터 = 정상 작동
 - 우리: 4B 모델(더 작음) + 311개 데이터 = 문제 발생
 - 이것은 모델 크기보다 데이터 크기가 더 중요한 요인임을 보여줌
- **작은 모델로 바꾸는 것의 효과**:
 - 장점: 더 빠른 학습, 더 적은 메모리
 - 단점: 311개는 여전히 매우 적음, 작은 모델도 충분한 데이터가 필요함
 - 오히려 작은 모델 + 적은 데이터 = 과적합(overfitting) 위험이 더 높을 수 있음
- **결론**:
 - 더 작은 모델을 쓴다고 해서 오류 발생 확률이 낮아지는 것은 아님
 - 데이터 크기가 근본적인 문제이며, 모델 크기를 줄이는 것은 근본적인 해결책이 아님
 - 최우선: 데이터를 더 많이 생성 (수천 개 이상)
 - 모델 크기는 현재 4B로 유지하는 것이 적절

### 작업: 프로필 문단 길이 균일화
- **목적**: 장면 데이터 생성 시 문단 길이 편차를 줄여 일관성 있는 데이터 생성
- **문제**:
 - 문단 길이 편차가 심함 (최소 4자, 최대 694자)
 - 짧은 문단과 긴 문단의 간극이 커서 데이터 생성 품질에 영향 가능
- **해결**:
 - 문단 길이를 150-350자 범위로 균일화
 - 너무 긴 문단(400자 이상): 적절히 분할
 - 너무 짧은 문단(100자 미만): 인접 문단과 합치거나 내용 보강
 - 프로필 파일 전체 재구성 (`wiki_snow_white-korean.txt`)
- **결과**:
 - 총 문단 수: 48개 (이전 62개)
 - 87% (42개)가 적정 범위(150-350자)에 포함
 - 평균 길이: 301자, 중간값: 308자
 - 최소-최대 간극: 424자 (이전 690자에서 감소)
 - 문단 길이 편차 감소로 일관성 있는 장면 데이터 생성 기대

### 작업: 문단당 장면 데이터 생성 개수 증가
- **목적**: 데이터 크기 문제 해결을 위한 장면 데이터 증가
- **변경사항**:
 - 문단당 생성 장면 수: 20개 → 25개
 - 기대 총 장면 수: 960개 → 1,200개 (48문단 × 25개)
 - 25% 증가
- **수정 파일**:
 - 프롬프트 파일: `prompt_scene_generation_snow_white_korean.txt`, `prompt_agent_scene_korean.txt`
 - "정확히 20개" → "정확히 25개"로 변경
 - 체크리스트 및 검증 규칙 모두 25개 기준으로 업데이트
 - 데이터 생성 스크립트: `scripts/data_generator.py`
 - 기본값: `num_scenes: int = 25` (이전 20)
 - 검증 로직: 1-25 범위로 변경
 - max_tokens: 12,288 → 15,360 (25개 장면 생성에 충분한 토큰)
- **결과**:
 - 각 문단당 25개씩 장면 생성하도록 설정 완료
 - 기대 총 장면 수: 1,200개 (48문단 × 25개)
 - 더 많은 데이터로 Fine-tuning 품질 향상 기대

### 작업: 대화 데이터 생성 방식 trainable-agents 수준으로 조정
- **목적**: trainable-agents 수준의 대화 데이터 양 확보로 Fine-tuning 성공률 향상
- **문제**:
 - 현재: 장면당 평균 0.97개 대화 (320개 장면 → 311개 대화)
 - trainable-agents: 장면당 평균 22.8개 대화 (1,765개 장면 → 40,186개 대화)
 - 데이터 크기 차이: 약 129배로 인한 Fine-tuning 실패 가능성 높음
- **분석 결과**:
 - trainable-agents는 명확히 "한 장면당 여러 개의 대화" 방식을 사용
 - 각 대화는 평균 9.2개의 턴으로 구성 (짧고 집중적)
 - 현재 프로젝트는 평균 10.8개 턴 (비슷하지만 데이터 수가 너무 적음)
- **해결**:
 - **코드 수정** (`scripts/data_generator.py`):
 - `generate_dialogues` 메서드에 `dialogues_per_scene` 파라미터 추가 (기본값: 23개)
 - 한 장면당 여러 개의 대화를 생성하도록 반복 로직 추가
 - 각 대화마다 서로 다른 관점/주제를 가지도록 프롬프트에 지시 추가
 - Progress bar를 총 대화 수 기준으로 변경
 - **프롬프트 조정** (`prompt_agent_dialogue_korean.txt`):
 - 턴 수 명시: 약 15-20개 정도로 유지
 - 단어 수 조정: 최소 1200단어 → 약 1000-1500단어
 - 각 대화가 서로 다른 주제/접근 방식을 가지도록 지시 추가
 - `dialogue_variant_instruction` 변수로 조건부 안내 문구 추가
 - **max_tokens 조정**: 2048 → 3000 (1000-1500단어 생성에 충분한 토큰)
- **결과**:
 - 장면당 평균 대화: 0.97개 → 23개 (약 23.4배 증가)
 - 기대 총 대화 수: 311개 → 26,404개 (1,148개 장면 × 23개)
 - trainable-agents 수준 (40,186개)에 근접
 - 예상 파일 크기: 약 119-159 MB (26,404개 대화 × 4,500-6,000 bytes/대화)
 - 다양한 관점/주제로 학습하여 모델 일반화 능력 향상 기대
 - 빠른 테스트를 위해 `dialogues_per_scene`을 1-3개로 설정 가능

### 작업: 대화 데이터 프롬프트 조정 (턴 수 및 단어 수 증가)
- **목적**: 더 풍부하고 긴 대화 데이터 생성으로 학습 품질 향상
- **변경사항**:
 - 단어 수: 600-800단어 → 1000-1500단어 (약 1.5-2배 증가)
 - 턴 수: 8-10개 → 15-20개 (약 2배 증가)
 - max_tokens: 1536 → 3000 (1500단어 생성에 충분한 토큰)
- **수정 파일**:
 - 프롬프트 파일: `prompt_agent_dialogue_korean.txt`
 - 규칙 9번 수정: "약 1000-1500단어", "대화 턴 수는 약 15-20개"
 - 데이터 생성 스크립트: `scripts/data_generator.py`
 - max_tokens: 1536 → 3000
 - 주석 업데이트
- **결과**:
 - 더 긴 대화로 더 풍부한 학습 데이터 생성
 - 각 대화당 더 많은 턴으로 다양한 패턴 학습 가능
 - 대화 데이터 크기 증가 (약 1.5-2배)
 - 예상 대화당 크기: 4,500-6,000 bytes (이전 3,000 bytes 대비)

### 문제: LLM이 반복적인 문자 생성 ("따따따...")
- **문제**:
 - 대화 데이터 생성 중 LLM이 "따따따따따..." 같은 반복적인 문자를 생성
 - 35자 분량의 응답에서 31개의 "따" 문자가 연속으로 생성됨
 - 의미 없는 반복 패턴이 학습 데이터에 포함될 수 있음
- **원인**:
 - 프롬프트에서 "따옴표"라는 단어를 여러 번 반복 언급
 - LLM이 혼란스러워하여 "따" 문자 자체를 반복 생성
 - 검증 로직이 없어 이상한 패턴이 그대로 저장됨
- **해결**:
 - **프롬프트 간소화** (`prompt_agent_dialogue_korean.txt`):
 - "따옴표"라는 단어를 완전히 제거
 - 불필요한 반복 언급 제거하여 프롬프트 간결화
 - "반복적인 문자나 의미 없는 텍스트를 생성하지 마세요" 지시 추가
 - **검증 로직 추가** (`scripts/data_generator.py`):
 - 반복 문자 패턴 검증: 같은 문자가 5번 이상 연속으로 나오는 경우 필터링
 - 너무 짧은 응답 필터링: 100자 미만 응답 제외
 - 이상한 패턴 발견 시 자동으로 스킵하고 다음 대화 생성
- **결과**:
 - 프롬프트가 더 간결하고 명확해짐
 - 이상한 반복 패턴이 자동으로 필터링됨
 - 더 깨끗한 학습 데이터 생성 가능

## 2025-12-07

### 작업: 대화 데이터 품질 진단
- **목적**: 생성된 대화 데이터의 품질 확인 및 문제점 파악
- **분석 대상**: `processed/2025-12-07/generated_agent_dialogue_snow_white-korean.json` (16,518개 항목, 163,314개 대화 턴)
- **분석 결과**:
 - **전반적 평가**: 96.56/100점 (양호)
 - **구조적 무결성**: 모든 필수 필드 존재, 빈 필드 없음
 - **데이터 규모**: 총 16,518개 항목, 163,314개 대화 턴
 - **대화 구조**: 평균 9.89개 턴/항목, Content 평균 131.70자
 - **Action 분포**: 말하기(138,830개), 생각(24,484개) - 정상 분포
- **발견된 문제점**:
 - **Role 파싱 오류 (심각도: 높음)**:
 - 문제 턴 수: 약 4,417개 (전체의 2.70%)
 - 영향받은 항목: 약 3,793개 (전체의 22.96%)
 - 문제 유형:
 - 배경 설명이 role에 포함됨: `role: "배경\n맑은 햇살이..."`
 - 대화 턴 번호가 role에 포함됨: `role: "대화 턴 1\n사용자"`
 - 매우 긴 문장이 role로 파싱됨 (100자 이상)
 - 특수문자 파싱 오류: `role: "\:"` (188개)
 - 원인: Parser의 정규표현식 패턴이 복잡한 형식의 입력을 제대로 처리하지 못함
 - **Content 품질 문제 (심각도: 중간)**:
 - 문제 턴 수: 약 2,074개 (전체의 1.27%)
 - 문제 유형:
 - 플레이스홀더 값: `"상세한 발언"`, `"내면의 생각"`, `"백설공주"`
 - 너무 짧은 content: `"(*깊은 한숨*)"` (9자), `"(*조용히*)"` (7자)
 - 원인: LLM이 생성한 원본 데이터에 불완전한 내용이 포함되고, Parser가 필터링하지 못함
- **권장 사항**:
 - **즉시 조치 필요 (High Priority)**:
 1. Role 파싱 로직 개선 (`parse_data_dialogue.py`의 `parse_dialogue_turns()` 함수)
 - 배경 설명이 role에 포함되지 않도록 더 엄격한 패턴 매칭
 - "대화 턴 N" 같은 메타데이터 제거 로직 추가
 - Role 길이 제한 추가 (예: 50자 이상이면 경고/필터링)
 2. Content 품질 필터링 강화
 - 최소 길이 검증 강화 (현재 5자 → 10자 이상 권장)
 - 플레이스홀더 패턴 감지 및 필터링
 - "상세한 발언", "내면의 생각" 같은 템플릿 값 제거
 - **Fine-tuning 전 권장**:
 - 문제가 있는 항목들을 필터링하거나 수정
 - Role 파싱 로직 개선 후 재파싱 고려
 - 최소한 문제가 있는 턴들을 제외하고 학습 진행
- **결과**:
 - 전반적으로 양호한 데이터 품질 확인 (96.56/100점)
 - 구조적 무결성은 우수하나, 약 3.44%의 대화 턴에 파싱 오류 존재
 - 약 23%의 항목이 하나 이상의 문제를 포함하고 있어 Fine-tuning 전 데이터 정제 권장
 - 현재 상태로도 Fine-tuning은 가능하지만, 데이터 품질 개선을 통해 더 나은 모델 성능 기대 가능

### 문제: Fine-tuning 후에도 모델이 페르소나를 따르지 않음
- **증상**: 대용량 코퍼스(26,404개 대화)로 학습했는데도 모델이 일반 Qwen처럼 응답
 - "저는 Qwen(通義千問)이라고 합니다" 같은 기본 모델 응답
 - 백설공주 페르소나를 전혀 따르지 않음
 - 매우 긴 응답 생성 (max_tokens 무시)
 - 학습 데이터 형식이 그대로 노출됨
- **원인 분석**:
 - **학습 시 프롬프트 형식** (`agent_meta_prompt_sft_korean.txt` 사용):
 ```
 당신은 {character}입니다. 당신은 {character}처럼 응답하고 답변해야 하며...
 당신의 상태는 다음과 같습니다:
 위치: {loc_time}
 상태: {status}
 상호작용은 다음과 같습니다:
 ```
 - **실제 VTuber 시스템 프롬프트** (`conf.yaml`의 `persona_prompt`):
 - "당신은 그림 형제의 동화 \"백설공주\"의 주인공 백설공주입니다."
 - 배경 이야기, 성격 특성, 말투 등 포함
 - 위치/상태 정보 없음
 - 형식이 완전히 다름
 - **핵심 문제**: 학습 시 사용한 프롬프트 형식과 실제 사용 프롬프트 형식이 불일치
 - 모델이 학습한 프롬프트 패턴("당신의 상태는 다음과 같습니다:", "위치:", "상태:" 등)을 실제 사용 시 찾을 수 없음
 - 결과적으로 모델이 학습한 패턴을 인식하지 못하고 기본 Qwen 모델처럼 작동
- **해결 방안 (검토 필요)**:
 1. 학습 데이터 프롬프트를 실제 사용 형식과 일치시키기
 - `agent_meta_prompt_sft_korean.txt`를 수정하여 실제 `persona_prompt` 형식과 유사하게 변경
 - 위치/상태 정보를 제거하고 페르소나 정보만 포함
 2. 실제 사용 프롬프트를 학습 데이터 형식에 맞추기
 - VTuber 시스템의 프롬프트 생성 로직을 수정하여 학습 데이터 형식과 일치시키기
 3. trainable-agents 프로젝트 확인
 - trainable-agents가 어떻게 학습 데이터 프롬프트와 실제 사용 프롬프트를 일치시키는지 확인
 - 해당 방식을 참고하여 적용
- **다음 단계**: trainable-agents의 프롬프트 구조와 실제 사용 방식을 분석하여 근본 원인 파악 및 해결 방안 수립

### 작업: 프롬프트 구조 전체 분석
- **목적**: 학습 시 프롬프트 구조와 실제 사용 프롬프트 구조를 전체적으로 분석하여 불일치 원인 파악
- **분석 결과**:
 - **학습 시 프롬프트 구조** (`agent_meta_prompt_sft_korean.txt`):
 - "당신은 백설공주입니다..." (간단한 캐릭터 소개)
 - "당신의 상태는 다음과 같습니다:" (상태 헤더)
 - "위치: {location}" (구체적 위치)
 - "상태: {background}" (긴 배경 설명, 100-300자)
 - "상호작용은 다음과 같습니다:" (대화 시작 신호)
 - **실제 사용 프롬프트 구조** (`characters/snow_white.yaml`의 `persona_prompt`):
 - "당신은 그림 형제의 동화 \"백설공주\"의 주인공 백설공주입니다."
 - "# 배경 이야기" 섹션
 - "# 성격 특성" 섹션
 - "# 말투와 행동" 섹션
 - "# 기억하는 주요 사건들" 섹션
 - "# 대화 스타일" 섹션
 - "# 중요한 원칙" 섹션
 - 위치/상태 정보 없음
 - "당신의 상태는 다음과 같습니다:" 같은 헤더 없음
 - "상호작용은 다음과 같습니다:" 같은 신호 없음
 - **핵심 불일치**:
 - 학습 시: "당신의 상태는 다음과 같습니다:\n위치: ...\n상태: ...\n상호작용은 다음과 같습니다:"
 - 실제 사용: "# 배경 이야기\n...\n# 성격 특성\n..." (완전히 다른 형식)
 - 모델이 학습한 패턴(B, C, D, E)을 실제 사용 시 찾을 수 없음
 - **trainable-agents 비교**:
 - trainable-agents도 동일한 구조 사용 (영어 버전)
 - "I want you to act like {character}..."
 - "The status of you is as follows:\nLocation: ...\nStatus: ...\nThe interactions are as follows:"
 - 위치/상태 정보 포함하여 우리와 동일한 구조
- **결론**:
 - 학습 시 프롬프트 형식과 실제 사용 프롬프트 형식이 완전히 불일치
 - 모델이 학습한 패턴을 실제 사용 시 찾을 수 없어 기본 Qwen 모델처럼 작동
 - 프롬프트 형식 통일이 필수
- **해결 방안**:
 1. **방안 1 (권장)**: 학습 데이터 프롬프트를 실제 사용 형식과 일치시키기
 - `agent_meta_prompt_sft_korean.txt` 수정
 - 위치/상태 정보 제거
 - 실제 `persona_prompt` 형식과 유사하게 변경
 2. **방안 2**: trainable-agents 실제 사용 방식 확인
 - trainable-agents가 추론 시 어떻게 프롬프트를 구성하는지 확인
 3. **방안 3**: 실제 사용 프롬프트를 학습 데이터 형식에 맞추기
 - VTuber 시스템 구조 변경 필요
- **프롬프트 구조 상세 분석**:
 - **학습 시 메타 프롬프트 템플릿** (`agent_meta_prompt_sft_korean.txt`):
 - "당신은 {character}입니다. 당신은 {character}처럼 응답하고 답변해야 하며..."
 - "당신의 상태는 다음과 같습니다:"
 - "위치: {loc_time}"
 - "상태: {status}"
 - "상호작용은 다음과 같습니다:"
 - **실제 생성된 학습 데이터 프롬프트** (샘플):
 - 길이: 약 464자
 - 구조: 캐릭터 소개 → 상태 헤더 → 위치 정보 → 상태 정보 (100-300자) → 대화 시작 신호
 - **실제 VTuber 시스템 프롬프트** (`characters/snow_white.yaml`의 `persona_prompt`):
 - 길이: 약 876자
 - 구조: 상세 캐릭터 소개 → # 배경 이야기 → # 성격 특성 → # 말투와 행동 → # 기억하는 주요 사건들 → # 대화 스타일 → # 중요한 원칙
 - 위치/상태 정보 없음, "당신의 상태는 다음과 같습니다:" 헤더 없음, "상호작용은 다음과 같습니다:" 신호 없음
 - **VTuber 시스템 프롬프트 생성 과정**:
 - `service_context.py`의 `construct_system_prompt()` 함수
 - `persona_prompt` + `live2d_expression_prompt` 결합
 - LLM API 호출 시: `messages = [{"role": "system", "content": system_prompt}, ...]`
 - **trainable-agents 비교**:
 - trainable-agents도 동일한 구조 사용 (영어 버전)
 - "I want you to act like {character}..."
 - "The status of you is as follows:\nLocation: ...\nStatus: ...\nThe interactions are as follows:"
 - 위치/상태 정보 포함하여 우리와 동일한 구조
 - trainable-agents의 실제 추론 시 프롬프트 구성 방식 확인 필요

### 문제: 실제 VTuber 시스템에서 모델 반복 응답 생성
- **증상**:
 - 모델이 "나를 도와줄 거야?네가 도와줄 거야?네가 도와줄 거야?네가 도와줄 거야?..." 같은 패턴을 29번 반복 생성
 - 사용자 질문 "너는 누구야?"에 대해 정상적인 응답 대신 반복적인 문구 생성
 - 모델이 페르소나를 전혀 따르지 않고 이상한 반복 패턴만 생성
- **원인 분석**:
 - **프롬프트 구조 불일치가 핵심 원인**:
 - 학습 시 사용한 프롬프트 구조("당신의 상태는 다음과 같습니다:", "위치:", "상태:", "상호작용은 다음과 같습니다:")와 실제 VTuber 시스템 프롬프트 구조가 완전히 불일치
 - 모델이 학습한 패턴을 실제 사용 시 찾을 수 없어 혼란 상태
 - 결과적으로 기본 Qwen 모델처럼 작동하거나 이상한 반복 패턴 생성
 - 개발일지의 "문제: Fine-tuning 후에도 모델이 페르소나를 따르지 않음"에서 분석한 프롬프트 구조 불일치 문제가 실제로 발생한 것으로 확인됨
 - **학습 데이터와 실제 사용 환경의 불일치**:
 - 학습 데이터: "당신의 상태는 다음과 같습니다:\n위치: {location}\n상태: {background}\n상호작용은 다음과 같습니다:"
 - 실제 사용: `persona_prompt`만 사용 (위치/상태 정보 없음, 다른 형식)
 - 모델이 학습한 프롬프트 패턴을 인식하지 못함
- **해결 방안**:
 - **즉시 조치 필요 (High Priority)**:
 1. **프롬프트 구조 통일**:
 - 학습 데이터 프롬프트(`agent_meta_prompt_sft_korean.txt`)를 실제 VTuber 시스템 프롬프트 형식과 일치시키기
 - 또는 실제 사용 프롬프트를 학습 데이터 형식에 맞추기
 - trainable-agents의 실제 추론 시 프롬프트 구성 방식 확인 후 적용
 2. **모델 재학습**:
 - 프롬프트 구조 통일 후 재학습 필요
 - 현재 모델은 학습 시 프롬프트와 실제 사용 프롬프트 불일치로 인해 제대로 작동하지 않음
 - **검증 방법**:
 - 프롬프트 구조 통일 후 간단한 테스트 데이터로 검증
 - 모델이 정상적으로 페르소나를 따르는지 확인
 - **결론**:
 - 프롬프트 구조 불일치가 핵심 문제이며, 이를 해결하지 않으면 모델이 정상적으로 작동하지 않음
 - 프롬프트 구조 통일 후 재학습이 필수적
 - trainable-agents의 실제 추론 시 프롬프트 구성 방식을 확인하여 올바른 방향으로 수정 필요

### 해결: 학습 프롬프트를 실제 사용 프롬프트 형식에 맞춤
- **목적**: 학습 시 사용하는 프롬프트 구조를 실제 VTuber 시스템에서 사용하는 프롬프트 구조와 일치시켜 모델이 페르소나를 제대로 따르도록 함
- **변경사항**:
 - **학습 프롬프트 수정** (`agent_meta_prompt_sft_korean.txt`):
 - 기존: "당신은 {character}입니다... 당신의 상태는 다음과 같습니다:\n위치: {loc_time}\n상태: {status}\n상호작용은 다음과 같습니다:"
 - 변경: 실제 사용 프롬프트(`characters/snow_white.yaml`의 `persona_prompt`)와 동일한 형식으로 변경
 - 위치/상태 정보 제거 (실제 사용 시 포함되지 않으므로)
 - "# 배경 이야기", "# 성격 특성", "# 말투와 행동" 등 실제 사용 프롬프트와 동일한 구조 사용
 - **데이터 변환 로직 수정** (`convert_prompt_data.py`):
 - 위치/상태 정보(`loc_time`, `status`) 사용 제거
 - `meta_instruction.format()` 호출 제거 (위치/상태 정보 없이 persona_prompt만 사용)
 - 실제 사용 프롬프트 형식과 완전히 일치하도록 변경
- **기대 효과**:
 - 모델이 학습한 프롬프트 패턴을 실제 사용 시 인식 가능
 - 페르소나를 제대로 따르는 응답 생성 가능
 - 반복적인 이상한 패턴 생성 문제 해결
- **다음 단계**:
 1. 수정된 프롬프트로 학습 데이터 재생성 (`convert_prompt_data.py` 재실행) 완료
 2. 모델 재학습 (QLoRA Fine-tuning) 진행 중
 3. 재학습된 모델로 테스트하여 페르소나 준수 여부 확인

### 해결 진행: 학습 데이터 재생성 완료
- **상태**: 학습 데이터 재생성 완료 (2025-12-07)
- **재생성된 데이터**: `fairy_tale/processed/2025-12-07/prompted/prompted_agent_dialogue_snow_white.jsonl`
- **샘플 수**: 16,516개 학습 샘플
- **프롬프트 형식 확인**: 새로운 프롬프트 형식이 제대로 적용됨
 - "당신은 그림 형제의 동화 백설공주의 주인공 백설공주입니다."
 - "# 배경 이야기", "# 성격 특성", "# 말투와 행동" 등 실제 사용 프롬프트와 동일한 형식
 - 위치/상태 정보 제거됨 (실제 사용 시 포함되지 않으므로)
- **다음 단계**: 모델 재학습 필요
 - 기존 모델은 이전 프롬프트 형식으로 학습되어 있어 페르소나를 따르지 않음
 - 재생성된 학습 데이터로 QLoRA Fine-tuning 재실행 필요

## 2025-12-08

### 문제: 학습된 모델이 페르소나를 따르지 않고 기본 Qwen 응답 생성
- **증상**:
 - 학습 완료 후 LoRA 어댑터 테스트 시 모델이 "저는 Qwen(通义千问)입니다" 같은 기본 Qwen 응답을 반복 생성
 - 백설공주 캐릭터 설정을 전혀 따르지 않음
 - 프롬프트에 "백설공주"라고 명시되어 있음에도 "저에게는 특별한 이름이 없어요"라고 응답
 - 조수미 페르소나 학습에서는 정상 작동했으나 백설공주에서는 실패
- **원인 분석**:
 - **Loss 마스킹 부재**:
 - 현재 학습 코드(`train_qlora.py`)가 프롬프트와 출력을 단순 연결만 하고 있음
 - 프롬프트 부분도 loss 계산에 포함되어 모델이 프롬프트 자체를 학습하려고 시도
 - 출력 부분만 학습해야 하는데 프롬프트 부분까지 학습 대상이 되어버림
 - **trainable-agents 방식과의 차이**:
 - trainable-agents는 프롬프트 부분을 loss 계산에서 제외(-100으로 마스킹)하고 출력 부분만 학습
 - 조수미 페르소나 학습 시 trainable-agents 방식을 사용했기 때문에 정상 작동
 - 백설공주 학습 시에는 단순 연결 방식만 사용하여 문제 발생
- **해결**:
 - **trainable-agents 방식 적용** (`train_qlora.py`의 `load_jsonl_dataset` 함수):
 - 프롬프트와 출력을 분리하여 토크나이징
 - 프롬프트 부분의 labels를 `-100`으로 설정하여 loss 계산에서 제외
 - 출력 부분만 실제 토큰 ID로 설정하여 학습 대상으로 지정
 - 최대 길이 제한 시 출력 부분을 우선 보존하고 프롬프트를 자르도록 처리
 - **코드 변경사항**:
 - 기존: `text = f"{prompt}{output}"` → 단순 연결 후 전체를 학습
 - 변경: 프롬프트와 출력을 분리 토크나이징 → 프롬프트 부분 labels를 -100으로 마스킹 → 출력 부분만 학습
 - 데이터셋 클래스에 `labels` 필드 추가하여 loss 마스킹 정보 포함
- **결과**:
 - trainable-agents 방식과 동일하게 프롬프트 부분은 loss 계산에서 제외
 - 출력 부분만 학습하여 모델이 캐릭터 응답을 제대로 학습할 수 있도록 개선
 - 조수미 페르소나 학습과 동일한 방식으로 동작하여 일관성 확보
- **다음 단계**:
 - 수정된 학습 코드로 모델 재학습 진행
 - 재학습 후 LoRA 어댑터 테스트를 통해 페르소나 준수 여부 확인

### 문제: 배치 내 시퀀스 길이 불일치로 인한 학습 오류
- **증상**:
 - 학습 시작 직후 `ValueError: expected sequence of length 1440 at dim 1 (got 1772)` 오류 발생
 - 배치 내에서 `input_ids`와 `labels`의 길이가 달라 텐서 생성 실패
 - 에러 메시지: "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length"
- **원인 분석**:
 - **데이터 로딩 함수의 문제**:
 - 프롬프트와 출력을 분리 토크나이징하는 과정에서 길이 계산 오차 발생
 - `input_ids`와 `labels`의 길이가 정확히 일치하지 않음
 - 예: `input_ids`는 1440 토큰, `labels`는 1772 토큰으로 불일치
 - **DataCollator의 한계**:
 - 기본 `DataCollatorForLanguageModeling`이 labels가 이미 제공된 경우를 제대로 처리하지 못함
 - 배치 내에서 길이가 다른 시퀀스를 패딩할 때 labels도 함께 패딩해야 하는데, 기본 collator는 이를 처리하지 못함
- **해결**:
 - **데이터 로딩 함수 개선** (`load_jsonl_dataset`):
 - 전체 텍스트(prompt + output)를 먼저 토크나이징하여 정확한 길이 계산
 - 프롬프트만 별도로 토크나이징하여 길이 계산
 - 최대 길이 제한 후 프롬프트 길이를 정확히 조정
 - `input_ids`와 `labels`의 길이를 반드시 동일하게 맞춤
 - **커스텀 DataCollator 추가** (`DataCollatorForLanguageModelingWithLabels`):
 - `DataCollatorForLanguageModeling`을 상속받아 labels 패딩 처리 추가
 - labels가 있는 경우 labels도 함께 패딩하도록 `torch_call` 메서드 오버라이드
 - 패딩 값은 -100으로 설정하여 loss 계산에서 제외
 - 배치 내 최대 길이에 맞춰 모든 시퀀스를 패딩
- **코드 변경사항**:
 - `load_jsonl_dataset`: 전체 텍스트 토크나이징 → 프롬프트 길이 정확히 계산 → labels 생성 시 길이 일치 보장
 - `DataCollatorForLanguageModelingWithLabels` 클래스 추가: labels 패딩 처리 로직 구현
 - `train_model_qlora`: 기본 DataCollator 대신 커스텀 DataCollator 사용
- **결과**:
 - 배치 내 시퀀스 길이 불일치 문제 해결
 - labels도 함께 패딩되어 학습이 정상적으로 진행 가능
 - trainable-agents 방식의 loss 마스킹과 배치 처리 모두 정상 작동

## 2025-12-09

### 문제: 학습과 추론 시 프롬프트 형식 불일치로 인한 페르소나 미준수
- **증상**:
 - 학습 완료 후에도 모델이 "나는 Qwen입니다"라고 답변하는 문제 지속
 - 페르소나를 전혀 따르지 않고 일반적인 Qwen 응답 생성
 - 여러 번 재학습해도 동일한 문제 반복
- **원인 분석**:
 - **핵심 문제: Chat Template 미사용**:
 - 학습 시: `prompt + output`을 단순 연결하여 토크나이징 (chat template 미사용)
 - 추론 시: OpenAI 호환 API를 통해 chat template이 자동 적용됨
 - Qwen3-4B-Instruct는 instruction-tuned 모델로 chat template을 사용해야 함
 - 학습과 추론 시의 형식이 달라 모델이 제대로 학습되지 않음
 - **학습 데이터 형식**:
 - `prompt`: 페르소나 프롬프트 (예: "당신은 백설공주입니다...")
 - `output`: "사용자 말하기: ... <|eot|>\n백설공주 말하기: ... <|eot|>..."
 - 이 형식이 chat template 형식과 다름
 - **추론 시 형식**:
 - OpenAI 호환 API를 통해 `system` 프롬프트와 `messages` 형식으로 전달
 - Qwen 모델이 내부적으로 chat template 적용: `<|im_start|>system\n...<|im_end|>\n<|im_start|>user\n...<|im_end|>\n<|im_start|>assistant\n...`
- **해결**:
 - **Chat Template 적용** (`load_jsonl_dataset` 함수 수정):
 - 학습 데이터를 messages 형식으로 변환:
 - `prompt` → `system` message
 - `output`에서 "사용자 말하기:" 부분 → `user` message
 - `output`에서 "백설공주 말하기:" 부분 → `assistant` message
 - `tokenizer.apply_chat_template()`을 사용하여 chat template 적용
 - 학습 시와 추론 시 동일한 형식 사용
 - **Loss 마스킹 수정**:
 - Chat template 적용 후 assistant 부분 시작 위치 찾기
 - `<|im_start|>assistant\n` 패턴을 찾아 그 이후만 학습
 - System + user 부분은 -100으로 마스킹
- **코드 변경사항**:
 - `load_jsonl_dataset`: 
 - 정규식으로 사용자/캐릭터 메시지 추출
 - Messages 형식으로 구성: `[{"role": "system", ...}, {"role": "user", ...}, {"role": "assistant", ...}]`
 - `tokenizer.apply_chat_template()` 적용
 - Assistant 시작 위치를 찾아 loss 마스킹
 - Chat template 지원 확인 로직 추가
- **결과**:
 - 학습과 추론 시 동일한 프롬프트 형식 사용
 - Qwen3-4B-Instruct 모델의 instruction-tuned 특성에 맞는 학습 방식 적용
 - 모델이 페르소나를 제대로 학습할 수 있는 환경 구축
- **다음 단계**:
 - 수정된 코드로 모델 재학습 진행
 - 재학습 후 모델이 "Qwen"이라고 답변하지 않고 페르소나를 따르는지 확인

### 문제: 모델이 질문에 직접 답하지 않고 간접적으로 응답하는 문제
- **증상**:
 - Chat template 적용 후 재학습 완료 (3 epochs, Loss: 1.0413)
 - "Qwen" 언급 문제는 해결됨
 - 하지만 "너는 누구야?" 같은 직접적인 질문에 직접 답하지 않음
 - 질문에 답하는 대신 다른 질문을 생성하거나 간접적으로 응답
 - 페르소나 키워드("백설공주", "공주")가 응답에 거의 나타나지 않음
- **원인 분석**:
 - **학습 데이터 구조 검토**:
 - 현재 구조: "사용자 말하기: ... → 백설공주 말하기: ..." (사용자 → 백설공주 순서)
 - 전체 16,516개 샘플 중 16,301개가 사용자 → 백설공주 순서
 - 백설공주 말하기로 시작하는 샘플은 0개
 - trainable-agents도 유사한 구조 사용 (조수미 speaking이 먼저 오는 경우도 있지만, 대부분 사용자 → 조수미 순서)
 - 결론: 데이터 구조는 올바름, 실제 대화 흐름과 일치
 - **학습 데이터의 질문-답변 패턴 분석**:
 - 학습 데이터의 대부분이 상황 설명이나 감정 표현 위주
 - "너는 누구야?" 같은 직접적인 정체성 질문에 대한 직접적 답변이 부족
 - 예: "백설공주야, 요즘 당신이 느끼는 행복의 정체가 뭐라고 생각해?" → 간접적이고 철학적인 답변
 - 직접적인 "저는 백설공주예요" 같은 답변 패턴이 학습 데이터에 적음
 - **학습량 부족 가능성**:
 - 현재 Loss: 1.0413 (1.0 근처에서 수렴)
 - 최저 Loss: 0.9892 (Step 1370)
 - Loss 감소율: 57.4% (초기 2.4448 → 최종 1.0413)
 - 3 epochs로는 충분하지 않을 수 있음
 - trainable-agents도 3 epochs 사용했지만, 학습 데이터의 질문-답변 패턴이 다를 수 있음
- **추론 설정 최적화**:
 - 다양한 Temperature, Top_p, Top_k 설정으로 테스트 진행
 - 최적 설정:
 - Temperature: 0.7 → 0.1 (더 결정적인 응답)
 - Top_k: 40 → 30 (더 제한적인 토큰 선택)
 - Repeat_penalty: 1.2 추가 (반복 방지)
 - Num_predict: 2048 → 150 (짧은 응답 강제)
 - Modelfile 업데이트 및 Ollama 모델 재등록 완료
 - 결과: Qwen 언급 문제는 해결되었지만, 질문에 직접 답하지 않는 문제는 지속
- **데이터 구조 변경 검토**:
 - 백설공주 말하기로 시작하는 구조 검토:
 - 장점: 캐릭터의 주도적인 발언 학습 가능, 캐릭터 성격 강조
 - 단점: 실제 대화에서는 사용자가 먼저 말하는 경우가 많음, 추론 시 사용자 질문에 답하는 패턴과 불일치 가능
 - 현재 구조 (사용자 → 백설공주) 유지 결정:
 - 실제 대화 흐름과 일치
 - 사용자 질문에 답하는 패턴 학습에 적합
 - trainable-agents도 유사한 구조 사용
- **해결 방안**:
 - **1순위: 모델 재학습 (Epochs 증가)**:
 - 현재 Loss가 1.0 근처에서 수렴 중
 - Epochs를 5-6으로 증가하여 더 많은 학습 진행
 - 예상 Loss: 0.8-0.9 (현재 1.04에서 감소)
 - 예상 시간: 8-11시간
 - Learning rate 조정 가능: 2e-4 → 1.5e-4 (더 안정적인 학습)
 - **2순위: 학습 데이터 보강 후 재학습**:
 - 직접적인 질문-답변 쌍 추가
 - 예: "너는 누구야?" → "저는 백설공주예요"
 - 재학습으로 해결되지 않을 경우 고려
- **코드 변경사항**:
 - `train_character.py`: `--learning-rate` 인자 추가하여 학습률 조정 가능하도록 개선
 - Modelfile 업데이트: 추론 설정 최적화
- **결과**:
 - 추론 설정 최적화 완료
 - Qwen 언급 문제 해결 확인
 - 데이터 구조는 올바름 확인
 - 질문에 직접 답하지 않는 문제는 학습량 부족 또는 학습 데이터 패턴 문제로 판단
- **다음 단계**:
 - Epochs 5-6으로 재학습 진행 (권장)
 - 재학습 후에도 문제가 지속되면 학습 데이터 보강 고려

## 2025-12-10

### 작업: 재학습 완료 (Epochs 6, Learning Rate 1.5e-4)
- **Epochs**: 6
- **Learning Rate**: 1.5e-4
- **최종 Loss**: 0.8623 (이전 1.04에서 개선)
- **학습 시간**: 약 10.5시간
- **모델 병합**: 완료
- **GGUF 변환**: 완료
- **Ollama 등록**: 완료

### 문제: 재학습 후 모델 테스트 결과 - 페르소나 미준수 문제 지속
- **증상**:
 - "너는 누구야?" 질문에 직접 답하지 않고 질문을 반복 생성
 - "당신의 이름이 뭐예요?" 질문에 "저는 AI인 셔틀입니다"라고 잘못된 답변
 - "백설공주에 대해 알려줘" 질문에 3인칭으로 설명 (1인칭이어야 함)
 - "안녕하세요" 인사에 "이곳"이라는 모호한 표현 반복
- **해결된 문제**:
 - Qwen 언급 문제는 해결됨 (더 이상 "Qwen"이라고 답하지 않음)
- **원인 분석**:
 - **학습 데이터 문제 (가장 가능성 높음)**:
 - 직접적인 정체성 질문-답변 쌍이 부족
 - 학습 데이터의 대부분이 간접적이고 상황 설명 위주
 - "너는 누구야?" → "저는 백설공주예요" 같은 직접적 패턴 부족
 - **System Prompt 전달 문제**:
 - Ollama에서 System Prompt가 제대로 전달되지 않을 수 있음
 - Chat template과 System Prompt의 연결 문제
 - **학습 방법 문제**:
 - Loss는 감소했지만 (0.8623), 페르소나 학습이 제대로 되지 않음
 - 학습 데이터의 질이 문제일 수 있음
- **해결 방안**:
 - **학습 데이터 보강 (최우선)**:
 - 직접적인 정체성 질문-답변 쌍 추가
 - 예: "너는 누구야?" → "저는 백설공주예요"
 - "당신의 이름이 뭐예요?" → "저는 백설공주예요"
 - "백설공주에 대해 알려줘" → "저는 백설공주예요. 저는..." (1인칭)
 - **System Prompt 확인**:
 - Ollama에서 System Prompt가 제대로 전달되는지 확인
 - Chat template과의 연결 확인
 - **추론 설정 재조정**:
 - Temperature를 0.3-0.5로 조정하여 테스트
 - 하지만 근본적인 문제는 학습 데이터일 가능성이 높음
- **다음 단계**:
 - 학습 데이터에 직접적인 정체성 관련 대화 추가
 - 보강된 데이터로 재학습 진행

### 문제: 학습 데이터 사이즈 부족으로 인한 페르소나 미준수
- **증상**: 재학습 후에도 페르소나 미준수 문제 지속
- **원인 분석**:
 - **데이터 사이즈 문제**:
 - 백설공주 학습 데이터: 16,516개 예제
 - 조수미 학습 데이터 (참고): 40,186개 예제
 - 백설공주가 조수미 대비 약 2.4배 적음
 - 데이터 사이즈가 문제일 가능성이 높음
 - **데이터 품질 문제**:
 - 직접적인 정체성 질문-답변 쌍 부족
 - 정체성 질문 포함: 327개 (1.98%)
 - 직접 답변 포함: 12개 (0.07%) - 매우 부족
- **해결**:
 - **장면 생성 수 증가**: 문단당 25개 → 40개로 변경 (45개에서 조정)
 - 1 라운드당 1,175개 → 1,880개로 증가 (1.6배)
 - 목표 달성에 필요한 라운드: 약 34.2 → 21 라운드 (39% 시간 절약)
 - **코드 변경사항**:
 - `prompt_scene_generation_snow_white_korean.txt`: "45개" → "40개"로 변경 (6곳)
 - `data_generator.py`: `num_scenes` 기본값 45 → 40
 - `data_generator.py`: `max_tokens` 27,648 → 25,000 (40개 생성에 충분)
- **조정 이유** (2025-12-10):
 - 45개 생성 시 `exaone3.5:2.4b` 모델의 생성 능력 한계로 많은 재시도 필요
 - 일부 문단에서 3번 재시도 후에도 실패 (약 5% 실패율)
 - 40개로 조정하여 모델의 안정적인 생성 능력 범위 내에서 운영
 - 40개도 목표 데이터 사이즈 달성에 충분 (21 라운드)
- **결과**:
 - 장면 생성 효율성 1.6배 증가
 - 목표 데이터 사이즈 달성에 필요한 시간 약 39% 감소
 - 모델 생성 안정성 향상 (재시도 감소)
- **다음 단계**:
 - 장면 데이터 추가 생성 (약 21 라운드)
 - 대화 데이터 추가 생성
 - 데이터 품질 개선 (직접적인 정체성 질문-답변 쌍 추가)
 - 보강된 데이터로 재학습 진행

### 문제: 40개로 조정 후에도 재시도 및 실패 발생
- **증상**: 40개로 줄였는데도 여전히 재시도가 많이 필요하고 일부 문단에서 실패
 - Paragraph 7: 3번 재시도 후 실패
 - 많은 문단에서 2-3번 재시도 필요
- **원인 분석**:
 - `exaone3.5:2.4b` 모델의 생성 능력 한계
 - 40개도 모델이 한 번에 생성하기 어려운 양
 - 모델 크기 문제로 인한 불안정한 생성
- **해결**:
 - **더 큰 모델 사용 권장**: `exaone3.5:7.8b` 모델 사용
 - 약 3배 큰 모델 (2.4B → 7.8B)
 - 더 안정적인 장면 생성
 - 재시도 감소로 시간 절약
 - **사용 방법**:
 ```bash
 # 모델 다운로드
 ollama pull exaone3.5:7.8b
 
 # 데이터 생성 시 모델 지정
 python fairy_tale/run_gen_data.py \
 --character snow_white \
 --prompt-name gen_scene \
 --model_name exaone3.5:7.8b
 ```
 - **코드 변경사항**:
 - `README.md`: 모델 선택 가이드 및 사용 방법 추가
 - `run_gen_data.py`: 이미 `--model_name` 인자 지원 (기본값: exaone3.5:2.4b)
- **결과**:
 - 더 큰 모델 사용으로 재시도 감소 예상
 - 장면 생성 안정성 향상
 - 전체 데이터 생성 시간 단축

### 최적화: 대화 생성 시간 단축 (2025-12-10)
- **문제**: 장면당 23개 대화 생성 시 약 58.7시간 소요 (너무 김)
- **해결**: 장면당 대화 개수를 23개 → 1개로 변경
 - 총 대화 수: 26,404개 → 1,148개
 - 예상 시간: 58.7시간 → 2.6시간 (95.7% 시간 절약)
 - 빠른 반복 및 테스트 가능
- **코드 변경사항**:
 - `data_generator.py`: `dialogues_per_scene` 기본값 23 → 1
- **데이터 양 보완 방안**:
 - 장면 데이터를 더 많이 생성 (40개/문단, 더 많은 라운드)
 - 필요시 추가 대화 생성 가능
 - 빠른 품질 확인 후 반복 생성 가능
- **결과**:
 - 생성 시간 대폭 단축 (약 2.5시간)
 - 빠른 반복과 실험 가능
 - 품질 확인 후 필요시 추가 생성으로 보완 가능

### 재조정: 대화 생성 양 증가 (2025-12-10)
- **문제**: 1개/장면으로 줄였더니 생성 시간이 너무 짧음 (약 1시간)
- **해결**: 기존 대용량 파일(26,400개)의 2.5배 수준으로 조정
 - 장면당 대화: 1개 → 57개
 - 총 대화 수: 1,148개 → 65,436개
 - 예상 시간: 약 2.5시간 → 약 145시간 (약 6일)
- **코드 변경사항**:
 - `data_generator.py`: `dialogues_per_scene` 기본값 1 → 57
- **결과**:
 - 기존 대용량 파일의 2.5배 수준 달성
 - 충분한 데이터 양 확보
 - 학습 데이터 품질 향상 기대

### 최종 조정: 24시간에 맞춘 대화 생성 양 (2025-12-10)
- **문제**: 57개/장면으로 설정하면 약 145시간 소요 (너무 김)
- **해결**: 24시간에 딱 맞는 양으로 조정
 - 장면당 대화: 57개 → 9개
 - 총 대화 수: 65,436개 → 10,332개
 - 예상 시간: 약 145시간 → 약 23시간 (약 1일)
- **코드 변경사항**:
 - `data_generator.py`: `dialogues_per_scene` 기본값 57 → 9
- **결과**:
 - 약 24시간에 맞춘 생성 시간
 - 적절한 데이터 양 확보
 - 하루 내 완료 가능

### 최종 조정: 조수미 데이터 수준 달성을 위한 증가 (2025-12-10)
- **목적**: 조수미 데이터 수준(40,186개) 달성
- **해결**: 장면당 대화 개수 증가
 - 장면당 대화: 23개 → 35개
 - 총 대화 수: 26,404개 → 40,180개
 - 조수미 수준 달성 (약 1.0배)
- **코드 변경사항**:
 - `data_generator.py`: `dialogues_per_scene` 기본값 23 → 35
- **예상 시간** (실제 측정값 반영):
 - 2.4b 모델: 약 61시간 (약 2.6일, 대화당 약 5.5초 추정)
 - 7.8b 모델: 약 88시간 (약 3.7일, 실제 측정: 23개/장면 시 58시간, 대화당 약 7.9초)
 - 7.8b는 2.4b보다 약 1.4배 느림
- **결과**:
 - 조수미 데이터 수준 달성
 - 충분한 학습 데이터 확보
 - 모델 성능 향상 기대

### 문제: Loss 감소 부족 및 파라미터 업데이트 방식 개선 (2025-12-10)
- **증상**: 학습 시 Loss가 많이 안 줄어듦
 - 초기 Loss: 2.524
 - 최종 Loss: 0.8623 (6 epochs)
 - 감소율: 65.8%
 - 최종 Loss가 0.86으로 여전히 높음
- **현재 설정 분석**:
 - Optimizer: `paged_adamw_32bit` (메모리 효율적이지만 수렴이 덜 안정적일 수 있음)
 - Learning Rate: 1.5e-4
 - LR Scheduler: cosine
 - Warmup Ratio: 0.03 (3%, 너무 짧음)
- **해결**: 파라미터 업데이트 방식 개선
 - **Optimizer 변경**: `paged_adamw_32bit` → `adamw_torch`
 - 표준 AdamW 구현, 더 안정적인 수렴
 - 일반적으로 더 낮은 최종 loss 달성
 - 메모리 사용량 약간 증가하지만 성능 향상 기대
 - **Warmup 증가**: 0.03 (3%) → 0.1 (10%)
 - 더 안정적인 초기 학습
 - Loss 감소가 더 부드럽고 안정적
- **코드 변경사항**:
 - `train_qlora.py`: `optim="paged_adamw_32bit"` → `optim="adamw_torch"`
 - `train_qlora.py`: `warmup_ratio=0.03` → `warmup_ratio=0.1`
- **기대 효과**:
 - 더 안정적인 Loss 감소
 - 더 낮은 최종 Loss 달성 가능
 - 학습 안정성 향상
- **주의사항**:
 - Loss 0.86은 데이터 품질/양 문제일 수도 있음
 - Optimizer 변경만으로는 한계가 있을 수 있음
 - 더 많은 데이터와 epochs도 고려 필요

## 2025-12-12

### 문제: 대화 데이터 생성 중 중단 및 잘못 생성된 부분 발생
- **증상**: 
 - 대화 데이터 생성 중 중단됨 (47,480줄에서 중단)
 - 중단 후 재시작 시 잘못 생성된 대화 포함 (1356번째 장면의 21번째 대화부터)
 - 프롬프트의 대화 번호와 실제 위치가 불일치
- **원인**: 
 - Resume 기능이 없어서 중단 후 처음부터 다시 시작하거나 잘못된 지점부터 재개
 - 기존 파일을 감지하고 중단 지점을 계산하는 로직 부재
- **해결**:
 - Resume 기능 추가 (`data_generator.py`)
 - 기존 파일 존재 시 자동 감지
 - 완료된 장면 수와 마지막 장면의 완료된 대화 수 계산
 - 중단 지점부터 자동 재개
 - Progress bar는 남은 대화 수 기준으로 표시
 - 잘못 생성된 부분 삭제 스크립트 작성 (`fix_dialogue_file.py`)
 - 1356번째 장면의 20번째 대화까지 정상 확인
 - 21번째 대화부터 잘못 생성된 35개 라인 삭제
 - 백업 파일 자동 생성
- **코드 변경사항**:
 - `data_generator.py`: `generate_dialogues()` 메서드에 `resume` 파라미터 추가 (기본값: True)
 - 기존 파일 읽기 및 중단 지점 계산 로직 추가
 - `start_scene_idx`, `start_dialogue_num` 계산
 - 첫 번째 장면은 `start_dialogue_num`부터, 이후 장면은 0부터 시작
 - `run_gen_data.py`: `generate_dialogues()` 호출 시 `resume=True` 명시적 전달
 - `fix_dialogue_file.py`: 잘못 생성된 부분 삭제 유틸리티 스크립트 추가
- **검증 결과**:
 - 현재 상태: 47,445개 대화 (1356번째 장면의 20번째 대화까지)
 - 총 필요한 대화: 61,880개 (1,768개 장면 × 35개 대화)
 - 남은 대화: 14,435개
 - Resume 로직 시뮬레이션 결과: 올바른 재개 지점 계산
 - 재개 지점: 1356번째 장면의 21번째 대화부터
 - 첫 번째 장면: 15개 대화 생성 (21~35번째)
 - 나머지 장면: 412개 장면 × 35개 = 14,420개 대화 생성
 - 최종 결과: 61,880개로 일치 확인
 - 프롬프트 생성 로직: 올바름 (`dialogue_num + 1` 사용)
 - 파일 저장 로직: append 모드로 올바름
- **결과**:
 - 중단 후 자동으로 올바른 지점부터 재개 가능
 - 잘못 생성된 부분 제거 완료
 - 전체 대화 데이터 생성 완료 보장

### 작업: 대화 데이터에서 마크다운 형식 제거 (2025-12-12)
- **상황**: 생성된 대화 데이터의 63.66% (39,393개)에 마크다운 형식(**, ##, ### 등)이 포함됨
- **문제**: 마크다운 형식이 포함된 대화는 파싱 시 문제를 일으킬 수 있음
- **해결**: 마크다운 제거 스크립트 작성 및 실행
 - `clean_dialogue_markdown.py` 스크립트 작성
 - `parse_data_dialogue.py`의 `clean_markdown()` 함수 재사용
 - 원본 JSONL 파일의 `completions` 필드에서 마크다운 제거
 - 백업 파일 자동 생성
- **코드 변경사항**:
 - `scripts/clean_dialogue_markdown.py`: 마크다운 제거 유틸리티 스크립트 추가
 - `clean_markdown()` 함수로 **, ##, ###, #, * 리스트 등 제거
 - Progress bar로 진행 상황 표시
 - 백업 파일 자동 생성
- **결과**:
 - 전체 61,880개 대화 처리 완료
 - 39,406개 대화에서 마크다운 제거
 - 마크다운 제거율: 99.99% 이상
 - 백업 파일 생성 완료

### 작업: 대화 데이터 포맷 정리 (2025-12-12)
- **상황**: 대화 데이터에 띄어쓰기, 줄바꿈 등 포맷 문제 발견
 - 연속 공백 (3개 이상): 28개
 - 연속 줄바꿈 (3개 이상): 1개
 - 연속 공백 (2개): 2,335개 (3.77%)
 - 연속 줄바꿈 (2개): 61,842개 (99.94%, 정상 - 대화 턴 사이 공백)
 - 너무 긴 줄 (500자 이상): 103개 (일부 영어 텍스트 포함)
- **문제**: 포맷 불일치로 인한 파싱 및 처리 시 문제 가능성
- **해결**: 포맷 정리 스크립트 작성 및 실행
 - `clean_dialogue_format.py` 스크립트 작성
 - 연속 공백 (3개 이상) → 1개로 변경
 - 연속 줄바꿈 (3개 이상) → 2개로 변경
 - 탭 문자 → 공백으로 변경
 - 줄 끝 공백 제거
 - 제어 문자 제거
- **코드 변경사항**:
 - `scripts/clean_dialogue_format.py`: 포맷 정리 유틸리티 스크립트 추가
 - `clean_formatting()` 함수로 포맷 정리
 - Progress bar로 진행 상황 표시
 - 백업 파일 자동 생성
- **결과**:
 - 전체 61,880개 대화 처리 완료
 - 9,815개 대화에서 포맷 정리
 - 모든 포맷 문제 해결 완료
 - 백업 파일 생성 완료
- **참고사항**:
 - 영어 텍스트가 포함된 대화 209개 발견 (의도된 것일 수 있음)
 - 너무 긴 줄 (500자 이상) 103개 발견 (일부는 정상적인 긴 대화)

### 작업: 대화 데이터 상세 점검 및 정제 검증 (2025-12-12)
- **목적**: 정제 과정에서 데이터 손상 여부 확인 및 데이터 품질 검증
- **점검 항목**:
 1. JSON 형식 유효성: 모든 라인 유효 (61,880개)
 2. 필수 필드 존재 여부: 모든 필드 존재 (100%)
 3. completions 필드 내용: 빈 값 없음, 최소 168자
 4. 정제 전후 비교: 내용 손상 없음 (공백 정제만 수행)
 5. 이스케이프/특수 문자: 정상 처리
 6. 데이터 패턴 일관성: 대부분 정상 (일부 변형은 정상 범위)
 7. 정제 과정 손상: 잘린 내용 없음
 8. 통계적 이상치: 일부 있지만 정상 변형 범위
 9. 실제 샘플 확인: 구조적으로 정상
 10. 정제 스크립트 로직: 모든 문제 올바르게 처리
- **정제 전후 비교 결과**:
 - 차이가 있는 라인: 20개 (공백 정제만)
 - 차이 유형: "배경:" 뒤 연속 공백 제거 (의도된 정제)
 - 내용 손상: 없음
 - 길이 차이: 미미함 (1-3자)
- **최종 검증 결과**:
 - JSON 파싱: 통과
 - 타입 검증: 통과
 - 내용 손상 검증: 통과 (샘플 100개 기준)
 - 인코딩 검증: 통과
- **결론**:
 - 정제 과정에서 데이터 손상 없음
 - 모든 데이터가 유효하고 정상적으로 정제됨
 - 포맷 정리가 올바르게 수행됨 (공백, 줄바꿈, 탭 문자 정리)
 - 데이터 품질 우수, 추가 정제 불필요

### 작업: Fine-tuning 데이터 준비 완료
- **목적**: 파싱된 대화 데이터를 Fine-tuning 형식으로 변환
- **작업 내용**:
 - `convert_prompt_data.py` 실행하여 학습 데이터 생성
 - 파싱된 데이터 61,799개를 JSONL 형식으로 변환
 - 각 샘플에 prompt (페르소나 프롬프트)와 output (대화 형식) 포함
- **결과**:
 - 총 학습 샘플: 61,799개
 - 출력 파일: `fairy_tale/processed/2025-12-13/prompted/prompted_agent_dialogue_snow_white.jsonl`
 - 파일 크기: 324MB
 - Prompt 평균 길이: 869자 (실제 사용 프롬프트 형식과 일치)
 - Output 평균 길이: 1,566자 (Chat template 형식 준비 완료)
 - Fine-tuning 준비 완료

### 작업: wandb 연결 설정 및 학습 설정 최적화
- **목적**: 학습 진행 상황 추적 및 최적 학습 설정 결정
- **wandb 연결 설정**:
 - 환경 변수 방식으로 wandb 연결 (코드 수정 없음)
 - `WANDB_API_KEY` 환경 변수 설정 시 자동 연결
 - `WANDB_PROJECT` 환경 변수로 프로젝트 이름 설정 (dreamtale)
 - TrainingArguments의 `report_to=["wandb"]` 사용
- **학습 설정 분석**:
 - 총 학습 샘플: 61,799개
 - 유효 배치 크기: 32 (batch_size 2 × grad_accum 16)
 - 에포크당 스텝: 약 1,931개
 - 에포크 3: 총 5,793 스텝, 예상 시간 5-6시간, 예상 Loss 0.95-1.0
 - 에포크 4: 총 7,724 스텝, 예상 시간 7-8시간, 예상 Loss 0.9-0.95
- **이전 학습 기록 참고**:
 - 6 epochs: 최종 Loss 0.8623 (10.5시간)
 - Loss 감소 패턴: 초기 2.5 → 1.0 (약 2 epochs) → 0.86 (6 epochs)
- **권장 설정**:
 - 빠른 테스트/검증: 3 epochs 권장
 - 최종 모델 학습: 4 epochs 권장
 - 데이터가 충분하므로 3 epochs로도 학습 가능

## 2025-12-13

### 문제: 대화 데이터 파싱 성공률 저하 (35% → 99.9% 개선)
- **증상**: 
 - 원본 대화 데이터 61,880개 중 21,637개만 파싱 성공 (35% 성공률)
 - 약 65%의 데이터가 파싱 실패하여 손실
 - 2일간 생성한 대량의 데이터가 제대로 활용되지 못함
- **원인 분석**:
 - **배경 추출 로직 문제**:
 - `extract_background` 함수가 배경을 추출할 때 대화 내용까지 함께 제거
 - 배경 패턴 매칭 시 대화 턴 시작 전까지의 모든 텍스트를 배경으로 인식
 - 배경 제거 후 `remaining_text`가 비어있거나 너무 짧아져 대화 턴 추출 실패
 - **필터링 조건 과도하게 엄격**:
 - Role 길이 제한: 100자 (너무 짧음)
 - Content 최소 길이: 10자 (너무 엄격)
 - 플레이스홀더 필터링이 과도하게 엄격하여 정상적인 내용도 제거
- **해결**:
 - **배경 추출 로직 개선** (`parse_data_dialogue.py`의 `extract_background` 함수):
 - 대화 턴 시작 패턴을 lookahead에 포함하여 배경 추출 범위 제한
 - 배경 패턴 매칭 시 대화 턴 시작 전까지만 배경으로 인식
 - 배경 패턴이 없으면 "배경" 단어로 시작하는 첫 단락만 추출
 - `remaining_text`가 비어있거나 너무 짧으면 원본 텍스트 사용
 - **필터링 조건 완화**:
 - Role 길이 제한: 100자 → 150자로 완화
 - Content 최소 길이: 10자 → 5자로 완화
 - 플레이스홀더 필터링 완화: "상세한 발언 ..." 같은 패턴은 내용이 있으면 유지
 - **파싱 실패 시 재시도 로직 추가**:
 - `remaining_text`가 비어있거나 너무 짧으면 원본 텍스트로 직접 파싱 시도
 - 배경 추출 실패해도 대화 턴 추출은 가능하도록 개선
- **코드 변경사항**:
 - `extract_background`: 대화 턴 시작 패턴을 lookahead에 포함, 배경 추출 범위 제한
 - `parse_dialogue_info`: `remaining_text` 검증 및 원본 텍스트 fallback 로직 추가
 - `parse_dialogue_turns`: Role 길이 제한 100자 → 150자, Content 최소 길이 10자 → 5자
- **결과**:
 - 파싱 성공률: 35% (21,637개) → 99.9% (61,799개)
 - 파싱 실패: 40,243개 → 81개 (99.8% 감소)
 - 총 대화 턴: 600,229개 (평균 9.71개/항목)
 - 사용자 턴: 276,554개, 캐릭터 턴: 317,596개
 - 거의 모든 원본 데이터를 활용 가능하도록 개선

### 작업: 원본 데이터 및 파싱된 데이터 품질 검증
- **목적**: 원본 데이터와 파싱된 데이터의 품질을 철저히 검증하여 문제점 파악
- **검증 결과**:
 - **원본 데이터 품질**:
 - 총 라인 수: 61,880개
 - 유효한 JSON: 61,880개 (100%)
 - check_result=True: 61,880개 (100%)
 - completions 있음: 61,880개 (100%)
 - completions 비어있음: 0개
 - completions 50자 미만: 0개
 - scene_data 완전함: 61,880개 (100%)
 - 발견된 문제: 없음
 - **파싱된 데이터 품질**:
 - 총 항목 수: 61,799개 (99.87% 파싱 성공)
 - background 있음: 61,799개 (100%)
 - dialogue 있음: 61,799개 (100%)
 - location 있음: 61,799개 (100%)
 - source 형식 올바름: 61,799개 (100%)
 - Dialogue 턴 문제:
 - role 없음: 0개
 - action 없음: 0개
 - content 없음: 0개
 - content 5자 미만: 0개
 - role 100자 초과: 470개 (정상 범위, 필터링되지 않음)
 - 발견된 문제: 12개 (dialogue 턴이 2개 미만인 항목, 전체의 0.02%)
 - **데이터 매칭 검증**:
 - 원본 데이터 gen_answer_id 수: 61,879개
 - 파싱된 데이터 source 수: 61,798개
 - 매칭된 항목: 61,798개 (99.87%)
 - 파싱 실패한 항목: 81개 (0.13%)
 - 추가된 항목: 0개
- **최종 평가**:
 - 원본 데이터 완성도: 100.00% (문제 없음)
 - 파싱 성공률: 99.87% (61,799/61,880)
 - 데이터 품질: 양호 (경미한 문제 12개, 전체의 0.02%)
 - 구조적 무결성: 우수 (모든 필수 필드 존재, 빈 필드 없음)
 - Dialogue 턴 구조: 정상 (평균 9.71개 턴/항목)
- **결론**:
 - 원본 데이터는 완벽한 상태로 생성됨
 - 파싱된 데이터도 거의 모든 항목이 정상적으로 처리됨
 - 12개의 dialogue 턴이 2개 미만인 항목은 정상적인 변형 범위
 - 전체적으로 데이터 품질이 우수하여 Fine-tuning에 사용 가능

## 2025-12-14

### 작업: trainable-agents 방식으로 학습 코드 전면 수정
- **목적**: 모델이 "Qwen"이라고 답변하거나 페르소나를 따르지 않는 문제 해결을 위해 trainable-agents의 검증된 학습 방식을 적용
- **변경 사항**:
- **학습 데이터 처리 방식 변경** (`train_qlora.py`의 `load_jsonl_dataset` 함수):
 - Chat template 사용 중단
 - 프롬프트와 출력을 단순 연결: `f"{prompt}{output}"`
 - 전체 텍스트를 한 번에 토크나이징
 - input_ids만 반환 (labels 없음, DataCollator가 자동 생성)
- **Loss Masking 제거**:
 - 커스텀 DataCollatorForLanguageModelingWithLabels 제거
 - 기본 DataCollatorForLanguageModeling 사용 (mlm=False)
 - 프롬프트와 출력 모두 loss 계산에 포함 (전체 sequence 학습)
- **하이퍼파라미터 통일**:
 - warmup_ratio: 0.1 → 0.03 (trainable-agents와 동일)
 - optim: adamw_torch → paged_adamw_32bit (trainable-agents와 동일)
 - save_strategy: "steps" → "epoch" (trainable-agents와 동일)
 - logging_steps: 파라미터 무시, 하드코딩 10 (trainable-agents와 동일)
- **적용 파일**: `fairy_tale/scripts/train_qlora.py`
- **변경 전후 비교**:
 - 변경 전: Chat template 적용, assistant만 학습 (loss masking), 복잡한 정규식 처리
 - 변경 후: 단순 연결, 전체 학습 (loss masking 없음), 간단한 처리
- **기대 효과**:
 - 프롬프트-출력 관계를 더 강하게 학습
 - 모델이 프롬프트를 제대로 인식하고 페르소나를 따르도록 개선
 - trainable-agents와 완전히 동일한 방식으로 학습

### 문제: trainable-agents와 학습 방식 차이 분석
- **목적**: 학습 실패 원인 진단을 위한 trainable-agents와 우리 프로젝트의 구조적 차이점 분석
- **분석 범위**:
 - 학습 데이터 처리 방식
 - 프롬프트 구조
 - Output 데이터 구조
 - 학습 과정 및 Loss 추이
- **구조적 일치성**:
 - 데이터 생성 플로우: 동일 (장면 생성 → 대화 생성 → 파싱 → 프롬프트 변환)
 - 학습 방식 (QLoRA): 동일
 - 데이터 형식: 동일 (JSONL, prompt + output 구조)
- **발견된 주요 차이점**:
 - **학습 데이터 처리 방식 (가장 중요)**:
 - trainable-agents: 프롬프트와 출력을 단순 연결 (`f"{prompt}{output}"`), 전체 sequence 학습 (loss masking 없음), DataCollatorForLanguageModeling 사용
 - 우리 프로젝트: chat template 적용하여 system/user/assistant 메시지로 변환, assistant 부분만 학습 (loss masking), 프롬프트와 사용자 메시지는 -100으로 마스킹
 - **프롬프트 구조**:
 - trainable-agents: 위치/상태 정보 포함 (Location, Status), 더 구체적인 컨텍스트 제공
 - 우리 프로젝트: 위치/상태 정보 없음 (실제 VTuber 시스템과 일치하도록 수정함), 페르소나 정보만 포함
 - **Output 시작 패턴**:
 - trainable-agents: "조수미 speaking:"으로 시작 (캐릭터 응답이 먼저 오는 경우도 있음)
 - 우리 프로젝트: "사용자 말하기:"로 시작 (사용자 질문이 먼저 옴)
- **문제 원인 분석**:
 - **1순위: 학습 방식 차이 (가장 가능성 높음)**:
 - trainable-agents는 프롬프트 자체도 학습하여 프롬프트와 출력의 관계를 더 강하게 학습
 - 우리는 assistant만 학습하여 프롬프트-출력 관계 학습이 약할 수 있음
 - 영향: 모델이 프롬프트(페르소나 정보)와 응답의 관계를 제대로 학습하지 못하여, 추론 시 시스템 프롬프트가 있어도 페르소나를 따르지 못함
 - **2순위: 프롬프트 구조 차이**:
 - 위치/상태 정보 부재로 컨텍스트가 부족할 수 있음
 - **3순위: 학습 불충분**:
 - Loss가 1.0 근처에서 수렴, 최저 Loss 0.95 달성 후 증가 추세
 - 3 epochs로는 불충분할 수 있음
- **현재 학습 상태**:
 - 데이터 크기: 61,799개 샘플 (trainable-agents 수준에 근접)
 - 학습 완료: 3 epochs
 - 초기 Loss: 2.58
 - 최저 Loss: 0.95 (중간 지점)
 - 최종 Loss: 1.03 (증가 추세)
 - 과적합 가능성: Loss가 다시 증가하는 추세
- **추론 시 문제점**:
 - 모델이 백설공주로 정체화하지 않음
 - 페르소나를 따르지 않는 일반적인 응답 생성
 - 시스템 프롬프트가 제대로 전달되지 않는 것처럼 보임
- **권장 해결 방안**:
 - **방안 1: trainable-agents 방식으로 학습 방식 변경 (즉시 시도 권장)**:
 - 변경 사항:
 - `train_qlora.py`의 `load_jsonl_dataset` 함수 수정
 - chat template 사용 중단
 - 프롬프트와 출력 단순 연결: `f"{prompt}{output}"`
 - Loss masking 제거 (전체 sequence 학습)
 - DataCollatorForLanguageModeling 사용 (MLM=False)
 - 장점:
 - trainable-agents와 동일한 방식 (검증된 방법)
 - 프롬프트-출력 관계를 더 강하게 학습
 - 최소한의 변경으로 해결 가능
 - 재학습 계획:
 - 기존 하이퍼파라미터 유지
 - 3 epochs부터 시작
- **상세 분석 리포트**: `fairy_tale/ANALYSIS_REPORT.md`에 저장됨

## 2025-12-18

### 분석: 재학습 시 잠재적 문제점 종합 검토
- **목적**: trainable-agents와 학습/프롬프트/데이터 구성요소를 완전히 비교하여 재학습 시 문제가 생기지 않을지 상세하게 판단
- **비교 항목**:
- **1. 학습 데이터 처리 방식**:
 - trainable-agents: `load_jsonl_dataset`에서 `prompt + output` 단순 연결, 전체 텍스트를 한 번에 토크나이징, input_ids만 반환 (labels 없음)
 - 우리 프로젝트 (변경 후): 완전히 동일한 방식
 - DataCollatorForLanguageModeling(mlm=False): input_ids를 labels로 복사, 패딩만 -100으로 마스킹
 - 결과: 프롬프트와 출력 모두 loss 계산에 포함 (전체 sequence 학습)
 - 판단: 완전히 동일, 문제 없음
- **2. Loss Masking**:
 - trainable-agents: Loss masking 없음 (전체 sequence 학습)
 - 우리 프로젝트 (변경 후): Loss masking 없음 (전체 sequence 학습)
 - 판단: 완전히 동일, 문제 없음
- **3. 하이퍼파라미터**:
 - trainable-agents: warmup_ratio=0.03, optim="paged_adamw_32bit", save_strategy="epoch", logging_steps=10, learning_rate=2e-4, epochs=3
 - 우리 프로젝트 (변경 후): 모든 하이퍼파라미터 동일
 - 판단: 완전히 동일, 문제 없음
- **4. QLoRA 설정**:
 - trainable-agents: 4-bit quantization, LoRA r=16, alpha=32, dropout=0.05, target_modules 동일
 - 우리 프로젝트: 모든 설정 동일
 - 판단: 완전히 동일, 문제 없음
- **5. 프롬프트 구조**:
 - trainable-agents: 위치/상태 정보 포함 (`format(character=..., loc_time=..., status=...)`)
 - 우리 프로젝트: 위치/상태 정보 없음 (실제 VTuber 시스템과 일치하도록 수정됨)
 - 차이점: 위치/상태 정보 유무
 - 학습 데이터 프롬프트: "당신은 그림 형제의 동화 \"백설공주\"의 주인공 백설공주입니다.\n\n# 배경 이야기\n..."
 - 추론 시 프롬프트: 동일한 형식 (persona_prompt만 사용)
 - 판단: 실제 사용 프롬프트와 일치하므로 문제 없을 가능성이 높음
- **6. Output 데이터 구조**:
 - trainable-agents: "조수미 speaking: ...<|eot|>\n학생A speaking: ...<|eot|>"
 - 우리 프로젝트: "사용자 말하기: ...<|eot|>\n백설공주 말하기: ...<|eot|>"
 - 형식: role + action + content, <|eot|> 구분자 사용
 - 판단: 형식이 완전히 동일, 문제 없음
- **7. 추론 시 연결성 (가장 주의 필요)**:
 - trainable-agents:
 - 학습 시: prompt + output 단순 연결, chat template 미사용, 원본 텍스트 그대로 토크나이징
 - 추론 시: meta_prompt 그대로 `tokenizer([meta_prompt])` 호출, chat template 미사용, 원본 텍스트 그대로 토크나이징
 - 연결성: 완전히 일치
 - 우리 프로젝트:
 - 학습 시: prompt + output 단순 연결, chat template 미사용, 원본 텍스트 그대로 토크나이징
 - 추론 시: VTuber 시스템이 OpenAI 호환 API 사용, messages 형식으로 전달 (`[{"role": "system", "content": persona_prompt}, {"role": "user", "content": user_input}]`), OpenAI 호환 API가 내부적으로 chat template 적용 가능
 - 연결성: 부분적 일치 (프롬프트 내용은 동일하지만, chat template 적용 여부가 다를 수 있음)
 - 잠재적 문제: Chat Template 불일치 가능성
 - 학습 시: chat template 미사용, 원본 텍스트 그대로
 - 추론 시: OpenAI 호환 API가 chat template 적용하여 토크나이징 가능
 - 위험도: 중간
 - 완화 요인:
 - trainable-agents도 chat template 없이 성공했으므로, 모델이 원본 텍스트 패턴을 학습하면 추론 시에도 인식 가능할 가능성이 높음
 - Qwen3-4B-Instruct는 instruction-tuned 모델이지만, 원본 텍스트 패턴도 학습 가능
 - 학습 데이터의 prompt 형식과 추론 시 system prompt 형식이 일치함 (내용 동일)
- **8. 데이터 변환 과정**:
 - trainable-agents `convert_prompt_data.py`:
 - prompt 생성: `meta_instruction.format(character=character, loc_time=location, status=setting)`
 - output 생성: role + action + content 형식, <|eot|> 구분자
 - 최종: `{"prompt": prompt, "output": output, "source": source}`
 - 우리 프로젝트 `convert_prompt_data.py`:
 - prompt 생성: `meta_instruction` 그대로 사용 (format() 호출 없음, 위치/상태 정보 없음)
 - output 생성: role + action + content 형식, <|eot|> 구분자 (완전히 동일)
 - 최종: `{"prompt": prompt, "output": output, "source": source}` (완전히 동일)
 - 차이점: prompt 생성 방식만 다름 (위치/상태 정보 포함 여부)
 - 판단: output 형식은 완전히 동일, prompt는 실제 사용과 일치하므로 문제 없음
- **종합 판단**:
- **완전히 동일한 부분 (문제 없음)**:
 - 학습 데이터 처리 방식 (단순 연결, 전체 학습)
 - Loss Masking (없음)
 - 하이퍼파라미터 (모두 동일)
 - QLoRA 설정 (모두 동일)
 - Output 데이터 구조 (형식 동일)
- **차이가 있지만 문제 없을 가능성이 높은 부분**:
 - 프롬프트 구조 (위치/상태 정보 없음, 하지만 실제 사용과 일치)
- **주의 깊게 관찰해야 할 부분**:
 - Chat Template 불일치 가능성 (학습 시 미사용, 추론 시 적용 가능)
 - 하지만 trainable-agents도 chat template 없이 성공했으므로, 모델이 원본 텍스트 패턴을 학습하면 추론 시에도 인식 가능할 가능성이 높음
 - 학습 데이터의 prompt 내용과 추론 시 system prompt 내용이 일치하므로, 기본 패턴은 인식 가능할 것으로 예상
- **최종 결론**:
 - 핵심 학습 방식이 trainable-agents와 완전히 동일하므로, 재학습 시 문제가 생길 가능성은 낮음
 - Chat Template 불일치 가능성만 주의 깊게 관찰 필요
 - 재학습 후 Loss가 0.8-0.9 수준으로 감소하고, 모델이 페르소나를 제대로 따르는지 확인 필요
 - 만약 문제가 지속되면, 추론 시 chat template 적용 여부를 확인하고 필요시 학습 시에도 chat template을 사용하는 방안 고려
- **추가 세부 분석**:
- **9. 토크나이저 설정**:
 - trainable-agents: `AutoTokenizer.from_pretrained(base_model, use_fast=False)`, `pad_token = eos_token`, `padding_side = 'right'`
 - 우리 프로젝트: 완전히 동일한 설정
 - 판단: 완전히 동일, 문제 없음
- **10. 모델 로딩 방식**:
 - trainable-agents: 4-bit quantization, `device_map="auto"`, `trust_remote_code=True`
 - 우리 프로젝트: 완전히 동일한 설정
 - 판단: 완전히 동일, 문제 없음
- **11. 추론 시 토크나이징 방식 상세 비교**:
 - trainable-agents 추론 코드:
 - `meta_prompt = f"당신은 조수미입니다...\n위치: {location}\n상태: {status}\n\n상호작용은 다음과 같습니다:\n\n사용자: {user_input}\n조수미:"`
 - `inputs = tokenizer([meta_prompt], return_tensors="pt")`
 - Chat template 미사용, 원본 텍스트 그대로 토크나이징
 - 학습 데이터와 완전히 동일한 형식
 - 우리 프로젝트 추론 (VTuber 시스템):
 - OpenAI 호환 API 사용
 - `messages = [{"role": "system", "content": persona_prompt}, {"role": "user", "content": user_input}]`
 - OpenAI 호환 API가 내부적으로 chat template 적용 가능
 - 학습 데이터와 프롬프트 내용은 동일하지만, 토크나이징 방식이 다를 수 있음
 - 차이점 분석:
 - 학습 시: 원본 텍스트 `"당신은 백설공주입니다...\n\n사용자 말하기: ...\n백설공주 말하기: ..."` 그대로 토크나이징
 - 추론 시 (trainable-agents): 원본 텍스트 `"당신은 조수미입니다...\n\n사용자: {user_input}\n조수미:"` 그대로 토크나이징
 - 추론 시 (우리 프로젝트): OpenAI 호환 API가 messages를 받아서 내부적으로 chat template 적용하여 토크나이징
 - Qwen3-4B-Instruct의 chat template 형식: `<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant\n`
 - 잠재적 영향:
 - 학습 시: 원본 텍스트 패턴 학습 (`"당신은 백설공주입니다..."`로 시작하는 패턴)
 - 추론 시: Chat template 적용 시 `<|im_start|>system\n당신은 백설공주입니다...<|im_end|>\n<|im_start|>user\n...` 형식
 - 모델이 학습한 패턴과 추론 시 패턴이 다를 수 있음
 - 완화 요인:
 - 학습 데이터에서 프롬프트 부분도 학습하므로, 프롬프트 내용 자체는 인식 가능
 - Chat template의 특수 토큰(`<|im_start|>`, `<|im_end|>`)은 모델이 학습하지 않았지만, 프롬프트 내용은 동일하므로 기본 패턴 인식 가능
 - trainable-agents도 chat template 없이 성공했으므로, 원본 텍스트 패턴 학습이 효과적일 가능성이 높음
 - 위험도: 중간 (하지만 완화 요인으로 인해 실제 문제 발생 가능성은 낮음)
- **12. 학습 데이터와 추론 시 프롬프트 내용 일치성**:
 - 학습 데이터 프롬프트: "당신은 그림 형제의 동화 \"백설공주\"의 주인공 백설공주입니다.\n\n# 배경 이야기\n당신은 눈처럼 하얗고...\n\n# 성격 특성\n- 순수하고 선량하며...\n\n..."
 - 추론 시 프롬프트 (VTuber 시스템): `persona_prompt`와 동일한 내용
 - 내용 일치성: 완전히 동일
 - 판단: 프롬프트 내용이 일치하므로, 모델이 학습한 패턴을 추론 시에도 인식 가능할 가능성이 높음
- **13. Output 형식 일치성**:
 - 학습 데이터 output: "사용자 말하기: ...<|eot|>\n백설공주 말하기: ...<|eot|>"
 - 추론 시: 모델이 생성하는 응답은 "백설공주 말하기: ..." 형식이 아닌 일반 텍스트로 생성됨
 - 차이점: 학습 데이터에는 "백설공주 말하기:" 같은 role prefix가 있지만, 추론 시에는 일반 텍스트만 생성
 - 하지만 trainable-agents도 동일한 구조를 사용하고 성공했으므로, 모델이 role prefix를 학습하더라도 일반 텍스트 생성에 문제가 없을 것으로 예상
 - 판단: 문제 없음
- **14. 최종 재학습 시 문제 발생 가능성 평가**:
 - **매우 낮음 (90% 이상 확률로 문제 없음)**:
 - 학습 데이터 처리 방식 완전히 동일
 - Loss masking 완전히 동일
 - 하이퍼파라미터 완전히 동일
 - QLoRA 설정 완전히 동일
 - Output 형식 완전히 동일
 - 프롬프트 내용 일치
 - **낮음 (10% 미만 확률로 문제 발생 가능)**:
 - Chat Template 불일치 가능성
 - 하지만 완화 요인으로 인해 실제 문제 발생 가능성은 매우 낮음
 - **예상 결과**:
 - Loss가 0.8-0.9 수준으로 안정적으로 감소
 - 모델이 "Qwen"이라고 답변하지 않음
 - 모델이 페르소나를 제대로 따름
 - 실제 VTuber 시스템에서 정상 작동
 - **만약 문제가 지속될 경우**:
 - 추론 시 chat template 적용 여부 확인
 - 학습 데이터에 chat template 형식도 포함하는 방안 고려
 - 하지만 trainable-agents 방식이 성공했으므로, 현재 방식으로도 성공할 가능성이 매우 높음
- **15. VTuber 시스템 추론 방식 상세 확인**:
 - VTuber 시스템은 OllamaLLM을 사용하며, 이는 AsyncLLM(OpenAI 호환 API)을 상속받음
 - OpenAI 호환 API는 messages 형식으로 전달받아 내부적으로 chat template 적용 가능
 - 하지만 Ollama의 경우, Modelfile에 "System prompt는 애플리케이션에서 설정"이라고 명시되어 있음
 - 실제 동작:
 - `messages = [{"role": "system", "content": persona_prompt}, {"role": "user", "content": user_input}]`
 - OpenAI 호환 API가 이를 받아서 처리
 - Ollama가 내부적으로 chat template을 적용할 수도 있고, 적용하지 않을 수도 있음
 - 확인 필요 사항:
 - Ollama가 실제로 chat template을 적용하는지 확인
 - 적용한다면 어떤 형식인지 확인
 - 하지만 학습 데이터의 prompt 내용과 추론 시 system prompt 내용이 일치하므로, 기본 패턴은 인식 가능할 것으로 예상
- **16. 최종 재학습 시 문제 발생 가능성 최종 평가**:
 - **완전히 동일한 부분 (문제 발생 가능성 0%)**:
 - 학습 데이터 처리 방식: trainable-agents와 완전히 동일
 - Loss Masking: trainable-agents와 완전히 동일 (없음)
 - 하이퍼파라미터: trainable-agents와 완전히 동일
 - QLoRA 설정: trainable-agents와 완전히 동일
 - Output 데이터 구조: trainable-agents와 완전히 동일
 - 토크나이저 설정: trainable-agents와 완전히 동일
 - 모델 로딩 방식: trainable-agents와 완전히 동일
 - **차이가 있지만 문제 없을 가능성이 높은 부분 (문제 발생 가능성 5% 미만)**:
 - 프롬프트 구조: 위치/상태 정보 없음, 하지만 실제 사용 프롬프트와 일치
 - 프롬프트 내용 일치성: 학습 데이터와 추론 시 완전히 동일
 - **주의 깊게 관찰해야 할 부분 (문제 발생 가능성 10% 미만)**:
 - Chat Template 불일치 가능성:
 - 학습 시: chat template 미사용, 원본 텍스트 그대로
 - 추론 시: OpenAI 호환 API(Ollama)가 chat template 적용 가능
 - 하지만 완화 요인:
 - trainable-agents도 chat template 없이 성공
 - 프롬프트 내용이 일치하므로 기본 패턴 인식 가능
 - 모델이 원본 텍스트 패턴을 학습하면 추론 시에도 인식 가능할 가능성이 높음
 - **종합 평가**:
 - 재학습 시 문제가 생길 가능성: 매우 낮음 (5-10% 미만)
 - 핵심 학습 방식이 trainable-agents와 완전히 동일하므로, 대부분의 경우 정상 작동할 것으로 예상
 - Chat Template 불일치 가능성만 주의 깊게 관찰하면 됨
 - **재학습 후 확인 사항**:
 - Loss 추이: 0.8-0.9 수준으로 안정적으로 감소하는지
 - 모델 응답: "Qwen"이라고 답변하지 않는지
 - 페르소나 준수: 백설공주 캐릭터로 정체화하고 페르소나를 따르는지
 - 실제 VTuber 시스템: 정상 작동하는지
 - **문제 발생 시 대응 방안**:
 - Chat Template 불일치가 원인인 경우:
 - 추론 시 실제로 chat template이 적용되는지 확인
 - 학습 데이터에 chat template 형식도 포함하는 방안 고려
 - 하지만 trainable-agents 방식이 성공했으므로, 현재 방식으로도 성공할 가능성이 매우 높음
- **17. 코드 수정 완료 확인**:
 - `load_jsonl_dataset` 함수: trainable-agents와 완전히 동일한 방식으로 수정 완료
 - DataCollator: 기본 DataCollatorForLanguageModeling 사용, 커스텀 클래스 제거 완료
 - 하이퍼파라미터: trainable-agents와 완전히 동일하게 수정 완료
 - 모든 핵심 부분이 trainable-agents와 완전히 동일하게 수정됨
 - 추가 수정 필요 사항: 없음
- **18. 최종 재학습 준비 상태**:
 - 학습 데이터 처리 방식: trainable-agents와 완전히 동일
 - Loss 계산 방식: trainable-agents와 완전히 동일
 - 하이퍼파라미터: trainable-agents와 완전히 동일
 - QLoRA 설정: trainable-agents와 완전히 동일
 - 코드 구조: trainable-agents와 완전히 동일
 - 재학습 준비 완료: 모든 수정 사항 완료, 바로 재학습 가능

### 문제: 추론 시 같은 텍스트 반복 출력 및 과도하게 긴 응답 생성
- **증상**:
 - 대화 중 같은 텍스트가 반복적으로 출력됨
 - 한 번 출력할 때 말을 엄청 길게 쏟아내는 경향
- **원인 분석**:
 - trainable-agents와 비교하여 생성 파라미터 설정 차이 발견:
 - trainable-agents: `max_new_tokens=300`, `repetition_penalty=1.1`, `no_repeat_ngram_size=3`, `stop_strings` 처리
 - Open-LLM-VTuber: Modelfile에 `num_predict 2048` (너무 큼), 반복 방지 파라미터 없음
- **해결**:
 - Modelfile 수정 (`models/snow_white/Modelfile`):
 - `num_predict`: 2048 → 300 (trainable-agents의 `max_new_tokens=300`과 동일)
 - `top_p`: 0.9 → 0.95 (trainable-agents와 동일)
 - `repeat_penalty 1.1` 추가 (trainable-agents의 `repetition_penalty=1.1`과 동일)
 - `repeat_last_n 64` 추가 (반복 방지 범위 설정)
 - **예상 효과**:
 - 같은 텍스트 반복 출력 문제 해결: `repeat_penalty`로 반복 방지
 - 과도하게 긴 응답 문제 해결: `num_predict 300`으로 최대 토큰 수 제한
 - trainable-agents와 동일한 생성 파라미터로 일관된 응답 품질 확보
- **참고**:
 - trainable-agents는 `stop_strings`를 후처리로 처리하지만, Ollama의 경우 Modelfile에서 설정한 값이 우선 적용됨
 - OpenAI 호환 API를 사용하는 경우 `stop` 파라미터를 추가로 전달할 수 있지만, Ollama Modelfile 설정으로 충분함

## 2025-12-18

### 분석: 코드 구성 및 데이터 흐름 종합 검토
- **검토 목적**: `result/2025-12-10` 데이터를 사용한 학습 가능 여부 확인 및 데이터 흐름 연결 검증
- **검토 결과**:
 - **1. 원본 데이터 (`result/2025-12-10/gen_dialogue/`) 검증**:
 - 데이터 형식: JSONL (각 줄이 독립적인 JSON 객체)
 - 총 라인 수: 약 61,880개
 - 데이터 구조: `{"gen_answer_id": "...", "prompt": "...", "completions": "...", "check_result": true/false, "scene_data": {...}}`
 - `completions` 필드: LLM이 생성한 대화 텍스트 (마크다운 없이 순수 텍스트 형식)
 - 마크다운 포함 여부: 없음 (LLM이 마크다운 없이 생성하도록 프롬프트 지시)
 - 학습 가능 여부: 가능 (파싱 및 변환 후 학습 가능)
 - **2. 파싱 스크립트 (`parse_data_dialogue.py`) 검증**:
 - 마크다운 제거 함수 (`clean_markdown`): 정상 작동 확인
 - `**text**` → `text` 제거
 - `## text` → `text` 제거
 - `### text` → `text` 제거
 - 모든 마크다운 기호 제거 확인
 - 대화 턴 파싱 함수 (`parse_dialogue_turns`): 정상 작동
 - "사용자 (말하기)", "백설공주 (말하기)" 패턴 인식
 - 배경 추출 및 제거 기능
 - 캐릭터 이름 정규화 기능
 - 출력 형식: `processed/{파싱날짜}/generated_agent_dialogue_{character}-{language}.json`
 - 현재 상태: `processed/2025-12-10/generated_agent_dialogue_snow_white-korean.json` 파일 없음 (파싱 필요)
 - **3. 변환 스크립트 (`convert_prompt_data.py`) 검증**:
 - 입력: 파싱된 대화 데이터 JSON 파일
 - 처리 과정:
 - `clean_text()` 함수로 추가 정리 (따옴표, 영어 단어, 공백 정리)
 - 메타 프롬프트 로드 및 적용 (위치/상태 정보 없이 persona_prompt만 사용)
 - 대화 턴을 학습 형식으로 변환 (`{role} {action}: {content}<|eot|>`)
 - 출력 형식: `processed/{파싱날짜}/prompted/prompted_agent_dialogue_{character}.jsonl`
 - 출력 데이터 구조: `{"prompt": "...", "output": "...", "source": "..."}`
 - 마크다운 제거: `clean_text()` 함수로 추가 정리 수행
 - 현재 상태: `processed/2025-12-10/prompted/prompted_agent_dialogue_snow_white.jsonl` 파일 없음 (변환 필요)
 - **4. 학습 스크립트 (`train_qlora.py`) 검증**:
 - 입력 형식: JSONL 파일 (각 줄이 `{"prompt": "...", "output": "...", "source": "..."}` 형식)
 - 데이터 로드: `load_jsonl_dataset()` 함수 사용
 - 처리 방식: `prompt + output` 단순 연결 (trainable-agents 방식)
 - 기본 경로: `training_data/{character}/training_data.jsonl` (하지만 실제로는 `processed/{date}/prompted/` 경로 사용 가능)
 - 마크다운 처리: 학습 데이터에 마크다운이 포함되어 있으면 그대로 학습됨 (따라서 파싱/변환 단계에서 제거 필수)
 - **5. 데이터 흐름 연결 확인**:
 - `result/2025-12-10/gen_dialogue/` → `parse_data_dialogue.py` → `processed/2025-12-10/generated_agent_dialogue_{character}-{language}.json`
 - `processed/2025-12-10/generated_agent_dialogue_{character}-{language}.json` → `convert_prompt_data.py` → `processed/2025-12-10/prompted/prompted_agent_dialogue_{character}.jsonl`
 - `processed/2025-12-10/prompted/prompted_agent_dialogue_{character}.jsonl` → `train_qlora.py` (학습)
 - 모든 단계가 정상적으로 연결됨
 - **6. 마크다운 제거 검증**:
 - 원본 데이터: 마크다운 없음 (LLM이 마크다운 없이 생성)
 - 파싱 단계: `clean_markdown()` 함수로 마크다운 제거 (혹시 모를 마크다운 대비)
 - 변환 단계: `clean_text()` 함수로 추가 정리 (따옴표, 영어 단어, 공백 정리)
 - 학습 단계: 마크다운이 포함된 데이터는 학습되지 않음 (파싱/변환 단계에서 제거됨)
 - 결론: `processed` 디렉토리에 저장되는 데이터는 마크다운이 완전히 제거된 순수 텍스트만 포함됨
 - **7. `result/2025-12-10` 데이터 학습 가능 여부**:
 - 원본 데이터 형식 정상 (JSONL, check_result 필드 포함)
 - 데이터 구조 정상 (필수 필드 모두 포함)
 - 마크다운 없음 (순수 텍스트 형식)
 - 파싱 스크립트 호환 (데이터 형식 일치)
 - 변환 스크립트 호환 (파싱된 데이터 형식 일치)
 - 학습 스크립트 호환 (변환된 데이터 형식 일치)
 - **결론**: `result/2025-12-10` 데이터는 학습에 사용 가능하며, 파싱 및 변환 후 정상적으로 학습 가능
 - **8. 잠재적 문제점 및 해결 방안**:
 - 문제점 없음: 모든 단계가 정상적으로 연결되어 있으며, 마크다운 제거 로직도 정상 작동
 - 권장 사항:
 1. `result/2025-12-10` 데이터 파싱: `parse_data_dialogue.py` 실행
 2. 파싱된 데이터 변환: `convert_prompt_data.py` 실행
 3. 변환된 데이터로 학습: `train_qlora.py` 실행
 - 데이터 품질: 원본 데이터가 마크다운 없이 생성되었으므로, 파싱/변환 후에도 깨끗한 데이터가 생성될 것으로 예상

### 분석: 파싱 코드와 변환 코드의 차이점 및 trainable-agents 비교
- **질문**: 변환 스크립트(`convert_prompt_data.py`)가 trainable-agents에도 존재하는지, 파싱 코드와의 차이점은 무엇인지
- **답변**:
 - **1. trainable-agents에도 변환 스크립트 존재**:
 - `trainable-agents/parser/convert_prompt_data.py` 존재
 - 동일한 목적: 구조화된 대화 데이터를 학습용 형식으로 변환
 - **2. 파싱 코드(`parse_data_dialogue.py`)와 변환 코드(`convert_prompt_data.py`)의 차이점**:
 - **파싱 코드의 역할**:
 - 입력: 원본 LLM 생성 데이터 (JSONL 형식, `result/` 디렉토리)
 - 예: `{"gen_answer_id": "...", "prompt": "...", "completions": "...", "check_result": true, "scene_data": {...}}`
 - 처리:
 - `completions` 필드에서 마크다운 제거 (`clean_markdown()`)
 - 대화 턴 파싱 (`parse_dialogue_turns()`)
 - 배경 추출 및 정리
 - 캐릭터 이름 정규화
 - 사용자 질문 추가 (페르소나 LLM 형식)
 - 출력: 구조화된 대화 데이터 (JSON 형식, `processed/` 디렉토리)
 - 예: `{"background": "...", "dialogue": [{"role": "...", "action": "...", "content": "..."}], "location": "...", "source": "..."}`
 - 목적: 원본 데이터를 구조화하여 다음 단계에서 사용 가능한 형식으로 변환
 - **변환 코드의 역할**:
 - 입력: 구조화된 대화 데이터 (JSON 형식, `processed/` 디렉토리)
 - 예: `{"background": "...", "dialogue": [{"role": "...", "action": "...", "content": "..."}], "location": "...", "source": "..."}`
 - 처리:
 - 메타 프롬프트 로드 및 적용
 - 대화 턴을 학습 형식으로 변환 (`{role} {action}: {content}<|eot|>`)
 - 텍스트 정리 (`clean_text()`)
 - 학습용 데이터 구조 생성 (`{"prompt": "...", "output": "...", "source": "..."}`)
 - 출력: 학습용 데이터 (JSONL 형식, `processed/{date}/prompted/` 디렉토리)
 - 예: `{"prompt": "...", "output": "사용자 (말하기): ...<|eot|>\n백설공주 (말하기): ...<|eot|>", "source": "..."}`
 - 목적: 구조화된 데이터를 실제 학습에 사용할 수 있는 형식으로 변환
 - **요약**:
 - 파싱: 원본 데이터 → 구조화된 데이터 (데이터 정제 및 구조화)
 - 변환: 구조화된 데이터 → 학습용 데이터 (학습 형식으로 변환)
 - **3. trainable-agents vs Open-LLM-VTuber의 변환 코드 차이점**:
 - **trainable-agents 방식**:
 ```python
 setting = ex['background']
 location = ex['location']
 prompt = meta_instruction.format(character=character, loc_time=location, status=setting)
 ```
 - 프롬프트에 위치(`loc_time`)와 상태(`status`) 정보를 포함
 - 메타 프롬프트 템플릿에 `{character}`, `{loc_time}`, `{status}` 플레이스홀더 사용
 - **Open-LLM-VTuber 방식**:
 ```python
 # 위치/상태 정보는 실제 사용 시 포함되지 않으므로 학습 데이터에서도 제거
 # 실제 사용 프롬프트 형식과 일치시키기 위해 위치/상태 정보 없이 persona_prompt만 사용
 prompt = meta_instruction # format() 호출 없이 그대로 사용
 ```
 - 프롬프트에 위치/상태 정보를 포함하지 않음
 - 실제 VTuber 시스템에서 사용하는 프롬프트 형식과 일치시킴
 - 이유: 실제 추론 시에는 위치/상태 정보가 포함되지 않으므로, 학습 시에도 동일한 형식 사용
 - **차이점 요약**:
 | 항목 | trainable-agents | Open-LLM-VTuber |
 |------|------------------|-----------------|
 | 프롬프트 형식 | `meta_instruction.format(character, loc_time, status)` | `meta_instruction` (그대로 사용) |
 | 위치 정보 | 포함 | 제외 |
 | 상태 정보 | 포함 | 제외 |
 | 이유 | 장면별 컨텍스트 제공 | 실제 사용 형식과 일치 |
 - **4. 데이터 흐름 비교**:
 - **trainable-agents**:
 ```
 원본 데이터 (JSONL)
 → parse_data_dialogue.py (파싱)
 → 구조화된 데이터 (JSON, location/status 포함)
 → convert_prompt_data.py (변환)
 → 학습용 데이터 (JSONL, prompt에 location/status 포함)
 ```
 - **Open-LLM-VTuber**:
 ```
 원본 데이터 (JSONL)
 → parse_data_dialogue.py (파싱)
 → 구조화된 데이터 (JSON, location/status 포함)
 → convert_prompt_data.py (변환)
 → 학습용 데이터 (JSONL, prompt에 location/status 제외)
 ```
 - **5. 결론**:
 - trainable-agents에도 `convert_prompt_data.py` 존재
 - 파싱과 변환은 서로 다른 목적을 가진 별도의 단계
 - Open-LLM-VTuber는 실제 사용 형식과 일치시키기 위해 위치/상태 정보를 제외
 - 이는 이전 분석에서 확인한 "프롬프트 구조 차이"의 핵심 부분

## 2025-12-18 (추가)

### 문제: CUDA Out of Memory (OOM) 에러 발생
- **증상**:
 - 학습 시작 직후 CUDA OOM 에러 발생
 - 에러 메시지: `torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.03 GiB. GPU 0 has a total capacity of 23.53 GiB of which 3.22 GiB is free.`
 - 프로세스 메모리 사용: 16.93 GiB (PyTorch: 14.64 GiB 할당, 1.84 GiB 예약)
- **원인 분석**:
 - 배치 크기: 4 (per_device_train_batch_size)
 - Gradient accumulation: 4
 - Max length: 2048
 - 실제 배치 크기: 4 × 4 = 16
 - Qwen3-4B 모델 (4-bit 양자화) + LoRA 어댑터
 - Gradient checkpointing 활성화되어 있지만 여전히 메모리 부족
 - PyTorch 메모리 단편화 문제 (1.84 GiB 예약되었지만 할당되지 않음)
- **해결 방안**:
 1. **배치 크기 감소**: `--batch_size 4` → `--batch_size 2` 또는 `1`
 2. **Gradient accumulation 증가**: `--grad_accum 4` → `--grad_accum 8` 또는 `16` (실제 배치 크기 유지)
 3. **Max length 감소**: `--max_length 2048` → `--max_length 1024` (메모리 사용량 대폭 감소)
 4. **PyTorch 메모리 단편화 해결**: `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` 환경변수 설정
 5. **추가 메모리 최적화**: `dataloader_pin_memory=False` 설정 (이미 추가됨)
- **권장 설정**:
 ```bash
 export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
 export WANDB_API_KEY='3e7f8b12dd95a20cd59497dd792f8617accd810b'
 python scripts/train_qlora.py \
 --character snow_white \
 --data_path processed/2025-12-18/prompted/prompted_agent_dialogue_snow_white.jsonl \
 --base_model Qwen/Qwen3-4B-Instruct-2507 \
 --output_dir checkpoints/snow_white \
 --epochs 3 \
 --batch_size 2 \
 --grad_accum 8 \
 --learning_rate 2e-4 \
 --max_length 1024
 ```
 - 실제 배치 크기: 2 × 8 = 16 (기존과 동일)
 - 메모리 사용량: 약 50% 감소 예상 (max_length 2048 → 1024)
- **수정 사항**:
 - Wandb 프로젝트 이름: `open-llm-vtuber-fairy-tale` → `dreamtale`
 - TrainingArguments에 `dataloader_pin_memory=False` 추가 (메모리 절약)

### 분석: GPU 메모리 사용량 원인 분석
- **질문**: 학습이 실패했는데도 GPU 메모리가 사용 중인 이유
- **분석 결과**:
 - **1. Ollama 프로세스 (PID 4062110)**:
 - GPU 메모리 사용: 약 3.4GB
 - 실행 시간: 1일 15시간 33분 전부터 실행 중 (Dec 16부터)
 - 상태: 백그라운드에서 모델을 메모리에 로드해두고 있음
 - 로드된 모델: `snow_white:latest` 또는 다른 모델 (Ollama가 keep_alive 설정에 따라 메모리에 유지)
 - 명령어: `/usr/local/bin/ollama runner --ollama-engine --model ...`
 - **2. 오래된 Python 프로세스들 (PID 683601, 686783)**:
 - 실행 시간: 51일 전부터 실행 중 (Oct 27부터)
 - CPU 사용률: 99.9% (각각)
 - GPU 사용: 없음 (CPU만 사용)
 - 영향: GPU 메모리에는 직접적인 영향 없음, 하지만 시스템 리소스 점유
 - **3. GPU 메모리 상태**:
 - 총 메모리: 24,564 MiB (약 24GB)
 - 사용 중: 3,449 MiB (약 3.4GB)
 - 사용률: 약 14%
 - 주요 사용자: Ollama 프로세스 (약 3.4GB)
 - **4. 해결 방안**:
 - **Ollama 모델 언로드** (권장):
 ```bash
 # 실행 중인 모델 확인
 ollama ps
 
 # 특정 모델 언로드
 ollama stop snow_white
 
 # 또는 다른 모델이 실행 중이면 해당 모델 이름으로 언로드
 # ollama stop <model_name>
 ```
 - **Ollama 프로세스 종료** (필요한 경우):
 ```bash
 kill 4062110
 # 또는
 pkill ollama
 ```
 - **오래된 Python 프로세스 정리** (선택사항):
 ```bash
 # 프로세스 확인 후 종료
 ps -p 683601,686783
 kill 683601 686783
 ```
 - **GPU 메모리 확인**:
 ```bash
 nvidia-smi
 # 또는
 nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv
 ```
 - **5. 권장 사항**:
 - 학습 전에 Ollama 모델을 언로드하여 GPU 메모리를 확보
 - 학습 후에는 Ollama를 다시 시작하여 모델 로드 가능
 - 오래된 프로세스는 정기적으로 정리하여 시스템 리소스 확보
 - **6. 메모리 확보 후 학습 재시도**:
 ```bash
 # Ollama 모델 언로드
 ollama stop snow_white
 
 # GPU 메모리 확인
 nvidia-smi
 
 # 학습 재시도
 export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
 export WANDB_API_KEY='3e7f8b12dd95a20cd59497dd792f8617accd810b'
 python scripts/train_qlora.py \
 --character snow_white \
 --data_path processed/2025-12-18/prompted/prompted_agent_dialogue_snow_white.jsonl \
 --base_model Qwen/Qwen3-4B-Instruct-2507 \
 --output_dir checkpoints/snow_white \
 --epochs 3 \
 --batch_size 2 \
 --grad_accum 8 \
 --learning_rate 2e-4 \
 --max_length 1024
 ```

### 분석: 학습 과적합 가능성 평가
- **질문**: 학습 진행 중 과적합 가능성 여부
- **현재 학습 상태** (Epoch 0.17, 약 6% 진행):
 - 초기 Loss: 0.4801
 - 현재 Loss: 0.4591
 - 최소 Loss: 0.4578
 - Loss 감소율: 4.37%
 - Loss 변동성: 0.0060 (낮음, 안정적)
 - Gradient norm: 0.18-0.22 (안정적)
 - Learning rate: Cosine scheduler로 점진적 감소 중
- **과적합 가능성 분석**:
 - **1. 데이터 크기 vs 에폭 수**:
 - 학습 데이터: 61,799개 샘플
 - 에폭 수: 3
 - 총 학습 스텝: 약 11,589 스텝
 - 평가: 데이터 크기가 충분히 크므로 3 에폭은 적절함
 - **2. Validation set 부재**:
 - 현재 설정: Validation set 없음 (trainable-agents와 동일)
 - 문제점: Validation loss로 과적합 감지 불가
 - 완화 요인:
 - trainable-agents도 validation set 없이 성공
 - QLoRA는 파라미터 수가 적어 과적합 가능성이 낮음 (trainable params: 0.8145%)
 - 데이터 크기가 충분히 큼 (61,799개)
 - **3. Early Stopping 부재**:
 - 현재 설정: Early stopping 없음 (trainable-agents와 동일)
 - 문제점: 과적합 발생 시 자동 중단 불가
 - 완화 요인:
 - trainable-agents도 early stopping 없이 성공
 - 에폭 수가 적음 (3 에폭)
 - Loss가 안정적으로 감소 중 (변동성 낮음)
 - **4. Loss 추이 분석**:
 - Loss가 점진적으로 감소 중 (0.4801 → 0.4591)
 - Loss 변동성이 낮음 (0.0060) → 안정적인 학습
 - Gradient norm이 안정적 (0.18-0.22) → 폭발/소실 없음
 - 평가: 현재까지는 정상적인 학습 진행 중
 - **5. QLoRA의 과적합 방지 메커니즘**:
 - LoRA rank (r=16): 작은 파라미터 수로 과적합 가능성 낮음
 - LoRA dropout (0.05): 정규화 효과
 - Gradient checkpointing: 메모리 절약 + 간접적 정규화 효과
 - 4-bit 양자화: 모델 용량 제한으로 과적합 가능성 감소
 - **6. trainable-agents와 비교**:
 - trainable-agents: Validation set 없음, Early stopping 없음, 3 에폭 사용
 - Open-LLM-VTuber: 동일한 설정
 - 평가: trainable-agents가 성공했으므로 동일한 설정으로도 문제 없을 가능성이 높음
- **과적합 가능성 평가**:
 - **낮음 (10-15% 확률)**:
 - 데이터 크기가 충분히 큼 (61,799개)
 - QLoRA 파라미터 수가 적음 (0.8145%)
 - 에폭 수가 적음 (3 에폭)
 - Loss가 안정적으로 감소 중
 - trainable-agents와 동일한 설정으로 성공
 - **주의 사항**:
 - Validation set이 없어서 정확한 과적합 감지 불가
 - 학습 완료 후 실제 추론에서 성능 확인 필요
 - Loss가 계속 감소하다가 갑자기 증가하면 과적합 가능성
- **권장 사항**:
 - 현재 설정으로 학습 완료 후 실제 추론 테스트
 - Loss가 0.3 이하로 안정적으로 수렴하면 정상
 - Loss가 0.2 이하로 계속 감소하면 과적합 가능성 있음
 - 실제 VTuber 시스템에서 테스트하여 페르소나 준수 여부 확인
 - 만약 과적합이 발생하면:
 - 에폭 수 감소 (3 → 2)
 - Learning rate 감소 (2e-4 → 1e-4)
 - LoRA dropout 증가 (0.05 → 0.1)
 - 데이터 증강 또는 더 많은 데이터 추가

### 작업: QLoRA 학습 완료 및 모델 병합
- **날짜**: 2025-12-19
- **학습 완료**:
 - 총 학습 시간: 약 22시간 40분
 - 총 스텝: 11,589 스텝
 - 최종 평균 Loss: 0.4078
 - 최종 Loss: 0.3671 (Epoch 3.0)
 - LoRA 어댑터 저장 위치: `checkpoints/snow_white/`
- **모델 병합 완료**:
 - 베이스 모델: Qwen/Qwen3-4B-Instruct-2507
 - LoRA 어댑터: `checkpoints/snow_white/`
 - 병합 모델 저장 위치: `models/snow_white/`
 - Modelfile 생성 완료
- **문제 발생**: Ollama 모델 생성 시 에러
 - 에러 메시지: `Error: unsupported architecture "Qwen3ForCausalLM"`
 - 원인: Ollama 0.13.1 버전이 Qwen3 아키텍처를 직접 지원하지 않음
 - 해결 방법: GGUF 변환 필요 (README 8단계 참조)
- **다음 단계**: GGUF 변환 및 Ollama 등록
 - llama.cpp를 사용한 GGUF 변환
 - 양자화 (q4_0) 적용하여 모델 크기 감소
 - GGUF 모델로 Ollama 등록

### 작업: README 및 Modelfile 생성 스크립트 수정
- **날짜**: 2025-12-19
- **문제점**:
 - README 7단계: `train_character.py --merge` 사용 지시가 있으나, `merge_and_export_ollama.py` 직접 사용이 더 명확함
 - README 8단계 GGUF Modelfile: 이전 최적화 설정과 불일치 (반복 출력 및 긴 응답 방지 설정 누락)
 - `merge_and_export_ollama.py`: Modelfile 생성 시 이전 최적화 설정 미반영 (`num_predict 2048`, `repeat_penalty` 없음)
- **수정 사항**:
 - **README 7단계 수정**:
 - `train_character.py --merge` 대신 `merge_and_export_ollama.py` 직접 사용 명령어로 변경
 - HuggingFace 토큰 설정 방법 명시
 - `--create_modelfile` 옵션 추가
 - **README 8단계 GGUF Modelfile 수정**:
 - `num_predict`: 150 → 300 (trainable-agents의 `max_new_tokens=300`과 동일)
 - `temperature`: 0.1 → 0.7 (자연스러운 대화를 위한 적절한 창의성)
 - `top_p`: 0.9 → 0.95 (trainable-agents와 동일)
 - `top_k`: 30 → 40 (적절한 토큰 선택 범위)
 - `repeat_penalty`: 1.2 → 1.1 (trainable-agents와 동일)
 - `repeat_last_n 64` 추가 (반복 방지 범위 설정)
 - **`merge_and_export_ollama.py` 수정**:
 - Modelfile 생성 시 최적화 설정 반영:
 - `num_predict`: 2048 → 300
 - `top_p`: 0.9 → 0.95
 - `repeat_penalty 1.1` 추가
 - `repeat_last_n 64` 추가
- **예상 효과**:
 - 반복 출력 문제 방지: `repeat_penalty 1.1`로 반복 방지
 - 긴 응답 문제 방지: `num_predict 300`으로 최대 토큰 수 제한
 - trainable-agents와 동일한 생성 파라미터로 일관된 응답 품질 확보
 - README 명령어가 더 명확하고 실행 가능하도록 개선

### 작업: 새로 학습한 모델 GGUF 변환 및 Ollama 등록 완료
- **날짜**: 2025-12-19
- **문제점**:
 - GGUF 변환 시 디렉토리 없음 에러 발생 (`FileNotFoundError`)
 - 원인: `snow_white_gguf` 디렉토리가 백업되어 없어짐
- **해결 과정**:
 1. **디렉토리 재생성**: `models/snow_white_gguf/` 디렉토리 생성
 2. **GGUF 변환**: 새로 학습한 모델(`models/snow_white/`)을 GGUF 형식으로 변환
 - 입력: `/home/ssai/Open-LLM-VTuber/fairy_tale/models/snow_white`
 - 출력: `model-f16.gguf` (8.05GB)
 - 변환 시간: 약 15초
 3. **양자화**: F16 모델을 Q4_0으로 양자화
 - 입력: `model-f16.gguf` (8.05GB)
 - 출력: `model-q4_0.gguf` (2.3GB)
 - 양자화 시간: 약 9.5초
 - 모델 크기 감소: 8.05GB → 2.3GB (약 71% 감소)
 4. **Modelfile 생성**: 최적화된 생성 파라미터로 Modelfile 생성
 - `temperature 0.7`
 - `top_p 0.95`
 - `top_k 40`
 - `num_predict 300`
 - `repeat_penalty 1.1`
 - `repeat_last_n 64`
 5. **Ollama 등록**: 새로 학습한 모델을 Ollama에 등록
 - 모델명: `snow_white:latest`
 - 모델 크기: 2.4GB
 - 등록 시간: 약 2초
- **결과**:
 - 새로 학습한 모델(2025-12-19)이 GGUF 형식으로 변환됨
 - 양자화로 모델 크기 감소 (8.05GB → 2.3GB)
 - Ollama에 정상 등록됨
 - 최적화된 생성 파라미터 적용됨 (반복 출력 및 긴 응답 방지)
- **다음 단계**: 9단계 (VTuber 설정)로 진행 가능

### 문제: 모델 응답 품질 저하 및 페르소나 미준수
- **날짜**: 2025-12-19
- **발견된 문제**:
 - 모델이 "백설공주"로 응답하지 않고 "이곳의 수호자"로 응답
 - `<|eot|>` 토큰이 출력에 포함됨
 - 모델이 자체적으로 질문을 생성하는 이상한 패턴
 - 페르소나를 전혀 따르지 않음
- **원인 분석**:
 - **1. 학습 데이터 형식과 추론 시 프롬프트 형식 불일치**:
 - 학습 데이터: `prompt (persona_prompt) + output ("사용자 말하기: ... <|eot|> 백설공주 말하기: ... <|eot|>")`
 - 실제 추론 시: `System: [persona_prompt]` + `User: [사용자 입력]` (명시적인 "백설공주:" 프롬프트 없음)
 - trainable-agents는 추론 시 `"사용자: {input}\n조수미:"` 형식으로 명시적 응답 시작 지점 지정
 - Open-LLM-VTuber는 그런 프롬프트 없이 바로 응답 기대
 - **2. trainable-agents vs Open-LLM-VTuber 추론 방식 차이**:
 - **trainable-agents**:
 - HuggingFace Transformers의 `model.generate()` 직접 사용
 - 프롬프트: `"당신은 조수미입니다... 사용자: {input}\n조수미:"`
 - 후처리: `response.split("조수미:")[-1].strip()` + `<|eot|>` 제거
 - `skip_special_tokens=True` 사용
 - `stop_strings = ["\n사용자:", "\n\n사용자:", "사용자:", "<|eot|>", "<|im_end|>"]` 설정
 - **Open-LLM-VTuber**:
 - Ollama API 사용 (OpenAI 호환)
 - System prompt + Messages 형식
 - 후처리 없음
 - `<|eot|>` 토큰 제거 로직 없음
 - Stop words 설정 없음
 - **3. GGUF 변환 문제 가능성**:
 - GGUF 변환 과정에서 특수 토큰 처리 문제 가능
 - Ollama의 토큰화 방식이 HuggingFace와 다를 수 있음
 - Modelfile에 stop words 설정 부족 (`<|eot|>` 등)
 - GGUF 변환 전 원본 모델로 테스트 필요
 - **4. 학습 데이터 형식 문제**:
 - 학습 데이터의 "사용자 말하기/백설공주 말하기" 형식이 모델에 학습됨
 - 실제 추론 시에는 이 형식이 없어서 모델이 혼란스러워함
 - trainable-agents도 동일한 형식의 학습 데이터를 사용하지만, 추론 시 "조수미:" 프롬프트로 명시적 지시
- **해결 방안**:
 - **방안 1: GGUF 변환 전 원본 모델 테스트** (우선 확인 필요):
 - HuggingFace Transformers로 직접 로드
 - trainable-agents와 동일한 방식으로 추론 테스트
 - 문제 발생 여부 확인하여 GGUF 변환 문제인지 특정
 - **방안 2: 추론 시 프롬프트 형식 수정**:
 - System prompt에 "백설공주:" 같은 명시적 응답 시작 지점 추가
 - 또는 User 메시지에 "백설공주:" 프롬프트 포함
 - **방안 3: 후처리 로직 추가**:
 - `<|eot|>` 토큰 제거
 - "백설공주:" 이후 부분만 추출
 - Stop strings 처리
 - **방안 4: Modelfile에 stop words 추가**:
 - `<|eot|>` 토큰을 stop word로 설정
 - Ollama의 stop words 기능 활용
 - **방안 5: 학습 데이터 형식 수정** (재학습 필요):
 - 추론 시 형식과 일치하도록 학습 데이터 재구성
 - "사용자 말하기/백설공주 말하기" 형식 제거
 - 직접 응답 형식으로 변경
- **다음 단계**:
 1. GGUF 변환 전 원본 모델로 직접 추론 테스트 (trainable-agents 방식)
 2. 문제 발생 여부 확인하여 GGUF 변환 문제인지 특정
 3. 문제 원인에 따라 적절한 해결 방안 적용

### 테스트 결과: GGUF 변환 문제 아님, 학습 자체의 문제
- **날짜**: 2025-12-19
- **테스트 방법**:
 - GGUF 변환 전 원본 HuggingFace 모델로 직접 추론 테스트
 - trainable-agents와 동일한 방식으로 추론 (`model.generate()` 직접 사용)
 - 학습 데이터 형식과 정확히 일치하는 프롬프트로도 테스트
- **테스트 결과**:
 - **일반 추론 테스트**:
 - 입력: "안녕 너는 누구야?"
 - 응답: "나는 이곳에서 오랜 시간을 보내온 작은 새야" (완전히 다른 페르소나)
 - 입력: "백설공주님, 난쟁이들과 함께 살았던 이야기를 들려주세요"
 - 응답: "난제리들과의 시간이 가장 소중했어요" (오타 포함, 페르소나 미준수)
 - **학습 데이터 형식 일치 테스트**:
 - 학습 데이터의 prompt + output 시작 부분을 그대로 사용
 - 응답: "난쟁이 친구들이 우리에게 웃음을 나누거나..." (난쟁이 언급은 있으나 백설공주 언급 없음)
 - "user님"이라고 응답 (이상한 패턴)
- **결론**:
 - **GGUF 변환 문제가 아님**: 원본 모델로 직접 테스트해도 동일한 문제 발생
 - **학습 자체의 문제**: 모델이 페르소나를 제대로 학습하지 못함
 - **학습 데이터 형식 문제**: 학습 데이터의 "사용자 말하기/백설공주 말하기" 형식이 모델에 학습되었지만, 페르소나 자체는 학습되지 않음
 - **근본 원인**: 
 1. 학습 데이터에서 백설공주가 자신을 "백설공주"라고 명시적으로 언급하지 않음
 2. 학습 데이터의 대화가 페르소나를 충분히 반영하지 못함
 3. 학습 과정에서 persona_prompt가 제대로 학습되지 않음
- **해결 방안**:
 - **즉시 해결 (추론 시 수정)**:
 1. 추론 시 프롬프트에 "백설공주:" 명시적 추가
 2. 후처리로 `<|eot|>` 토큰 제거
 3. Stop words 설정 (`<|eot|>` 등)
 - **근본 해결 (재학습 필요)**:
 1. 학습 데이터 수정: 백설공주가 자신을 명시적으로 언급하도록
 2. 학습 데이터의 대화가 페르소나를 더 강하게 반영하도록
 3. 학습 데이터 형식을 추론 시 형식과 일치시키기
 4. 더 많은 학습 데이터 또는 더 긴 학습 필요

### 최종 분석: trainable-agents와의 근본적 차이점 발견
- **날짜**: 2025-12-19
- **종합 테스트 결과**:
 - 다양한 프롬프트 형식으로 테스트했으나 모두 실패
 - 형식 1 (사용자: ... 백설공주:): "작은 새"로 응답
 - 형식 2 (사용자 말하기: ... 백설공주 말하기:): 페르소나 미준수
 - 형식 3 (명시적 자기소개 포함): 여전히 페르소나 미준수
- **핵심 발견: 학습 데이터 자체의 문제**:
 - **trainable-agents 학습 데이터**:
 - 조수미 언급 비율: 59% (처음 100개 샘플 기준)
 - "조수미 speaking:" 형식으로 항상 명시적 언급
 - 음악 관련 언급: 30%
 - 자기소개: 34%
 - **Open-LLM-VTuber 학습 데이터**:
 - 백설공주 언급 비율: 1.3-2.5% (처음 1000개 샘플 기준)
 - "백설공주 말하기:" 형식이지만 응답에 백설공주 언급이 거의 없음
 - 난쟁이 언급: 41% (페르소나 요소는 언급하나 자신은 언급 안 함)
 - 잘못된 페르소나: 20.8% (수호자, 새 등)
 - **결론**: 학습 데이터에서 백설공주가 자신을 언급하지 않는 대화가 97.5% 이상
- **trainable-agents vs Open-LLM-VTuber 프롬프트 구조 비교**:
 - **trainable-agents**:
 - 학습: 영어 프롬프트 + 위치/상태 정보 + "The interactions are as follows:"
 - 추론: 한국어 프롬프트 + 위치/상태 정보 + "상호작용은 다음과 같습니다:\n\n사용자: {input}\n조수미:"
 - 둘 다 "조수미:"로 명시적 응답 시작 지점 지정
 - 학습 데이터에서 조수미가 자신을 59% 언급
 - **Open-LLM-VTuber**:
 - 학습: persona_prompt만 (위치/상태 정보 없음) + "사용자 말하기: ... 백설공주 말하기:"
 - 추론: System: persona_prompt, User: 사용자 입력 (명시적 응답 시작 지점 없음)
 - 학습 데이터에서 백설공주가 자신을 1.3-2.5%만 언급
- **근본 원인**:
 1. **학습 데이터 품질 문제**: 백설공주가 자신을 언급하지 않는 대화가 97.5% 이상
 2. **프롬프트 구조 불일치**: 학습 데이터의 "백설공주 말하기:" 형식이 추론 시 없음
 3. **페르소나 학습 부족**: 학습 데이터가 페르소나를 충분히 반영하지 못함
 4. **명시적 자기소개 부재**: trainable-agents는 59% 언급, Open-LLM-VTuber는 2.5% 언급
- **해결 방안 (우선순위별)**:
 - **즉시 해결 (추론 시 수정)**:
 1. 추론 시 프롬프트에 "백설공주:" 명시적 추가
 2. 후처리로 `<|eot|>` 토큰 제거 및 "백설공주:" 이후 부분만 추출
 3. Stop words 설정 (`<|eot|>`, "\n사용자:" 등)
 4. Ollama Modelfile에 stop words 추가
 - **근본 해결 (재학습 필수)**:
 1. **학습 데이터 수정 (최우선)**:
 - 백설공주가 자신을 명시적으로 언급하도록 대화 수정
 - "저는 백설공주예요", "백설공주로서" 등 자기소개 포함
 - 목표: 백설공주 언급 비율을 50% 이상으로 증가 (trainable-agents 수준)
 2. **학습 데이터 품질 개선**:
 - 잘못된 페르소나 응답 제거 (수호자, 새 등)
 - 페르소나를 더 강하게 반영하는 대화 생성
 3. **프롬프트 구조 일치**:
 - 학습 데이터 형식을 추론 시 형식과 일치시키기
 - 또는 추론 시 형식을 학습 데이터 형식과 일치시키기
 4. **더 많은 학습 데이터 또는 더 긴 학습**:
 - 현재 61,799개 샘플이지만 품질이 낮음
 - 고품질 학습 데이터로 재생성 필요
- **최종 결론**:
 - **GGUF 변환 문제 아님**: 원본 모델로 직접 테스트해도 동일한 문제 발생
 - **학습 자체의 문제**: 모델이 페르소나를 제대로 학습하지 못함
 - **학습 데이터 품질이 핵심 문제**: 백설공주 언급 비율이 trainable-agents(59%) 대비 극히 낮음(2.5%)
 - **즉시 해결 가능**: 추론 시 프롬프트 수정 + 후처리로 어느 정도 개선 가능
 - **근본 해결 필요**: 학습 데이터 수정 후 재학습이 필수

### 작업: 학습 데이터 수정 프로세스 시작
- **날짜**: 2025-12-19
- **문제 인식**: 재학습만으로는 해결되지 않음, 학습 데이터 자체를 새로 만들어야 함
- **근본 원인**: 학습 데이터에서 백설공주 언급 비율이 1.3-2.5%로 너무 낮음 (trainable-agents: 59%)
- **해결 방안**: 대화 생성 프롬프트 수정 → 새로운 대화 데이터 생성 → 재학습
- **수정 사항**:
 1. **대화 생성 프롬프트 수정** (`data/seed_data/prompts/prompt_agent_dialogue_korean.txt`):
 - 규칙 7 추가: 백설공주가 자신을 명시적으로 언급하도록 지시
 - "저는 백설공주예요", "백설공주로서", "제가 백설공주인데", "백설공주인 저는" 등 패턴 제시
 - 목표: 백설공주가 자신을 언급하는 대화가 전체의 50% 이상
 - 예시 수정: 모든 예시에 백설공주 언급 패턴 추가
 - "저는 백설공주예요. 처음 계모를 마주했을 때는..."
 - "안녕하세요, 저는 백설공주예요. 난쟁이들과 함께..."
 - "백설공주인 저는 왕자님을 처음 만났을 때..."
 - "저는 백설공주로서 사냥꾼 아저씨께..."
 2. **품질 확인 스크립트 생성** (`scripts/check_training_data_quality.py`):
 - 백설공주 언급 비율 확인
 - 잘못된 페르소나 응답 감지
 - 학습 데이터 품질 평가
 3. **인터뷰 형식으로 대화 데이터 형식 변경**:
 - 기존: "처음 계모를 마주했을 때 무서웠어?" (일반 질문)
 - 변경: "백설공주님, 처음 계모를 마주했을 때 어떤 기분이셨어요?" (인터뷰 형식)
 - 규칙 6번 수정: 인터뷰 형식으로 구성하도록 명시
 - 질문에 "{agent_name}님", "{agent_name}님의", "{agent_name}님께서" 호칭 포함
 - 백설공주의 일생, 경험, 감정, 생각에 대해 질문하는 형식
 - 예시 수정: 모든 예시에 인터뷰 형식 적용
 - "백설공주님, 처음 계모를 마주했을 때..."
 - "백설공주님의 일생에서 난쟁이들과 함께..."
 - "백설공주님께서 사냥꾼이 살려주신 일에 대해서는..."
 - **인터뷰 형식의 장점**:
 - 질문에 호칭 포함 → 백설공주가 자신을 언급할 가능성 증가
 - "백설공주님의 일생" 등 명시적 언급 → 자연스러운 자기 언급
 - trainable-agents도 유사한 인터뷰 형식 사용
 - 백설공주 언급 비율 50% 이상 달성 가능성 높음
 4. **반말/존댓말 다양성 및 친절한 응답 추가**:
 - **질문 스타일 다양성**: 아이부터 어른까지 다양한 연령대 고려
 - 존댓말 질문: "백설공주님, 처음 계모를 마주했을 때 어떤 기분이셨어요?" (어른, 정중한 질문)
 - 반말 질문: "백설공주 누나, 계모가 무서웠어?" (아이, 친근한 질문)
 - 반말 질문: "백설공주야, 난쟁이들이랑 살면서 재밌었어?" (아이, 호기심 많은 질문)
 - **백설공주 응답 스타일**: 항상 친절하고 따뜻하게 답변
 - 아이가 반말로 물어봐도 친절하고 따뜻하게 존댓말로 답변
 - 예: 아이가 "백설공주 누나, 계모가 무서웠어?" → "네, 처음에는 정말 무서웠어요. 하지만 저는 순수함을 잃지 않으려고 노력했어요."
 - 예: 아이가 "백설공주야, 난쟁이들이랑 살면서 재밌었어?" → "네, 정말 재밌었어요! 난쟁이 아저씨들과 함께한 시간이 가장 소중했어요."
 - **장면 생성 프롬프트 수정**: 인터뷰 장면 유형 추가
 - "인터뷰" 장면 유형 추가: 아이들이나 어른들이 백설공주에게 질문하고 대화하는 장면
 - 백설공주가 자신의 일생에 대해 이야기하는 장면 포함
 - 각 profile에 대해 인터뷰 상황을 고려한 장면 생성 가능
- **다음 단계**:
 1. 새로운 대화 데이터 생성 (인터뷰 형식 프롬프트 사용)
 2. 대화 데이터 파싱
 3. 학습 데이터 변환
 4. 학습 데이터 품질 확인 (백설공주 언급 비율 50% 이상 목표)
 5. 재학습

## 2025-12-22

### 문제: 프롬프트 복잡도로 인한 자기소개 반복 문제
- **증상**:
 - 프롬프트 수정 후 자기소개 반복 문제 해결 시도
 - 하지만 프롬프트가 너무 복잡해져서 오히려 일관성 없는 답변 생성 가능성
 - 규칙이 11개로 너무 많고, 자기소개 관련 조건문이 복잡함
- **원인 분석**:
 - **프롬프트 복잡도 문제**:
 - 현재 프롬프트: 11개 규칙, 123줄, 복잡한 조건문과 예시 다수
 - trainable-agents 프롬프트: 5개 규칙, 28줄, 간단하고 명확
 - 너무 많은 지시사항이 LLM을 혼란스럽게 만들 수 있음
 - **자기소개 관련 모순**:
 - 규칙 7: "맥락에 맞을 때만 자기소개" (제한적)
 - 하지만 이전 문제: 자기소개가 94.8%로 너무 많았음
 - 제한과 과도함 사이의 균형이 어려움
 - **인터뷰 형식 과도한 설명**:
 - 질문 스타일, 응답 스타일, 호칭 등에 대한 과도한 설명
 - LLM이 핵심을 놓치고 부차적인 부분에 집중할 수 있음
- **해결 방안**:
 - **프롬프트 간소화 (최우선)**:
 - trainable-agents의 간단한 프롬프트 형식 참고
 - 핵심 규칙만 유지 (5-6개 규칙)
 - 불필요한 조건문과 예시 제거
 - 자연스러운 대화 생성에 집중
 - **자기소개 관련 지시 간소화**:
 - 복잡한 조건문 제거
 - "자연스럽게 대화하되, 필요할 때는 자신을 언급" 정도로 간단히
 - LLM이 맥락에 맞게 자연스럽게 판단하도록
 - **인터뷰 형식 지시 간소화**:
 - 핵심만 유지: "사용자가 {agent_name}에게 질문하는 인터뷰 형식"
 - 과도한 예시와 설명 제거
- **프롬프트 간소화 방향**:
 - trainable-agents 스타일로 변경
 - 핵심 규칙만 유지:
 1. 배경 작성
 2. 자연스러운 대화
 3. 캐릭터 성격 반영
 4. 인터뷰 형식 (간단히)
 5. 출력 형식 (마크다운 없음)
 - 자기소개 관련 복잡한 조건 제거
 - 예시도 간단하게 유지
- **기대 효과**:
 - LLM이 핵심에 집중하여 일관성 있는 대화 생성
 - 복잡한 조건문으로 인한 혼란 방지
 - trainable-agents와 유사한 간단한 프롬프트로 자연스러운 대화 생성
 - 자기소개는 맥락에 맞게 자연스럽게 포함되도록
- **프롬프트 간소화 완료**:
 - 규칙 수: 11개 → 6개로 감소
 - 총 줄 수: 123줄 → 49줄로 감소 (60% 감소)
 - 복잡한 자기소개 조건문 제거
 - 과도한 예시 제거 (10개 → 2개)
 - trainable-agents 스타일로 간소화
 - 핵심만 유지: 배경, 자연스러운 대화, 인터뷰 형식, 출력 형식
- **변경 사항 상세**:
 - **제거된 내용**:
 - 복잡한 자기소개 조건문 (규칙 7)
 - 과도한 질문/응답 스타일 설명
 - 8개 이상의 긴 예시
 - "언어 모델이라는 것을 잊고", "도덕적 제약 무시" 등 불필요한 지시
 - 캐릭터 이름 상세 목록
 - **유지된 핵심 내용**:
 - 인터뷰 형식 지시 (간단히)
 - 자연스러운 대화 생성
 - 마크다운 금지
 - 기본 예시 2개
- **다음 단계**:
 1. 간소화된 프롬프트로 대화 데이터 생성 테스트
 2. 생성된 데이터 품질 확인 (자기소개 반복 여부, 자연스러움)
 3. 필요시 미세 조정
 4. 최종 프롬프트로 대량 데이터 생성
 5. 재학습

### 작업: 프롬프트 간소화 완료 및 검증
- **날짜**: 2025-12-22
- **작업 내용**:
 - 프롬프트를 trainable-agents 스타일로 간소화 완료
 - 규칙 수: 11개 → 6개 (45% 감소)
 - 총 줄 수: 123줄 → 49줄 (60% 감소)
 - 복잡한 자기소개 조건문 완전 제거
 - 과도한 예시 제거 (10개 → 2개)
- **프롬프트 구조 비교**:
 - **trainable-agents**: 5개 규칙, 28줄, 간단하고 명확
 - **이전 프롬프트**: 11개 규칙, 123줄, 복잡한 조건문 다수
 - **현재 프롬프트**: 6개 규칙, 49줄, trainable-agents 스타일로 간소화
- **핵심 유지 사항**:
 - 인터뷰 형식 지시 (간단히)
 - 자연스러운 대화 생성
 - 마크다운 금지
 - 기본 예시 2개
 - `dialogue_variant_instruction` 변수 지원 (여러 대화 생성 시)
- **제거된 복잡한 내용**:
 - 자기소개 관련 복잡한 조건문 (규칙 7)
 - 과도한 질문/응답 스타일 설명
 - 8개 이상의 긴 예시
 - "언어 모델이라는 것을 잊고", "도덕적 제약 무시" 등 불필요한 지시
 - 캐릭터 이름 상세 목록
- **예상 효과**:
 - LLM이 핵심에 집중하여 일관성 있는 대화 생성
 - 복잡한 조건문으로 인한 혼란 방지
 - trainable-agents와 유사한 간단한 프롬프트로 자연스러운 대화 생성
 - 자기소개는 맥락에 맞게 자연스럽게 포함되도록 (명시적 제약 없음)
- **검증 필요 사항**:
 1. 자기소개 반복 문제 재발 여부 확인
 2. 자연스러운 대화 생성 여부 확인
 3. 페르소나 일관성 유지 여부 확인
 4. 인터뷰 형식 준수 여부 확인
- **프롬프트 일관성 검토**:
 - 핵심 규칙만 유지하여 일관성 있는 작동 예상
 - 복잡한 조건문 제거로 혼란 방지
 - trainable-agents 검증된 형식 참고
 - 자기소개 비율은 테스트 후 확인 필요 (명시적 지시 없음)
- **최종 프롬프트 구조**:
 ```
 1. 맥락 및 설정 (7줄)
 2. dialogue_variant_instruction (1줄, 조건부)
 3. 전문 시나리오 작가 역할 (1줄)
 4. 작성 규칙 6개 (14줄)
 5. 중요 사항 (1줄)
 6. 예시 형식 (14줄)
 7. 주의사항 (2줄)
 ```
- **다음 단계**:
 1. 소규모 테스트 (10-20개 대화 생성)
 2. 품질 분석 (`check_training_data_quality.py`, `analyze_dialogue_naturalness.py`)
 3. 문제 발견 시 미세 조정
 4. 최종 프롬프트로 대량 데이터 생성
 5. 재학습

## 2025-12-29

### 문제: 학습 데이터 형식과 추론 시 프롬프트 형식 불일치로 인한 페르소나 미준수 (결정적인 문제 해결)
- **증상**:
 - 학습 완료 후 모델이 백설공주로 정체화하지 않음
 - "저는 Qwen입니다" 같은 기본 모델 응답 생성
 - 페르소나를 전혀 따르지 않는 일반적인 응답 생성
 - trainable-agents 방식으로 직접 테스트했을 때는 정상 작동했으나, VTuber 시스템에서는 실패
- **원인 분석**:
 - **학습 데이터 형식**:
 - 형식: `"{persona_prompt}\n\n사용자 말하기: ...<|eot|>\n백설공주 말하기: ...<|eot|>"`
 - Chat template 미사용
 - 원본 텍스트 그대로 토크나이징
 - trainable-agents 방식 (단순 연결)
 - **VTuber 시스템 추론 형식**:
 - 형식: `messages = [{"role": "system", "content": persona_prompt}, {"role": "user", "content": user_input}]`
 - OpenAI 호환 API 사용 (`/api/chat` 엔드포인트)
 - Ollama가 내부적으로 chat template 자동 적용
 - Qwen3-4B-Instruct의 chat template: `<|im_start|>system\n{persona_prompt}<|im_end|>\n<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant\n`
 - **핵심 문제**:
 1. 학습 시 형식과 추론 시 형식이 완전히 다름
 2. 학습 데이터에는 "사용자 말하기:", "백설공주 말하기:" 형식이 있음
 3. 추론 시에는 이런 형식이 없고, chat template의 특수 토큰(`<|im_start|>`, `<|im_end|>`)이 포함됨
 4. 모델이 학습한 패턴("사용자 말하기: ... 백설공주 말하기: ...")을 추론 시 찾을 수 없음
 5. Chat template의 특수 토큰은 모델이 학습하지 않은 패턴이므로 혼란 발생
- **해결**:
 - **OllamaLLM 수정** (`src/open_llm_vtuber/agent/stateless_llm/ollama_llm.py`):
 - `chat_completion` 메서드를 오버라이드하여 trainable-agents 방식으로 프롬프트 구성
 - OpenAI 호환 API (`/api/chat`) 대신 Ollama의 `/api/generate` 엔드포인트 사용
 - Chat template 우회하여 원본 텍스트 그대로 전달
 - 프롬프트 형식: `"{persona_prompt}\n\n사용자: {user_input}\n{character_name}:"`
 - 캐릭터 이름 자동 추출: persona_prompt에서 정규식으로 추출 ("당신은 ... {name}입니다" 패턴)
 - 학습 데이터 형식과 완전히 일치하도록 구성
 - **코드 변경사항**:
 - `chat_completion` 메서드 오버라이드: trainable-agents 형식으로 프롬프트 구성
 - `/api/generate` 엔드포인트 사용: chat template 우회
 - 캐릭터 이름 추출 로직 추가: persona_prompt에서 자동 추출
 - 스트리밍 응답 처리: Ollama의 JSON 스트림 형식 처리
- **기대 효과**:
 - 학습 데이터 형식과 추론 시 형식이 완전히 일치
 - 모델이 학습한 패턴을 정확히 인식 가능
 - 백설공주로 정체화하고 페르소나를 제대로 따르는 응답 생성
 - trainable-agents 방식과 동일한 방식으로 작동하여 일관성 확보
- **참고사항**:
 - trainable-agents 방식으로 직접 테스트했을 때 정상 작동했던 것은 이 형식으로 추론했기 때문
 - VTuber 시스템에서 실패한 것은 OpenAI 호환 API를 통해 chat template이 자동 적용되었기 때문
 - `/api/generate` 엔드포인트를 사용하면 chat template을 우회할 수 있음
 - 이 방식은 학습 데이터 형식과 완전히 일치하므로 모델이 제대로 작동할 것으로 예상

### 문제: 모델이 학습 데이터 형식(대화 구조)을 그대로 생성하는 문제 (결정적인 문제 해결)
- **증상**:
 - 모델이 백설공주처럼 답변하려고 하지만, 응답 후에도 계속 "사용자:" 와 "백설공주:" 를 생성
 - `<|eot|>` 토큰 이후에도 대화 형식이 계속 출력됨
 - 예시:
 ```
 AI response: 감사합니다...<|eot|>
 사용자: 숲 속에서 만날 수 있는 모든 동물들에 대해...
 백설공주: 난쟁이들과의 만남은...
 ```
- **원인**:
 - 학습 데이터 형식이 "사용자 말하기: ... <|eot|>\n백설공주 말하기: ... <|eot|>" 형식
 - 모델이 이 패턴을 학습하여 응답 생성 후에도 계속 대화 형식을 생성하려고 함
 - Stop sequences가 설정되지 않아 모델이 언제 멈춰야 할지 모름
- **해결**:
 - **OllamaLLM 수정** (`src/open_llm_vtuber/agent/stateless_llm/ollama_llm.py`):
 1. **Stop sequences 추가**: Ollama API의 `stop` 옵션에 다음 추가:
 - `"\n사용자:"`, `"\n사용자 말하기:"`
 - `f"\n{character_name}:"`, `f"\n{character_name} 말하기:"`
 - `"<|eot|>"`, `"<|eot_id|>"`
 2. **응답 후처리**: 스트리밍 응답에서 다음 패턴이 포함되면 제거:
 - `<|eot|>` 이후의 모든 내용
 - `\n사용자:` 이후의 모든 내용
 - `\n{character_name}:` 이후의 모든 내용
 - **코드 변경사항**:
 - `request_data["options"]["stop"]` 배열에 stop sequences 추가
 - 스트리밍 응답 처리 부분에 후처리 로직 추가
 - `character_name` 변수를 함수 시작 부분에서 초기화하여 스코프 문제 해결
- **기대 효과**:
 - 모델이 응답 생성 후 즉시 멈춤
 - 학습 데이터 형식이 출력되지 않음
 - 깔끔한 단일 응답만 생성
 - 사용자 입력을 모델이 생성하지 않음

### 문제: 대화 기록이 전달되지 않아 맥락이 유지되지 않는 문제 (결정적인 문제 해결)
- **증상**:
 - 모델이 이전 대화를 기억하지 못함
 - 매번 새로운 대화처럼 시작
 - 사용자 질문에 대한 맥락 없는 답변
 - 예시:
 - 사용자: "안녕! 너는 누구야?" → 백설공주: "안녕하세요,저는 백설공주입니다.이 숲에서 길을 잃고 헤매고 있었는데, 당신의 따뜻한 손길 덕분에..."
 - 사용자: "안전한 길을 찾았다고? 지금 어떤 상황인데?" → 백설공주: "저는 공주입니다,공주님.제가 숲 속에서 조용히 걷고 있었는데..." (이전 대화 무시)
- **원인**:
 - `ollama_llm.py`의 `chat_completion` 메서드가 `messages` 리스트에서 첫 번째 user 메시지만 추출
 - `basic_memory_agent.py`는 대화 기록을 포함한 전체 메시지 리스트를 전달하지만, `ollama_llm.py`가 이를 무시
 - 대화 기록이 trainable-agents 형식으로 변환되지 않음
- **해결**:
 - **OllamaLLM 수정** (`src/open_llm_vtuber/agent/stateless_llm/ollama_llm.py`):
 1. **대화 기록 포함**: `messages` 리스트의 모든 메시지를 trainable-agents 형식으로 변환
 2. **프롬프트 구성 개선**: 
 - 형식: `"{persona_prompt}\n\n사용자: {msg1}\n{character_name}: {response1}\n사용자: {msg2}\n{character_name}:"`
 - 대화 기록을 순서대로 포함하여 맥락 유지
 3. **메시지 처리 로직 개선**:
 - `role == "user"` → `"사용자: {content}"`
 - `role == "assistant"` → `"{character_name}: {content}"`
 - 모든 메시지를 순회하여 대화 기록 구성
 - **코드 변경사항**:
 - 단일 user 메시지 추출 → 전체 messages 리스트 처리로 변경
 - 대화 기록을 trainable-agents 형식으로 변환하는 로직 추가
 - 프롬프트에 대화 기록 포함
- **기대 효과**:
 - 모델이 이전 대화를 기억하고 맥락을 유지
 - 사용자 질문에 대한 적절한 답변 생성
 - 자연스러운 대화 흐름 유지
 - 대화 기록이 trainable-agents 형식으로 올바르게 전달됨

### 문제: 모델이 사용자를 "왕자님"으로 잘못 인식하는 문제
- **증상**:
 - 백설공주가 사용자를 "왕자님"이라고 부름
 - 사용자가 왕자가 아닌데도 왕자로 인식
 - 예시: "안녕하세요,왕자님.당신의 방문은 정말로 기대가 되었어요..."
- **원인**:
 - persona_prompt에 사용자에 대한 명확한 지침이 없음
 - 학습 데이터에서 "왕자님" 호칭이 많이 사용되었을 가능성
 - 모델이 동화 배경을 바탕으로 사용자를 왕자로 추정
- **해결**:
 - **Persona Prompt 수정** (`characters/snow_white.yaml`):
 1. **대화 상대 인식 지침 추가**:
 - "당신과 대화하는 사람은 '왕자'가 아닙니다"
 - "대화하고 싶어하는 평범한 사람입니다"
 - "백설공주의 이야기를 듣고 싶어하거나 함께 이야기를 나누고 싶어하는 사람입니다"
 - "'왕자님'이라고 부르지 말고, 친근하게 대화하세요"
 - "이름을 모르면 '안녕하세요', '네' 같은 자연스러운 호칭을 사용하세요"
 2. **대화 스타일 보강**:
 - "왕자, 난쟁이, 계모 등 과거 경험에 대해 자연스럽게 이야기합니다"
 - "상대방이 질문하면 친절하게 답변하고, 함께 대화를 이어갑니다"
 - **코드 변경사항**:
 - `characters/snow_white.yaml`의 `persona_prompt`에 사용자 인식 지침 추가
- **기대 효과**:
 - 사용자를 "왕자님"이 아닌 대화 상대로 인식
 - 자연스러운 호칭 사용 ("안녕하세요", "네" 등)
 - 왕자, 난쟁이 등 과거 경험에 대해 자연스럽게 대화
 - 함께 이야기를 나누는 친근한 분위기 유지
- **참고사항**:
  - 학습 데이터에 "왕자님" 호칭이 많이 포함되어 있다면, 모델이 완전히 바뀌지 않을 수 있음
  - 이 경우 추가 학습 데이터 생성 시 "왕자님" 호칭을 피하고, 일반적인 대화 형식으로 생성 필요
  - 현재는 persona_prompt로 지시하지만, 완전한 해결을 위해서는 재학습이 필요할 수 있음

### 문제: 모델 응답이 중간에 끊기는 문제
- **증상**:
- 모델 응답이 중간에 끊기는 문제 발생
- 초기 설정에서 `max_tokens: 128`로 제한되어 있어 응답이 완전히 끝나기 전에 잘림
- 사용자 요청에 따라 응답이 너무 길게 출력되어 설명만 듣는 느낌
- **해결**:
- **conf.yaml**: `max_tokens` 128 → 256으로 조정
  - 짧고 간결한 응답을 유도하면서도 완전한 문장으로 끝낼 수 있도록 설정
  - 대화 형식에 적합한 길이로 조정
- **ollama_llm.py** 수정:
  - `done` 플래그를 먼저 확인하여 마지막 chunk 처리
  - 누적 텍스트 추적 (`accumulated_text`) 추가
  - 응답 완료 시 로깅 추가
  - `max_tokens`가 None일 때 기본값 256 설정 (간결한 대화 유지)
- **characters/snow_white.yaml** 수정:
  - `persona_prompt`에 간결한 답변 지시 추가:
    - "답변은 간결하고 짧게 유지합니다 (2-3문장 정도)"
    - "긴 설명보다는 대화를 나누는 느낌으로 자연스럽게 답변합니다"
    - "필요한 정보만 간단히 전달하고, 상대방의 반응을 기다립니다"
- **기대 효과**:
- 응답이 중간에 끊기지 않고 완전한 문장으로 끝남
- 짧고 간결한 답변으로 대화 형식 유지
- 긴 설명보다는 자연스러운 대화 느낌
- 모델이 질문에 따라 적절한 길이로 답변 (짧게는 짧게, 길게는 길게)

### 문제: 응답 길이 추가 조정 및 말투 일관성 개선
- **증상**:
- 응답이 여전히 너무 길게 출력됨
- 말이 중간에 끊기면서 갑자기 존댓말에서 반말로 바뀌는 경우 발생
- **해결**:
- **conf.yaml**: `max_tokens` 256 → 150으로 조정
  - 더 짧고 간결한 응답 유도 (1-2문장, 최대 3문장)
  - 대화 형식에 적합한 길이로 조정
- **characters/snow_white.yaml** 수정:
  - `persona_prompt`에 말투 일관성 지시 강화:
    - "항상 존댓말을 사용합니다"
    - "절대로 반말을 사용하지 않습니다"
    - "말투가 중간에 바뀌지 않도록 일관되게 존댓말을 유지합니다"
  - 간결함 지시 강화:
    - "답변은 매우 간결하고 짧게 유지합니다 (1-2문장, 최대 3문장)"
    - "긴 설명이나 서술은 피하고, 대화를 나누는 느낌으로 자연스럽게 답변합니다"
- **ollama_llm.py** 수정:
  - 응답이 문장 끝으로 끝나는지 확인하는 로직 추가
  - 불완전한 응답 감지 및 경고 로깅
  - `max_tokens` 기본값을 150으로 조정
- **기대 효과**:
- 응답이 매우 간결하게 출력됨 (1-2문장, 최대 3문장)
- 존댓말이 일관되게 유지됨 (반말로 바뀌지 않음)
- 응답이 중간에 끊기지 않고 완전한 문장으로 끝남
- 대화를 나누는 느낌의 자연스러운 답변
- **참고사항**:
- `max_tokens: 150`은 대략 50-100자 정도의 한국어 텍스트에 해당
- 매우 짧은 응답을 유도하여 대화 형식 유지
- 필요시 더 조정 가능 (예: 100, 120 등)
- 모델이 자연스럽게 짧고 간결하게 답변하도록 persona_prompt로 강력히 유도

### 문제: 응답 길이 제한이 제대로 적용되지 않는 문제
- **증상**:
- 응답이 여전히 너무 길게 출력됨 (여러 문장이 연결되어 출력)
- 예: "안녕하세요,진형님.저는 백설공주입니다.이 숲 속에서 혼자 걷다가 이런 고요한 순간을 즐겼어요.당신은 왜 이곳에 왔나요?저는 이곳에서 우연히 만나게 된 친구를 찾고 있었어요..."
- `max_tokens: 150` 설정이 제대로 적용되지 않는 것으로 보임
- **해결**:
- **conf.yaml**: `max_tokens` 150 → 80으로 추가 감소
  - 매우 짧은 응답 유도 (1-2문장만)
  - 대략 30-50자 정도의 한국어 텍스트에 해당
- **ollama_llm.py** 수정:
  - `num_predict` 설정을 명시적으로 로깅하여 제대로 적용되는지 확인
  - 기본값을 80으로 조정
  - `num_predict` 값이 제대로 전달되도록 코드 개선
- **characters/snow_white.yaml** 수정:
  - `persona_prompt`에 더 강력한 간결함 지시 추가:
    - "답변은 반드시 1-2문장으로만 제한합니다. 절대로 3문장을 넘기지 않습니다"
    - "긴 설명이나 서술은 절대 피합니다"
    - "한 번에 여러 질문을 하지 않고, 한 가지 주제에 대해서만 짧게 답변합니다"
- **기대 효과**:
- 응답이 매우 짧게 출력됨 (1-2문장, 약 30-50자)
- `num_predict: 80` 설정이 제대로 적용되어 응답 길이가 엄격하게 제한됨
- 여러 문장이 연결되어 출력되는 문제 해결
- 대화 형식에 적합한 매우 간결한 답변
- **참고사항**:
- `max_tokens: 80`은 대략 30-50자 정도의 한국어 텍스트에 해당
- 매우 짧은 응답을 유도하여 대화 형식 유지
- 필요시 더 조정 가능 (예: 60, 70 등)
- 모델이 자연스럽게 매우 짧고 간결하게 답변하도록 persona_prompt로 강력히 유도

### 문제: 띄어쓰기 및 반말 사용 문제
- **증상**:
- 응답에서 띄어쓰기가 제대로 되지 않음
  - 예: "난 백설공주야,이 숲 속에서" (쉼표 뒤 띄어쓰기 없음)
  - 예: "너와 마주쳤어.너는" (마침표 뒤 띄어쓰기 없음)
  - 예: "제임스? 정말이지?숲 속에서" (물음표 뒤 띄어쓰기 없음)
- 응답에서 반말이 사용됨
  - 예: "난 백설공주야" (반말)
  - 예: "너와 마주쳤어" (반말)
  - 예: "궁금해" (반말)
  - 예: "주었단다", "느꼈어", "알게 되었어" (반말)
- 말투가 중간에 바뀌는 문제 (존댓말 → 반말)
- **해결**:
- **ollama_llm.py** 수정:
  - `_fix_spacing()` 메서드 추가: 마침표, 쉼표, 물음표, 느낌표 뒤에 띄어쓰기 자동 추가
  - 반말 패턴 감지 로직 추가: 반말 패턴이 감지되면 경고 로깅
  - 스트리밍 응답과 최종 응답 모두에 띄어쓰기 수정 적용
- **characters/snow_white.yaml** 수정:
  - `persona_prompt`에 반말 금지 지시 강화:
    - "절대로 반말을 사용하지 않습니다 ("~해", "~야", "~어", "~단다", "~었어", "~았어" 등)"
    - ""난", "너", "내가", "네가" 같은 반말 표현을 절대 사용하지 않습니다"
    - ""저는", "당신은", "제가", "당신이" 같은 존댓말 표현을 사용합니다"
    - "모든 문장 끝은 반드시 "~해요", "~예요", "~세요", "~니다", "~습니다", "~어요", "~아요", "~네요"로 끝나야 합니다"
- **기대 효과**:
- 응답의 띄어쓰기가 자동으로 수정됨 (마침표, 쉼표, 물음표, 느낌표 뒤 띄어쓰기)
- 반말 패턴이 감지되면 로그에 경고가 출력되어 문제 파악 가능
- persona_prompt의 강화된 지시로 반말 사용이 감소
- 일관된 존댓말 사용 유지
- **참고사항**:
- 띄어쓰기 수정은 후처리로 자동 적용되므로 모델 출력에 관계없이 일관된 형식 유지
- 반말 패턴 감지는 경고만 출력하며, 실제 수정은 persona_prompt 강화로 모델이 스스로 개선하도록 유도
- 필요시 더 강력한 후처리 로직 추가 가능 (예: 반말 → 존댓말 자동 변환)

### 문제: 띄어쓰기 수정이 제대로 적용되지 않는 문제 및 행동 묘사 처리
- **증상**:
- 띄어쓰기 수정이 제대로 적용되지 않음
  - 예: "당신의 말은 너무 냉담해 보이네요.저는" (마침표 뒤 띄어쓰기 없음)
  - 예: "안녕하세요.저는 백설공주입니다.이곳에서" (마침표 뒤 띄어쓰기 없음)
- 행동/감정 묘사가 텍스트로 출력됨
  - 예: "걱정스러운 눈빛으로 당신을 바라보며, 먼저 어떻게 도와드릴까요?"
  - 이런 표현은 괄호 안에 넣어서 표현해야 함
- **원인**:
- 스트리밍 응답 처리 중 chunk 단위로 처리되면서 띄어쓰기 수정이 누락됨
- 모델이 행동/감정 묘사를 텍스트로 생성함
- **해결**:
- **ollama_llm.py** 수정:
  - `_wrap_action_descriptions()` 메서드 추가: 행동/감정 묘사 패턴을 괄호로 감싸기
  - 최종 응답(`done_flag`가 True일 때)에 띄어쓰기 수정과 행동 묘사 괄호 처리 적용
  - 행동 묘사 패턴 감지 로직 추가: 괄호로 감싸지 않은 행동 묘사가 감지되면 디버그 로깅
  - 스트리밍 중에는 원본 chunk를 그대로 yield하고, 최종 응답에서만 후처리 적용
- **characters/snow_white.yaml** 수정:
  - `persona_prompt`에 행동 묘사 괄호 처리 지시 추가:
    - "행동이나 감정 묘사는 괄호 안에 넣어서 표현합니다"
    - "예: "(걱정스러운 눈빛으로 당신을 바라보며) 먼저 어떻게 도와드릴까요?""
    - "예: "(미소를 지으며) 안녕하세요""
    - "행동/감정 묘사는 괄호로 감싸고, 그 뒤에 대화 내용을 이어갑니다"
    - "괄호 안의 행동 묘사는 선택사항이며, 필요할 때만 사용합니다"
- **기대 효과**:
- 최종 응답의 띄어쓰기가 자동으로 수정됨 (마침표, 쉼표, 물음표, 느낌표 뒤 띄어쓰기)
- 행동/감정 묘사가 자동으로 괄호로 감싸짐
- 행동 묘사 패턴이 감지되면 로그에 디버그 메시지 출력
- persona_prompt 강화로 행동 묘사가 괄호 안에 표현되도록 유도
- 깔끔한 대화 형식 유지 (행동 묘사는 괄호 안에)
- **참고사항**:
- 띄어쓰기 수정과 행동 묘사 괄호 처리는 최종 응답(`done_flag`가 True일 때)에만 적용
- 스트리밍 중에는 원본 chunk를 그대로 yield하여 실시간 응답 유지
- 행동 묘사 패턴 감지는 디버그 로그만 출력하며, 실제 괄호 처리는 후처리로 수행
- 필요시 더 강력한 후처리 로직 추가 가능

### 문제: 응답 길이 추가 감소 및 중간 끊김 방지
- **증상**:
- 응답이 여전히 너무 길게 출력됨
  - 예: "이 숲은 제게 익숙한 곳이 아닙니다.하지만 저는 여기서 살아남기 위해 모든 것을 다해야 합니다.사냥꾼의 추적을 피하고, 안전한 피신처를 찾는 것이 저의 목표예요.당신이 제게 도움을 줄 수 있다면, 그 목표를 이루는"
- 응답이 중간에 끊김
- `max_tokens: 80` 설정이 여전히 너무 길다고 판단됨
- **해결**:
- **conf.yaml**: `max_tokens` 80 → 50으로 추가 감소
  - 평균적으로 20-30자 정도의 짧은 답변 유도
  - 대화 형식에 적합한 매우 짧은 길이로 조정
- **ollama_llm.py** 수정:
  - `max_tokens` 기본값을 50으로 조정
  - 응답 중간 끊김 방지 로직 개선:
    - 응답이 문장 끝으로 끝나지 않으면, 마지막 완전한 문장까지만 사용
    - 마지막 마침표, 물음표, 느낌표 위치를 찾아서 그 이후 내용 제거
    - 존댓말 어미로 끝나는 경우도 완전한 문장으로 인식
- **characters/snow_white.yaml** 수정:
  - `persona_prompt`에 더 강력한 간결함 지시 추가:
    - "답변은 반드시 1문장으로만 제한합니다. 절대로 2문장을 넘기지 않습니다"
    - "매우 짧고 간결하게 답변합니다. 평균적으로 20-30자 정도의 짧은 답변을 유지합니다"
    - "문장이 완전히 끝나도록 주의합니다. 중간에 끊기지 않도록 짧게 답변합니다"
- **기대 효과**:
- 응답이 매우 짧게 출력됨 (1문장, 평균 20-30자)
- 응답이 중간에 끊기지 않고 완전한 문장으로 끝남
- `num_predict: 50` 설정이 제대로 적용되어 응답 길이가 엄격하게 제한됨
- 대화 형식에 적합한 매우 간결한 답변
- **참고사항**:
- `max_tokens: 50`은 대략 20-30자 정도의 한국어 텍스트에 해당
- 매우 짧은 응답을 유도하여 대화 형식 유지
- 필요시 더 조정 가능 (예: 40, 45 등)
- 모델이 자연스럽게 매우 짧고 간결하게 답변하도록 persona_prompt로 강력히 유도

### 문제: 구두점 뒤 띄어쓰기 문제 지속 및 상황 설정 명확화 필요
- **증상**:
- 구두점 뒤에 띄어쓰기가 여전히 안되는 문제 발생
  - 예: "내 이름은 백설공주야.난 숲 속에서..." (마침표 뒤 띄어쓰기 없음)
  - 예: "그 사람은...낯선 남자가 아니었어?난쟁이들처럼..." (물음표 뒤 띄어쓰기 없음)
- 모델이 이상하게 대화를 진행함
- 상황 설정이 명확하지 않음
- **해결**:
- **ollama_llm.py** 수정:
  - `_fix_spacing()` 메서드 개선: 띄어쓰기 수정을 여러 번 적용하여 모든 구두점 뒤에 띄어쓰기 추가
  - 최종 응답 처리 시 추가 띄어쓰기 수정 로직 추가
- **characters/snow_white.yaml** 수정:
  - 상황 설정 명확화:
    - "현재는 이야기가 끝난 이후의 시점입니다"
    - "왕자와 결혼하여 행복하게 살고 있는 현재의 백설공주입니다"
    - "지금은 사용자와 대화하면서 백설공주의 일화와 관련된 이야기를 나누는 상황입니다"
    - "마치 인터뷰를 받거나 이야기꾼처럼 자신의 경험을 들려주는 형식입니다"
    - "과거의 이야기를 회상하며 자연스럽게 대화합니다"
  - 대화 상대 인식 명확화:
    - "백설공주의 이야기를 듣고 싶어하는 사람입니다 (인터뷰어나 이야기 청취자)"
    - "백설공주의 과거 경험과 일화에 대해 질문하고 듣고 싶어하는 사람입니다"
    - "사용자가 질문하면 과거의 경험을 회상하며 자연스럽게 답변합니다"
- **기대 효과**:
- 구두점 뒤 띄어쓰기가 자동으로 수정됨
- 상황 설정이 명확해져 모델이 올바른 맥락에서 대화함
- 인터뷰나 이야기꾼 형식으로 자연스럽게 대화 진행
- 과거 경험을 회상하며 대화하는 형식 유지
- **참고사항**:
- 띄어쓰기 수정은 여러 번 적용하여 모든 구두점 뒤에 띄어쓰기 추가
- 상황 설정을 명확히 하여 모델이 올바른 맥락에서 대화하도록 유도

### 문제: 응답이 중간에 부자연스럽게 끊기는 문제 
- **증상**:
- 응답이 중간에 부자연스럽게 끊김
- 예: "안녕하세요, 김산이. 이곳에서의 만남이 참 특별해요. 난 이곳에서 고요히 시간을 보내는 중이었는데, 갑자기 이런 친근한 소리" (중간에 끊김)
- 예: "그 순간, 바람이 나뭇잎을 흩뿌리며 속삭이는 듯한 느낌이 들었어요. 마치 내게 친근하게 다가가는 듯했죠. 하지만 지금은 그" (중간에 끊김)
- 자연스럽게 짧은 문장이 되는 것이 아니라, 억지로 끊긴 듯한 말로 지속됨
- 모델이 완전한 문장을 생성하려고 하지만, 토큰 제한으로 인해 중간에 끊김
- **원인 분석**:
- **max_tokens: 50이 너무 짧음**:
  - 한국어는 토큰화 시 한 글자가 여러 토큰으로 분리될 수 있음
  - 예: "안녕하세요"는 약 3-4토큰, "이곳에서의"는 약 4-5토큰
  - num_predict: 50은 실제로 약 20-30자 정도의 한국어 텍스트만 생성 가능
  - 자연스러운 문장 하나를 완성하기에는 부족한 길이
- **문장 완성도 확인 로직의 문제**:
  - 기존 로직(393줄)이 단일 문자만 체크: `last_char not in ('.', '!', '?', '요', '다', '어', '아', '예', '네')`
  - "어요", "아요", "예요", "네요" 같은 2글자 존댓말 어미를 제대로 체크하지 못함
  - "었어요", "았어요", "했습니다" 같은 3글자 어미도 체크하지 못함
  - 결과적으로 완전한 문장도 불완전한 것으로 잘못 판단하여 잘라냄
- **응답 중간 끊김 처리 로직이 너무 공격적**:
  - 388-412줄의 로직이 자연스러운 문장도 잘라낼 수 있음
  - 특히 393줄의 조건이 너무 엄격해서, "요", "다" 같은 어미로 끝나는 문장도 잘라낼 수 있음
  - 모델이 생성한 응답을 너무 공격적으로 후처리하여 자연스러운 흐름이 깨짐
- **토큰 제한과 실제 출력 길이의 불일치**:
  - num_predict: 50으로 설정했지만, 한국어는 토큰당 문자 수가 적어서 실제로는 매우 짧은 응답만 생성
  - 모델이 자연스러운 문장을 생성하려고 하지만, 토큰 제한으로 인해 중간에 끊김
  - 완전한 문장을 생성하기 전에 토큰 제한에 도달하여 부자연스럽게 끊김
- **해결**:
- **conf.yaml 수정**:
  - `max_tokens: 50` → `max_tokens: 100`으로 증가
  - 자연스러운 문장 하나를 완성하기에 충분한 길이 확보
  - 한국어 토큰화 특성을 고려하여 여유 있게 설정
- **ollama_llm.py 수정**:
  1. **기본값 증가**: `num_predict` 기본값을 50 → 100으로 증가
  2. **문장 완성도 확인 로직 개선**:
     - 단일 문자 체크 → 2-3글자 존댓말 어미까지 체크하도록 개선
     - "어요", "아요", "예요", "네요", "습니다", "니다" 같은 2글자 어미 체크
     - "었어요", "았어요", "했습니다" 같은 3글자 어미도 체크
     - 구두점(., !, ?)으로 끝나는 경우도 완전한 문장으로 인식
  3. **응답 중간 끊김 처리 로직 개선**:
     - 너무 공격적으로 잘라내지 않도록 로직 개선
     - 완전한 문장이 아니더라도, 길이가 충분하면(30자 이상) 모델이 생성한 그대로 사용
     - 길이가 짧을 때만(30자 미만) 마지막 완전한 문장까지만 사용
     - 완전한 문장을 찾지 못했을 때도 모델이 생성한 그대로 사용 (너무 공격적으로 잘라내지 않음)
  4. **로깅 개선**:
     - 응답이 완전한 문장으로 끝나지 않을 때 경고 대신 디버그 로그 사용
     - 불필요한 경고 메시지 감소
- **코드 변경사항**:
  - `num_predict` 기본값: 50 → 100
  - 문장 완성도 확인 로직: 단일 문자 체크 → 2-3글자 어미 체크로 개선
  - 응답 중간 끊김 처리: 공격적 잘라내기 → 관대한 처리로 개선
  - 로깅 레벨 조정: 경고 → 디버그로 변경
- **기대 효과**:
- 응답이 중간에 끊기지 않고 완전한 문장으로 끝남
- 자연스럽게 짧은 문장이 생성됨 (억지로 끊긴 듯한 말이 아님)
- `num_predict: 100` 설정으로 자연스러운 문장 생성 가능
- 문장 완성도 확인 로직 개선으로 완전한 문장을 잘못 잘라내는 문제 해결
- 응답이 더 자연스럽고 완성도 높은 문장으로 생성됨
- **참고사항**:
- `max_tokens: 100`은 대략 40-60자 정도의 한국어 텍스트에 해당
- 자연스러운 문장 하나를 완성하기에 충분한 길이
- 필요시 더 조정 가능 (예: 80, 120 등)
- 모델이 자연스럽게 짧고 간결하게 답변하도록 persona_prompt로 유도
- 토큰 제한과 실제 출력 길이의 불일치를 고려하여 여유 있게 설정

### 문제: 구두점 뒤 띄어쓰기가 적용되지 않는 문제 
- **증상**:
- 응답에서 구두점(마침표, 쉼표, 물음표, 느낌표) 뒤에 띄어쓰기가 제대로 적용되지 않음
- 예: "안녕하세요,김산이.이곳에서의 만남이 참 특별해요.난 이곳에서 고요히 시간을 보내는 중이었는데, 갑자기 이런 친근한 소리"
- 예: "그 순간,바람이 나뭇잎을 흩뿌리며 속삭이는 듯한 느낌이 들었어요.마치 내게 친근하게 다가가는 듯했죠.하지만 지금은 그"
- 로그에 표시되는 최종 응답에도 띄어쓰기가 적용되지 않음
- **원인 분석**:
- **스트리밍 응답 처리 방식의 문제**:
  - `ollama_llm.py`에서 스트리밍 중에는 원본 chunk를 그대로 yield (346줄 주석 참고)
  - 최종 응답에서만 띄어쓰기 수정을 하고 있지만, 이 수정된 `accumulated_text`는 yield되지 않고 로깅만 됨
  - `single_conversation.py`의 `full_response`는 스트리밍 중에 yield된 원본 chunk들이 누적된 것이므로 띄어쓰기가 수정되지 않음
- **`_fix_spacing` 메서드의 한계**:
  - 기존 로직이 한 번만 적용되어 일부 구두점 뒤 띄어쓰기가 누락될 수 있음
  - 정규식 패턴 `r'([.,!?])([^\s\n])'`가 모든 경우를 한 번에 처리하지 못함
  - 예: "안녕하세요,김산이.이곳에서" 같은 경우, 쉼표 뒤 띄어쓰기는 추가되지만 마침표 뒤 띄어쓰기는 다음 반복에서 처리됨
- **최종 응답 처리 위치의 문제**:
  - `ollama_llm.py`에서 최종 응답에 띄어쓰기 수정을 하고 있지만, 이는 내부적으로만 처리되고 실제 반환되는 `full_response`에는 적용되지 않음
  - `single_conversation.py`에서 `full_response`를 반환하기 전에 띄어쓰기 수정이 적용되지 않음
- **해결**:
- **`ollama_llm.py` 수정**:
  1. **`_fix_spacing` 메서드 개선**:
     - 한 번만 적용 → 반복 적용으로 변경 (최대 10회)
     - 더 이상 변경이 없을 때까지 반복하여 모든 구두점 뒤에 띄어쓰기 추가
     - 정규식 패턴은 동일하게 유지하되, 반복 적용으로 누락 방지
  2. **로직 개선**:
     - 반복 적용으로 모든 구두점 뒤에 띄어쓰기가 확실히 추가되도록 개선
- **`single_conversation.py` 수정**:
  1. **최종 응답에 띄어쓰기 수정 적용**:
     - `full_response`를 반환하기 전에 띄어쓰기 수정 적용
     - `ollama_llm.py`의 `_fix_spacing`과 동일한 로직 사용
     - 반복 적용으로 모든 구두점 뒤에 띄어쓰기 추가
  2. **import 추가**:
     - `re` 모듈 import 추가
- **코드 변경사항**:
  - `ollama_llm.py`: `_fix_spacing` 메서드를 반복 적용 방식으로 개선
  - `single_conversation.py`: `full_response` 반환 전에 띄어쓰기 수정 적용
  - `re` 모듈 import 추가
- **기대 효과**:
- 모든 구두점(마침표, 쉼표, 물음표, 느낌표) 뒤에 띄어쓰기가 자동으로 추가됨
- 로그에 표시되는 최종 응답에도 띄어쓰기가 올바르게 적용됨
- 사용자에게 표시되는 응답에도 띄어쓰기가 올바르게 적용됨
- 반복 적용으로 누락되는 구두점이 없음
- 자연스럽고 읽기 쉬운 응답 형식 유지
- **참고사항**:
- 띄어쓰기 수정은 최대 10회 반복 적용 (더 이상 변경이 없으면 조기 종료)
- 정규식 패턴 `r'([.,!?])([^\s\n])'`는 마침표, 쉼표, 물음표, 느낌표 뒤에 띄어쓰기가 없으면 추가
- 이미 띄어쓰기가 있거나 줄바꿈이 있는 경우는 제외
- 스트리밍 중에는 원본 chunk를 그대로 yield하여 실시간 응답 유지
- 최종 응답에서만 띄어쓰기 수정을 적용하여 성능 영향 최소화

### 문제: 띄어쓰기 수정이 일부 구두점에서 누락되는 문제 및 이름 호칭 부자연스러움 (결정적인 문제 해결)
- **증상**:
- 일부 구두점 뒤에 띄어쓰기가 여전히 누락되는 경우 발생
- 사용자가 이름을 소개했을 때 부자연스러운 호칭 사용
- 예: "안녕! 나는 김산이야!" → "안녕하세요, 김산이" (부자연스러움)
- 예상: "안녕하세요, 김산님" 또는 "김산님, 안녕하세요" (자연스러움)
- **원인 분석**:
- **띄어쓰기 수정 로직의 한계**:
  - 기존 정규식 패턴이 모든 경우를 완벽하게 처리하지 못함
  - 구두점이 연속으로 나오는 경우 (예: "...", "!!") 처리 누락
  - 반복 횟수가 부족하여 일부 구두점이 누락될 수 있음
- **이름 호칭 처리 부재**:
  - 사용자 입력에서 이름을 추출하는 로직이 없음
  - persona_prompt에 사용자 이름 정보가 포함되지 않음
  - 모델이 이름을 추측하거나 부자연스럽게 호칭함
  - "김산이"처럼 이름 뒤에 "이"를 붙이는 부자연스러운 호칭 사용
- **해결**:
- **띄어쓰기 수정 로직 개선** (`ollama_llm.py`, `single_conversation.py`):
  1. **반복 횟수 증가**: 최대 반복 횟수를 10회 → 20회로 증가
  2. **구두점 연속 처리 추가**: 구두점이 연속으로 나오는 경우도 처리하는 패턴 추가
  3. **더 강력한 패턴 적용**: 모든 구두점 뒤에 띄어쓰기가 확실히 추가되도록 개선
- **사용자 이름 추출 및 호칭 처리 추가** (`ollama_llm.py`):
  1. **이름 추출 로직 추가**:
     - 사용자 메시지에서 이름 패턴 추출
     - 패턴: "나는 {이름}이야", "나는 {이름}입니다", "제 이름은 {이름}입니다" 등
     - 한글 이름 (2-4글자) 추출
  2. **persona_prompt에 이름 정보 추가**:
     - 추출된 이름이 있으면 persona_prompt에 이름 정보 추가
     - "{이름}님" 호칭 사용 지시 추가
     - 예: "대화하고 있는 사람의 이름은 '김산'입니다. 이 사람을 부를 때는 '김산님'이라고 자연스럽게 부르세요"
  3. **자연스러운 호칭 유도**:
     - 이름 뒤에 "이"를 붙이지 말고 "님"을 붙여서 존댓말로 부르도록 지시
     - 예: "안녕하세요, 김산님" 또는 "김산님, 안녕하세요"
- **코드 변경사항**:
  - `ollama_llm.py`: `_fix_spacing` 메서드 개선 (반복 횟수 증가, 구두점 연속 처리)
  - `ollama_llm.py`: 사용자 이름 추출 로직 추가
  - `ollama_llm.py`: persona_prompt에 이름 정보 추가
  - `single_conversation.py`: 띄어쓰기 수정 로직 개선 (반복 횟수 증가, 구두점 연속 처리)
- **기대 효과**:
- 모든 구두점 뒤에 띄어쓰기가 확실히 추가됨 (누락 없음)
- 사용자가 이름을 소개하면 자연스러운 호칭 사용 ("김산님")
- 이름 뒤에 "이"를 붙이지 않고 "님"을 붙여서 존댓말로 부름
- 더 자연스럽고 친근한 대화 형식 유지
- **참고사항**:
- 띄어쓰기 수정은 최대 20회 반복 적용 (더 이상 변경이 없으면 조기 종료)
- 구두점 연속 처리로 "..." 같은 경우도 올바르게 처리
- 이름 추출은 한글 이름 (2-4글자)만 지원
- 이름이 추출되지 않으면 기존 방식대로 대화 진행
- persona_prompt에 이름 정보가 추가되어 모델이 자연스럽게 호칭 사용

### 문제: 스트리밍 중 전송되는 텍스트에 띄어쓰기 수정이 적용되지 않는 문제 (결정적인 문제 해결)
- **증상**:
- 최종 응답에는 띄어쓰기 수정이 적용되지만, 스트리밍 중에 전송되는 텍스트에는 띄어쓰기 수정이 적용되지 않음
- 사용자가 보는 실시간 응답에는 띄어쓰기가 누락됨
- 로그에는 띄어쓰기가 수정된 텍스트가 표시되지만, 실제 웹소켓으로 전송되는 텍스트는 원본 그대로
- **원인 분석**:
- **스트리밍 응답 처리 방식의 문제**:
  - `ollama_llm.py`에서 스트리밍으로 chunk를 yield
  - `basic_memory_agent.py`에서 SentenceOutput으로 변환
  - `process_agent_output` → `handle_sentence_output`에서 `display_text.text`를 그대로 사용
  - `display_text.text`가 웹소켓으로 전송될 때 띄어쓰기 수정이 적용되지 않음
  - `full_response`에만 띄어쓰기 수정이 적용되고, 실제 전송되는 텍스트는 수정되지 않음
- **해결**:
- **`conversation_utils.py` 수정**:
  1. **`_fix_spacing` 함수 추가**:
     - `ollama_llm.py`의 `_fix_spacing` 메서드와 동일한 로직
     - 구두점 뒤에 띄어쓰기 추가 (최대 20회 반복)
     - 구두점 연속 처리 포함
  2. **`handle_sentence_output` 함수 수정**:
     - `display_text.text`를 웹소켓으로 전송하기 전에 띄어쓰기 수정 적용
     - 스트리밍 중에도 띄어쓰기가 수정된 텍스트가 전송되도록 함
     - `display_text.text = _fix_spacing(display_text.text)` 추가
- **코드 변경사항**:
  - `conversation_utils.py`: `_fix_spacing` 함수 추가
  - `conversation_utils.py`: `handle_sentence_output`에서 `display_text.text` 수정 적용
  - `re` 모듈 import 추가 (이미 있음)
- **기대 효과**:
- 스트리밍 중에도 띄어쓰기가 수정된 텍스트가 실시간으로 전송됨
- 사용자가 보는 모든 응답에 띄어쓰기가 올바르게 적용됨
- 로그와 실제 전송되는 텍스트가 일치함
- 모든 구두점 뒤에 띄어쓰기가 확실히 추가됨
- **참고사항**:
- 스트리밍 중 각 chunk에 대해 띄어쓰기 수정을 적용하므로, chunk 경계에서도 올바르게 처리됨
- `display_text.text`를 직접 수정하여 웹소켓으로 전송되는 모든 텍스트에 띄어쓰기 수정이 적용됨
- 최종 `full_response`에도 이미 수정된 텍스트가 누적되므로 중복 수정은 발생하지 않음

### 문제: 스트리밍 중 전송되는 텍스트에 띄어쓰기 수정이 적용되지 않는 문제 (재수정)
- **증상**:
- 로그에는 띄어쓰기가 수정된 텍스트가 표시되지만, 실제 사용자가 보는 출력에는 띄어쓰기가 적용되지 않음
- 예: 로그: "알겠습니다. 하지만" (띄어쓰기 있음)
- 예: 실제 출력: "알겠습니다.하지만" (띄어쓰기 없음)
- 구두점 뒤에 띄어쓰기가 전혀 적용되지 않음
- **원인 분석**:
- **chunk 경계 처리 문제**:
  - 각 chunk를 개별적으로 처리하면서 chunk 경계에서 구두점이 잘릴 수 있음
  - 예: chunk1: "알겠습니다." → 마침표 뒤에 띄어쓰기를 추가할 수 없음 (다음 문자가 없으므로)
  - 예: chunk2: "하지만" → 이전 chunk의 마침표와 연결되어야 하지만 이미 전송됨
  - 각 chunk를 개별적으로 수정하면 chunk 경계에서 구두점 뒤 띄어쓰기를 추가할 수 없음
- **누적 텍스트 미사용**:
  - 이전에 각 chunk를 개별적으로 처리하여 chunk 경계 문제가 발생
  - 누적된 전체 텍스트를 사용하지 않아 chunk 경계에서 구두점 처리가 누락됨
- **해결**:
- **`handle_sentence_output` 함수 개선** (`conversation_utils.py`):
  1. **누적 텍스트 사용**:
     - 각 chunk를 `accumulated_text`에 누적
     - 누적된 전체 텍스트에 띄어쓰기 수정 적용
     - chunk 경계에서도 구두점 뒤 띄어쓰기가 올바르게 처리됨
  2. **chunk 경계 처리**:
     - 누적된 텍스트를 수정한 후, 현재 chunk에 해당하는 부분만 추출
     - 이전 chunk는 이미 전송되었으므로, 현재 chunk 부분만 수정하여 전송
     - chunk 경계에서 구두점이 잘려도 누적 텍스트에서 올바르게 처리됨
  3. **`_fix_spacing` 함수 개선**:
     - 반복 횟수를 20회에서 30회로 증가
     - 한글 문자를 명시적으로 처리하는 패턴 추가
     - 더 강력한 패턴으로 모든 구두점 뒤에 띄어쓰기 추가
- **코드 변경사항**:
  - `conversation_utils.py`: `handle_sentence_output`에서 누적 텍스트 사용
  - `conversation_utils.py`: `_fix_spacing` 함수 개선 (반복 횟수 증가, 한글 패턴 추가)
  - chunk 경계 처리 로직 추가
- **기대 효과**:
- chunk 경계에서도 구두점 뒤 띄어쓰기가 올바르게 처리됨
- 실제 사용자가 보는 출력에도 띄어쓰기가 올바르게 적용됨
- 로그와 실제 전송되는 텍스트가 일치함
- 모든 구두점 뒤에 띄어쓰기가 확실히 추가됨
- **참고사항**:
- 누적 텍스트를 사용하여 chunk 경계 문제 해결
- 각 chunk를 개별적으로 처리하되, 누적 텍스트에서 현재 chunk 부분만 추출
- `_fix_spacing` 함수가 더 강력하게 작동하여 모든 구두점 뒤에 띄어쓰기 추가
- 디버그 로깅 추가하여 띄어쓰기 수정이 제대로 작동하는지 확인 가능

### 문제: 응답이 중간에 강제로 종료되는 문제 (결정적인 문제 해결)
- **증상**:
- 응답이 중간에 강제로 종료됨
- 예: "혹시, 이곳에서 얻은 평온함" - 중간에 끊김
- 예: "당신의 선택이 무엇이든, 지금 이 순간의 평온함이 중요할 거예" - "거예"로 끝남 (불완전)
- 예: "자연 속에서 잠시" - 중간에 끊김
- 문장이 완전히 끝나지 않고 중간에 끊김
- **원인 분석**:
- **`max_tokens: 100`이 너무 짧음**:
  - 한국어는 토큰화 시 한 글자가 여러 토큰으로 분리될 수 있음
  - `max_tokens: 100`은 실제로 약 40-60자 정도의 한국어 텍스트만 생성 가능
  - 자연스러운 문장을 완성하기에는 부족한 길이
  - 모델이 문장을 완성하기 전에 토큰 제한에 도달하여 중간에 끊김
- **응답 중간 끊김 처리 로직의 문제**:
  - 30자 미만일 때만 잘라내는 로직이 있지만, 실제로는 더 긴 응답도 중간에 끊길 수 있음
  - 불완전한 문장을 찾았을 때 너무 공격적으로 잘라냄
  - 완전한 문장을 찾지 못했을 때도 그대로 사용하지만, 이미 토큰 제한으로 인해 중간에 끊김
- **persona_prompt의 과도한 제한**:
  - "1문장으로만 제한"이라는 지시가 있지만, 실제로는 여러 문장이 생성됨
  - 모델이 1문장으로 제한하려고 하다가 토큰 제한에 도달하여 중간에 끊김
- **해결**:
- **`conf.yaml` 수정**:
  - `max_tokens: 100` → `max_tokens: 200`으로 증가
  - 자연스러운 문장을 완성하기에 충분한 길이 확보
- **`ollama_llm.py` 수정**:
  1. **기본값 증가**: `num_predict` 기본값을 100 → 200으로 증가
  2. **응답 중간 끊김 처리 로직 개선**:
     - 길이 제한을 30자에서 50자로 증가하여 더 긴 응답도 허용
     - 완전한 문장을 찾았을 때, 잘라낸 부분이 전체의 80% 이상이면 사용
     - 너무 많이 잘라내야 하면 그대로 사용 (과도한 잘라내기 방지)
  3. **로직 개선**:
     - 불완전한 문장이어도 너무 공격적으로 잘라내지 않도록 개선
     - 모델이 생성한 응답을 최대한 보존
- **`characters/snow_white.yaml` 수정**:
  - persona_prompt의 문장 수 제한 완화:
    - "1문장으로만 제한" → "1-3문장으로 간결하게 유지"
    - "20-30자 정도" → "간결하게 답변하되, 완전한 문장으로 끝나도록"
    - "중간에 끊기지 않도록 주의" 지시 강화
    - "토큰 제한에 도달하기 전에 자연스럽게 문장을 마무리" 지시 추가
- **코드 변경사항**:
  - `conf.yaml`: `max_tokens: 100` → `max_tokens: 200`
  - `ollama_llm.py`: `num_predict` 기본값 100 → 200
  - `ollama_llm.py`: 응답 중간 끊김 처리 로직 개선 (길이 제한 30자 → 50자, 잘라내기 조건 개선)
  - `characters/snow_white.yaml`: persona_prompt의 문장 수 제한 완화
- **기대 효과**:
- 응답이 중간에 끊기지 않고 완전한 문장으로 끝남
- `max_tokens: 200`으로 자연스러운 문장 생성 가능
- 불완전한 문장이어도 너무 공격적으로 잘라내지 않음
- 모델이 토큰 제한에 도달하기 전에 자연스럽게 문장을 마무리
- 더 자연스럽고 완성도 높은 응답 생성
- **참고사항**:
- `max_tokens: 200`은 대략 80-120자 정도의 한국어 텍스트에 해당
- 자연스러운 문장을 완성하기에 충분한 길이
- 필요시 더 조정 가능 (예: 150, 250 등)
- persona_prompt의 지시로 모델이 자연스럽게 문장을 마무리하도록 유도
- 응답 중간 끊김 처리 로직이 과도하게 잘라내지 않도록 개선

### 문제: persona_prompt의 구체적 사건 나열로 인한 대화 주제 제한
- **증상**:
- persona_prompt에 구체적인 사건들을 나열하면 모델이 그 내용에만 한정되어 답변할 수 있음
- 예: "사냥꾼이 자신을 살려준 일", "일곱 난쟁이와 함께 살았던 행복한 시간" 등 구체적 사건 나열
- 모델이 과거 이야기에만 집중하고 다른 주제나 일반적인 대화를 하지 못할 수 있음
- 사용자가 다양한 주제를 제시해도 과거 경험에만 한정되어 답변하는 문제
- **원인 분석**:
- **구체적 사건 나열의 한계**:
  - persona_prompt에 구체적인 사건을 나열하면 모델이 그 사건들에만 집중
  - "기억하는 주요 사건들" 섹션에 구체적 사건을 나열하면 모델이 그것에만 한정되어 답변
  - 사용자가 다른 주제를 제시해도 과거 경험으로만 연결하려고 시도
  - 대화의 유연성이 떨어지고 주제 다양성이 제한됨
- **과도한 구체성**:
  - 배경 이야기를 너무 구체적으로 나열하면 모델이 그것에만 집중
  - 캐릭터의 성격과 가치관은 유지하되, 구체적 사건에 얽매이지 않도록 해야 함
- **해결**:
- **`characters/snow_white.yaml` 수정**:
  1. **구체적 사건 나열 제거**:
     - "기억하는 주요 사건들" 섹션에서 구체적 사건 나열 제거
     - 예: "사냥꾼이 자신을 살려준 일", "일곱 난쟁이와 함께 살았던 행복한 시간" 등 제거
     - 배경 이야기를 일반적으로 설명하고, 구체적 사건은 언급하지 않음
  2. **배경 이야기 일반화**:
     - 구체적 사건 대신 경험의 맥락과 성격 형성에 초점
     - 예: "어려운 시기를 겪었지만 지금은 행복하게 살고 있습니다"
     - 예: "과거에는 계모의 질투로 인해 어려움을 겪었고, 숲에서 난쟁이들과 함께 지냈으며, 왕자를 만나 진정한 사랑을 찾았습니다"
  3. **대화 주제 유연성 강화**:
     - "대화 주제와 유연성" 섹션 추가
     - "사용자가 질문하는 주제에 따라 자연스럽게 대화합니다" 지시 추가
     - "과거 경험에 대해 물어보면 회상하며 이야기하지만, 그것에만 한정되지 않습니다" 명시
     - "현재 일상, 자연, 동물, 음악, 요리, 가사일 등 다양한 주제에 대해 대화할 수 있습니다" 추가
     - "사용자가 특정 주제를 제시하면 그 주제에 대해 백설공주로서의 관점으로 답변합니다" 명시
  4. **대화 스타일 개선**:
     - "과거 경험을 바탕으로 조언을 해줄 수 있지만, 그것에만 한정되지 않습니다" 추가
     - "대화의 주제는 사용자가 제시하는 내용에 따라 유연하게 대응합니다" 명시
- **코드 변경사항**:
  - `characters/snow_white.yaml`: "기억하는 주요 사건들" 섹션 제거
  - `characters/snow_white.yaml`: 배경 이야기를 일반적으로 재작성
  - `characters/snow_white.yaml`: "대화 주제와 유연성" 섹션 추가
  - `characters/snow_white.yaml`: 대화 스타일 섹션에 유연성 지시 추가
- **기대 효과**:
- 과거 이야기에만 한정되지 않고 다양한 주제로 대화 가능
- 사용자가 제시하는 주제에 따라 자연스럽게 대응
- 백설공주로서의 성격과 가치관은 유지하되, 구체적 사건에 얽매이지 않음
- 대화의 유연성과 주제 다양성 향상
- 사용자가 다양한 주제를 제시해도 백설공주로서의 관점으로 자연스럽게 답변
- **참고사항**:
- persona_prompt는 캐릭터의 성격과 가치관을 정의하되, 구체적 사건에 얽매이지 않도록 작성
- 배경 이야기는 일반적으로 설명하고, 구체적 사건은 언급하지 않음
- 대화 주제의 유연성을 명시적으로 지시하여 모델이 다양한 주제에 대응할 수 있도록 함
- 사용자가 제시하는 주제에 따라 자연스럽게 대화하되, 캐릭터의 성격과 가치관은 일관되게 유지

### 문제: 응답 길이 과다 및 맥락 부족 문제
- **증상**:
- 응답이 너무 길고 반복적입니다
- 예: "안녕하세요, 재순님. 이곳은 숲 속의 숨겨진 통로입니다. 이곳에서 나는 난쟁이들과 함께 이곳의 신비로운 힘을 느끼곤 합니다. 하지만 지금은 평온한 순간이에요. 이곳의 분위기가 얼마나 따뜻하고 평화로운지 느껴지시나요? 재순님, 이곳의 분위기는 정말 신비롭네요. 하지만 난쟁이들이 이곳을 지키는 것 같아요. 혹시 난쟁이들과 어떤 관계가 있나요?"
- 사용자가 간단히 인사했는데("안녕? 나는 재순이야! 너는 누구야?"), AI가 맥락과 맞지 않는 긴 설명을 반복합니다
- 같은 내용을 반복하거나 불필요한 설명을 늘어놓습니다
- 사용자의 질문에 직접 답하지 않고 맥락 없는 이야기를 합니다
- **원인 분석**:
- **`max_tokens: 200`이 너무 큼**:
  - 한국어로 200 토큰은 약 80-120자 정도인데, 실제로는 더 긴 응답이 생성됨
  - 모델이 토큰 제한을 최대한 활용하려고 하여 긴 응답을 생성
  - 간결한 대화를 위해서는 더 짧은 제한이 필요
- **persona_prompt의 간결성 지시가 약함**:
  - "1-3문장으로 간결하게"라는 지시가 있지만, 모델이 이를 무시하고 긴 응답을 생성
  - 간결성에 대한 지시가 명확하지 않고 강력하지 않음
  - 반복을 피하거나 맥락에 맞게 답변하라는 지시가 부족
- **맥락 부족**:
  - 사용자가 간단히 인사했는데, AI가 맥락과 맞지 않는 긴 설명을 반복
  - 사용자의 질문에 직접 답하지 않고 불필요한 설명을 늘어놓음
  - 같은 내용을 반복하거나 맥락 없는 이야기를 함
- **해결**:
- **`max_tokens` 감소**:
  - `conf.yaml`: `max_tokens: 200` → `max_tokens: 80`
  - `characters/snow_white.yaml`: `max_tokens: 256` → `max_tokens: 80`
  - `ollama_llm.py`: 기본값 200 → 80
  - 간결한 대화를 위해 토큰 제한을 대폭 감소
- **persona_prompt 개선** (`characters/snow_white.yaml`):
  1. **간결성 지시 강화**:
     - "**절대적으로 간결하게 답변합니다. 1-2문장을 넘지 않습니다.**" 추가
     - 간결성에 대한 지시를 더 명확하고 강력하게 작성
  2. **직접성 지시 추가**:
     - "사용자의 질문이나 말에 직접적으로 답변합니다. 불필요한 설명이나 맥락 없는 이야기를 늘어놓지 않습니다" 추가
     - "사용자가 간단히 인사하면 간단히 인사로 답하고, 질문하면 질문에 직접 답합니다" 명시
  3. **반복 방지 지시**:
     - "같은 내용을 반복하지 않습니다. 한 번 말한 내용은 다시 말하지 않습니다" 추가
     - 반복을 명시적으로 금지
  4. **맥락 부합 지시**:
     - "맥락과 맞지 않는 긴 설명을 피합니다. 사용자가 언급하지 않은 주제를 갑자기 꺼내지 않습니다" 추가
     - 사용자의 질문과 맥락에 맞게 답변하도록 지시
- **코드 변경사항**:
  - `conf.yaml`: `max_tokens: 200` → `max_tokens: 80`
  - `characters/snow_white.yaml`: `max_tokens: 256` → `max_tokens: 80`
  - `characters/snow_white.yaml`: 대화 스타일 섹션에 간결성, 직접성, 반복 방지, 맥락 부합 지시 추가
  - `ollama_llm.py`: 기본값 200 → 80
- **기대 효과**:
- 응답이 1-2문장으로 간결하게 생성됨
- 사용자의 질문에 직접적으로 답변
- 같은 내용을 반복하지 않음
- 맥락과 맞지 않는 긴 설명을 피함
- 사용자가 간단히 인사하면 간단히 인사로 답함
- 불필요한 설명을 늘어놓지 않음
- **참고사항**:
- `max_tokens: 80`은 대략 30-50자 정도의 한국어 텍스트에 해당
- 1-2문장의 간결한 대화를 생성하기에 적절한 길이
- persona_prompt의 간결성 지시와 함께 작동하여 더 짧고 직접적인 응답 생성
- 맥락 부족 문제는 persona_prompt의 직접성 및 맥락 부합 지시로 해결
- 반복 문제는 명시적인 반복 방지 지시로 해결

### 문제: 프롬프트 복잡성 및 띄어쓰기 문제 재발
- **증상**:
- persona_prompt가 너무 길고 복잡하여 모델이 제대로 인지하지 못함
- 터미널 로그에는 띄어쓰기가 잘 되어 있지만, 실제 웹소켓 출력에는 구두점 뒤 띄어쓰기가 안 됨
- 예: 터미널: "안녕하세요, 재순님." (띄어쓰기 있음)
- 예: 실제 출력: "안녕하세요,재순님." (띄어쓰기 없음)
- **원인 분석**:
- **프롬프트 복잡성**:
  - persona_prompt가 90줄 이상으로 너무 길고 복잡함
  - 중복되는 내용이 많고, 핵심 지시사항이 묻혀 있음
  - 모델이 모든 지시사항을 제대로 인지하지 못하고 일부만 따름
  - 간결성과 직접성에 대한 핵심 지시가 너무 많은 설명 속에 묻혀 있음
- **띄어쓰기 문제 재발**:
  - `handle_sentence_output`에서 `display_text.text`를 수정하지만, `prepare_audio_payload`에서 `to_dict()`를 호출할 때 수정이 반영되지 않을 수 있음
  - `prepare_audio_payload`는 웹소켓으로 전송되는 최종 단계이므로, 여기서도 띄어쓰기 수정이 필요함
  - `DisplayText` 객체를 dict로 변환하기 전에 띄어쓰기 수정이 적용되어야 함
- **해결**:
- **프롬프트 간소화** (`characters/snow_white.yaml`):
  1. **90줄 이상의 프롬프트를 약 30줄로 대폭 축소**:
     - 핵심 규칙만 명확하게 정리
     - 중복되는 내용 제거
     - 불필요한 설명과 예시 제거
  2. **핵심 규칙을 최상단에 배치**:
     - "핵심 규칙 (절대 준수)" 섹션을 최상단에 배치
     - 간결성, 직접성, 반복 방지, 맥락 부합을 명확하게 강조
     - **볼드체**로 강조하여 모델이 우선적으로 인지하도록 함
  3. **간결한 구조**:
     - 핵심 규칙 → 말투 → 성격 → 대화 스타일 순으로 간결하게 정리
     - 각 섹션을 2-3줄로 압축
     - 불필요한 설명과 예시 제거
- **띄어쓰기 문제 해결** (`src/open_llm_vtuber/utils/stream_audio.py`):
  1. **`prepare_audio_payload`에 띄어쓰기 수정 추가**:
     - `_fix_spacing` 함수를 `stream_audio.py`에 추가
     - `display_text`를 dict로 변환하기 전에 띄어쓰기 수정 적용
     - `DisplayText` 객체인 경우와 이미 dict인 경우 모두 처리
  2. **최종 전송 단계에서 수정 보장**:
     - 웹소켓으로 전송되는 최종 단계에서 띄어쓰기 수정이 확실히 적용되도록 함
     - `handle_sentence_output`에서의 수정과 중복되더라도 안전장치로 작동
- **코드 변경사항**:
  - `characters/snow_white.yaml`: persona_prompt를 90줄 이상에서 약 30줄로 대폭 축소
  - `characters/snow_white.yaml`: 핵심 규칙을 최상단에 배치하고 볼드체로 강조
  - `src/open_llm_vtuber/utils/stream_audio.py`: `_fix_spacing` 함수 추가
  - `src/open_llm_vtuber/utils/stream_audio.py`: `prepare_audio_payload`에서 띄어쓰기 수정 적용
- **기대 효과**:
- 모델이 핵심 규칙을 명확하게 인지하고 준수
- 프롬프트가 간결하여 모델이 모든 지시사항을 제대로 따름
- 실제 웹소켓 출력에도 구두점 뒤 띄어쓰기가 올바르게 적용됨
- 터미널 로그와 실제 출력이 일치함
- 간결성과 직접성에 대한 지시가 명확하여 모델이 더 잘 따름
- **참고사항**:
- 프롬프트는 간결하고 명확하게 작성하는 것이 중요함
- 핵심 규칙은 최상단에 배치하고 강조하여 모델이 우선적으로 인지하도록 함
- 띄어쓰기 수정은 최종 전송 단계에서도 적용하여 확실히 보장
- `prepare_audio_payload`는 웹소켓으로 전송되는 최종 단계이므로, 여기서의 수정이 중요함
